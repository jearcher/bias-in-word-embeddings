{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Re-implement_CN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/Re_implement_CN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8MHhIgRUhPg7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Re-implementation of Conceptor Negation\n",
        "\n",
        "\n",
        "*   Word-similarity task\n",
        "*   STS(Semantic Textual Similarity) tasks\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "a7t_W6SsklFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "For both tasks, I used small GloVe and word2vec word vector dataset, as well as Fasttext English word vector 1M dataset. \\\n",
        "Small GloVe: https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM \\\n",
        "Small word2vec: https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB \\\n",
        "Fasttext 1M: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\\\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "ph_Lvq6pk6pi",
        "colab_type": "code",
        "outputId": "93956c41-5d05-4149-b736-1d1c6dfec806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import tensorflow as tf\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "import functools as ft\n",
        "import os\n",
        "import io\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "MU_VLgQDCHli",
        "colab_type": "code",
        "outputId": "e09f3dc7-5307-4701-91c6-85f412c3b7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-MEN-TR-3k.txt\t\t     fasttext.bin\n",
            "EN-MTurk-287.txt\t\t     sample_data\n",
            "EN-RG-65.txt\t\t\t     SIF\n",
            "EN-RW-STANFORD.txt\t\t     small_glove.txt\n",
            "EN-SIMLEX-999.txt\t\t     small_word2vec.txt\n",
            "EN-SimVerb-3500.txt\t\t     wiki-news-300d-1M.vec\n",
            "enwiki-20150602-words-frequency.txt  wiki-news-300d-1M.vec.zip\n",
            "EN-WS-353-ALL.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "URfoVDwtlD7N",
        "colab_type": "code",
        "outputId": "4503c530-ac61-45eb-e2ff-3cec283b6535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM # download a small subset of glove\n",
        "!gdown https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB # download a small subset of word2vec\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM\n",
            "To: /content/small_glove.txt\n",
            "333MB [00:02, 165MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB\n",
            "To: /content/small_word2vec.txt\n",
            "267MB [00:01, 159MB/s]\n",
            "sample_data  small_glove.txt  small_word2vec.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UFwPOpDGW0gn",
        "colab_type": "code",
        "outputId": "9d9ad514-7bcf-4eac-d14a-e39fce4996dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh\n",
            "To: /content/fasttext.bin\n",
            "2.42GB [00:55, 43.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-4CYJTO-dew0",
        "colab_type": "code",
        "outputId": "9abb17fe-c8e0-4682-9783-db909e86a01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5090
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(list(fasttext2.vocab)))\n",
        "print(list(fasttext2.vectors)[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1999995\n",
            "[array([-2.820e-02, -5.570e-02, -4.510e-02, -4.340e-02,  7.120e-02,\n",
            "       -8.550e-02, -1.085e-01, -5.610e-02, -4.523e-01, -2.020e-02,\n",
            "        9.750e-02,  1.047e-01,  1.962e-01, -6.930e-02,  2.130e-02,\n",
            "       -2.350e-02,  1.336e-01, -4.200e-02, -5.640e-02, -7.980e-02,\n",
            "        4.240e-02, -4.090e-02, -5.360e-02, -2.520e-02,  1.350e-02,\n",
            "        6.400e-03,  1.235e-01,  4.610e-02,  1.200e-02, -3.720e-02,\n",
            "        6.500e-02,  4.100e-03, -1.074e-01, -2.630e-02,  1.133e-01,\n",
            "       -2.900e-03,  6.710e-02,  1.065e-01,  2.340e-02, -1.600e-02,\n",
            "        7.000e-03,  4.355e-01, -7.520e-02, -4.328e-01,  4.570e-02,\n",
            "        6.040e-02, -7.400e-02, -5.500e-03, -8.900e-03, -2.926e-01,\n",
            "       -5.450e-02, -1.519e-01,  9.900e-02, -1.930e-02, -5.000e-03,\n",
            "        5.110e-02,  4.040e-02,  1.023e-01, -1.280e-02,  4.880e-02,\n",
            "       -1.567e-01, -7.590e-02, -1.900e-02,  1.442e-01,  4.700e-03,\n",
            "       -1.860e-02,  1.400e-02, -3.850e-02, -8.530e-02,  1.572e-01,\n",
            "        1.770e-01,  8.400e-03, -2.500e-02, -1.145e-01, -6.630e-02,\n",
            "       -1.244e-01, -3.977e-01, -1.240e-02, -4.586e-01, -2.200e-02,\n",
            "        5.746e-01,  2.180e-02, -7.540e-02,  9.900e-03,  3.970e-02,\n",
            "       -1.540e-02,  4.240e-02, -1.500e-02, -1.600e-03,  3.050e-02,\n",
            "        1.010e-02,  2.266e-01,  1.394e-01,  1.890e-02,  6.900e-03,\n",
            "        3.940e-02,  3.550e-02, -1.110e-02, -6.870e-02, -7.800e-03,\n",
            "        2.240e-02,  8.170e-02, -1.949e-01,  1.000e-04,  4.047e-01,\n",
            "       -2.370e-02, -6.560e-02, -6.840e-02,  2.330e-02,  4.380e-02,\n",
            "        1.203e-01, -2.760e-02,  4.160e-02,  1.140e-02, -4.529e-01,\n",
            "        1.538e-01,  1.323e-01, -1.860e-02, -9.140e-02, -3.120e-02,\n",
            "        1.051e-01,  2.120e-02,  7.980e-02, -1.040e-02, -2.060e-02,\n",
            "       -2.500e-03,  4.300e-03, -3.780e-02,  2.689e-01,  7.470e-02,\n",
            "       -4.180e-02, -4.800e-03, -3.870e-02,  4.320e-02,  1.704e-01,\n",
            "        6.140e-02,  9.050e-02, -4.360e-02, -1.410e-02, -3.150e-02,\n",
            "        2.760e-02,  1.510e-02, -1.030e-02, -2.660e-02, -5.120e-02,\n",
            "       -4.080e-02, -6.510e-02,  6.620e-02, -9.360e-02,  1.371e-01,\n",
            "        4.580e-02, -1.366e-01, -7.500e-03, -1.040e-02, -7.320e-02,\n",
            "        1.205e-01,  1.035e-01,  1.060e-02, -3.170e-02, -3.160e-02,\n",
            "        6.639e-01, -2.200e-03, -1.343e-01,  1.440e-02, -3.380e-02,\n",
            "        3.400e-03, -4.290e-02, -8.210e-02,  3.700e-03,  1.029e-01,\n",
            "       -2.040e-02, -2.690e-02,  5.200e-03, -1.034e-01,  1.068e-01,\n",
            "        1.210e-02,  9.800e-02, -4.580e-02,  1.990e-02, -1.320e-02,\n",
            "        1.936e-01, -2.130e-02,  2.090e-02, -2.500e-03,  4.160e-02,\n",
            "       -3.370e-02,  5.160e-02, -1.014e-01,  2.030e-02,  1.980e-02,\n",
            "       -3.050e-02, -3.130e-02,  5.430e-02, -1.060e-02,  1.441e-01,\n",
            "       -1.780e-02, -6.270e-02,  4.750e-02,  3.520e-02, -2.540e-02,\n",
            "       -9.490e-02,  4.010e-02,  3.170e-02,  5.500e-03, -5.360e-02,\n",
            "        1.910e-02, -5.110e-02, -4.090e-02, -3.000e-03,  1.582e-01,\n",
            "        1.080e-02,  5.237e-01,  4.360e-02,  3.060e-02, -3.920e-02,\n",
            "        1.770e-02,  6.900e-03,  6.050e-02,  1.206e-01, -2.160e-02,\n",
            "       -6.330e-02, -2.965e-01,  5.210e-02, -1.500e-02, -2.207e-01,\n",
            "       -6.420e-02, -9.060e-02, -1.210e-02,  5.690e-02,  9.440e-02,\n",
            "       -6.520e-02, -1.080e-02, -4.770e-02,  2.300e-03,  7.700e-03,\n",
            "       -1.547e-01,  4.630e-02,  6.980e-02, -3.760e-02, -2.910e-02,\n",
            "        3.300e-03, -1.020e-02, -7.430e-02,  8.500e-03,  8.050e-02,\n",
            "       -2.910e-02, -6.740e-02, -5.860e-02, -6.530e-02,  2.830e-02,\n",
            "       -2.550e-02,  8.690e-02, -8.680e-02,  9.000e-03,  3.245e-01,\n",
            "       -5.730e-02, -2.890e-02,  4.700e-02, -1.170e-02,  1.740e-02,\n",
            "        1.320e-02, -2.260e-02, -6.640e-02,  1.880e-02,  2.630e-02,\n",
            "        1.110e-02, -4.900e-03, -6.560e-02,  2.950e-02,  4.350e-02,\n",
            "        2.900e-02,  1.163e-01,  4.480e-02, -1.139e-01, -5.530e-02,\n",
            "       -5.280e-02,  1.745e-01, -1.460e-02, -1.308e-01, -6.070e-02,\n",
            "       -1.340e-02,  7.810e-02,  3.780e-02,  2.280e-02, -7.280e-02,\n",
            "       -5.900e-03,  1.580e-02, -1.410e-02, -2.000e-04,  1.930e-02,\n",
            "       -1.480e-02, -4.630e-02,  4.440e-02,  3.034e-01,  1.020e-01,\n",
            "       -8.710e-02,  3.170e-02, -3.700e-02, -7.250e-02, -4.200e-03],\n",
            "      dtype=float32), array([ 2.310e-02,  1.700e-02,  1.570e-02, -7.730e-02,  1.088e-01,\n",
            "        3.100e-03, -1.487e-01, -2.672e-01, -3.570e-02, -4.870e-02,\n",
            "        8.070e-02,  1.532e-01, -7.390e-02, -2.910e-02, -4.450e-02,\n",
            "       -1.400e-03,  1.014e-01,  1.860e-02, -2.530e-02,  2.000e-02,\n",
            "       -2.600e-03, -1.790e-02,  5.000e-04,  5.400e-03, -1.340e-02,\n",
            "        2.330e-02, -7.550e-02, -1.560e-02,  4.150e-02, -4.985e-01,\n",
            "        4.100e-02, -6.160e-02,  4.700e-03,  3.250e-02, -1.620e-02,\n",
            "       -1.720e-02,  9.880e-02,  7.660e-02, -7.960e-02, -3.450e-02,\n",
            "        1.240e-02, -1.007e-01, -2.920e-02, -7.620e-02, -1.261e-01,\n",
            "       -5.310e-02,  4.240e-02,  1.440e-02, -6.830e-02,  2.859e-01,\n",
            "        3.990e-02,  2.010e-02,  3.240e-01, -6.560e-02, -4.970e-02,\n",
            "        9.000e-03,  9.020e-02, -1.380e-02, -4.120e-02, -2.970e-02,\n",
            "        3.139e-01, -1.428e-01,  1.660e-02, -2.190e-02, -5.750e-02,\n",
            "        1.359e-01, -1.655e-01,  1.900e-03,  3.230e-02, -1.300e-03,\n",
            "       -3.033e-01, -9.100e-03,  1.462e-01,  1.860e-01, -5.240e-02,\n",
            "        1.886e-01, -7.372e-01, -2.480e-02, -2.050e-02,  2.200e-03,\n",
            "        5.988e-01, -3.590e-02, -2.690e-02, -4.830e-02,  1.090e-02,\n",
            "       -4.400e-03,  5.920e-02,  1.740e-02,  1.000e-03, -1.200e-03,\n",
            "       -2.510e-02,  4.620e-01, -4.430e-02, -3.500e-02,  1.150e-02,\n",
            "        1.496e-01,  3.125e-01, -9.100e-03,  2.517e-01,  6.540e-02,\n",
            "        2.370e-02, -4.320e-02,  9.520e-02,  6.500e-02, -2.932e-01,\n",
            "        6.300e-02,  2.360e-02,  3.400e-02, -1.200e-03,  8.890e-02,\n",
            "       -6.000e-04, -1.736e-01,  3.740e-02,  3.130e-02, -6.184e-01,\n",
            "        2.820e-02, -3.836e-01,  5.890e-02,  2.443e-01,  6.020e-02,\n",
            "        5.700e-03, -3.800e-03,  1.352e-01,  5.300e-03,  1.930e-02,\n",
            "       -2.130e-02,  2.480e-02,  2.140e-02,  2.334e-01, -4.380e-02,\n",
            "        5.270e-02,  2.620e-02,  6.550e-02, -8.590e-02,  2.642e-01,\n",
            "       -3.930e-02, -1.630e-02,  6.810e-02, -1.750e-02, -1.158e-01,\n",
            "        9.500e-02,  4.750e-02,  6.900e-03,  5.164e-01, -2.600e-03,\n",
            "       -2.550e-02, -8.010e-02, -2.620e-02,  1.113e-01,  7.980e-02,\n",
            "       -1.500e-03,  2.520e-02, -3.790e-02, -2.600e-02, -2.820e-02,\n",
            "       -4.200e-02,  4.820e-02, -1.750e-02,  2.820e-02,  4.000e-02,\n",
            "        3.998e-01, -1.054e-01,  7.550e-02,  1.027e-01, -1.990e-02,\n",
            "        3.810e-02, -3.330e-02, -3.420e-02,  2.670e-02,  8.650e-02,\n",
            "        2.400e-03, -9.100e-03,  1.630e-02, -2.870e-02,  3.640e-02,\n",
            "       -2.020e-02, -3.670e-02, -3.560e-02, -6.140e-02, -5.510e-02,\n",
            "        2.649e-01, -3.710e-02,  2.070e-02,  3.640e-02,  5.120e-02,\n",
            "       -8.430e-02, -1.380e-02,  7.100e-02,  8.430e-02,  2.910e-02,\n",
            "       -1.000e-02,  3.980e-02, -6.460e-02, -5.950e-02, -2.580e-02,\n",
            "       -2.820e-02,  3.110e-02,  1.470e-02, -4.490e-02,  2.760e-02,\n",
            "       -1.168e-01,  2.190e-02, -2.310e-02, -1.620e-02, -2.860e-02,\n",
            "        1.280e-02, -2.590e-02,  1.530e-02,  1.042e-01, -1.207e-01,\n",
            "       -1.350e-02,  5.405e-01, -3.620e-02,  4.760e-02, -1.800e-02,\n",
            "       -7.350e-02,  3.400e-03, -2.600e-03, -5.700e-03,  3.800e-02,\n",
            "       -4.010e-02, -1.016e-01,  3.440e-02,  4.020e-02,  5.130e-02,\n",
            "       -8.150e-02,  3.900e-02,  7.600e-03,  1.750e-02,  3.000e-03,\n",
            "       -7.070e-02,  1.500e-02, -1.174e-01,  2.660e-02, -7.950e-02,\n",
            "        1.988e-01,  9.780e-02, -5.870e-02, -5.330e-02,  2.730e-02,\n",
            "        4.420e-02, -4.630e-02, -7.080e-02,  1.760e-02, -9.940e-02,\n",
            "        8.460e-02,  3.620e-01, -2.070e-02,  2.560e-02, -1.450e-02,\n",
            "        3.090e-02,  8.200e-03,  4.200e-03, -3.140e-02,  1.196e-01,\n",
            "       -3.460e-02,  3.860e-02, -3.680e-02, -3.330e-02, -3.200e-03,\n",
            "       -4.800e-03, -6.000e-04,  5.090e-02, -2.320e-02,  1.183e-01,\n",
            "       -1.314e-01,  1.490e-02,  7.620e-02, -1.610e-02,  1.600e-02,\n",
            "        3.900e-02, -1.922e-01,  3.100e-03, -6.660e-02,  5.930e-02,\n",
            "       -6.210e-02,  4.210e-02,  3.280e-02, -9.010e-02, -1.590e-02,\n",
            "        1.015e-01,  6.164e-01, -6.500e-02,  1.241e-01,  5.900e-03,\n",
            "        6.530e-02, -3.860e-02,  1.660e-02,  4.030e-02,  1.690e-02,\n",
            "       -8.000e-04,  5.200e-03, -3.630e-02, -2.508e-01,  1.252e-01,\n",
            "       -1.008e-01, -3.080e-02,  7.440e-02, -1.118e-01,  9.630e-02],\n",
            "      dtype=float32), array([-8.490e-02, -5.820e-02, -3.210e-02,  4.830e-02, -1.850e-02,\n",
            "       -8.360e-02,  3.550e-02, -4.900e-03,  4.080e-02,  6.200e-02,\n",
            "       -6.910e-02,  1.964e-01,  2.620e-02, -1.129e-01,  3.700e-03,\n",
            "       -4.150e-02,  8.210e-02, -5.870e-02, -6.190e-02, -5.000e-03,\n",
            "       -1.940e-02, -5.570e-02,  3.300e-03,  2.590e-02, -2.000e-02,\n",
            "        4.570e-02, -3.200e-03, -5.490e-02, -7.630e-02,  9.820e-02,\n",
            "       -1.610e-02, -1.230e-02, -8.010e-02, -4.770e-02,  5.590e-02,\n",
            "        4.100e-03, -4.400e-02,  4.470e-02, -7.250e-02,  1.337e-01,\n",
            "        3.100e-03,  2.260e-02, -2.380e-02, -5.231e-01, -9.610e-02,\n",
            "       -4.470e-02,  8.530e-02,  4.000e-02, -2.980e-02, -2.125e-01,\n",
            "       -3.170e-02,  1.255e-01,  1.059e-01, -3.360e-02, -3.430e-02,\n",
            "        1.063e-01,  5.000e-03,  2.630e-01,  2.400e-03, -9.020e-02,\n",
            "       -4.640e-02,  1.566e-01,  2.300e-03, -1.340e-02, -4.300e-02,\n",
            "       -1.372e-01,  2.770e-02,  6.290e-02,  2.930e-02,  2.234e-01,\n",
            "       -4.412e-01, -2.270e-02, -1.240e-02,  1.725e-01, -1.077e-01,\n",
            "       -1.300e-03, -9.133e-01,  9.180e-02, -7.310e-02, -1.920e-02,\n",
            "        3.905e-01, -1.510e-02, -2.500e-02,  3.430e-02, -5.800e-02,\n",
            "       -4.000e-02,  7.490e-02,  4.300e-03, -3.960e-02, -4.850e-02,\n",
            "        2.590e-02,  2.128e-01,  1.130e-02,  7.100e-03, -1.450e-02,\n",
            "        1.590e-02,  1.943e-01,  6.000e-03, -5.600e-03, -9.360e-02,\n",
            "       -1.750e-02,  4.530e-02,  1.851e-01,  9.650e-02,  7.260e-02,\n",
            "        1.010e-02,  6.100e-02,  7.410e-02, -3.100e-03,  2.374e-01,\n",
            "        2.190e-02, -3.550e-01,  2.410e-02, -2.510e-02, -6.988e-01,\n",
            "       -6.590e-02,  1.613e-01,  2.290e-02, -1.041e-01, -4.390e-02,\n",
            "       -1.282e-01, -6.370e-02,  2.910e-02,  8.400e-03,  7.250e-02,\n",
            "        2.300e-03,  3.480e-02,  5.200e-02,  3.231e-01,  8.310e-02,\n",
            "       -8.120e-02, -5.850e-02,  2.590e-02, -8.390e-02, -1.581e-01,\n",
            "        2.290e-02, -1.054e-01, -4.470e-02,  7.000e-04,  1.580e-01,\n",
            "       -2.500e-02, -5.200e-02,  2.120e-02,  2.872e-01,  2.540e-02,\n",
            "       -8.910e-02,  1.588e-01, -5.800e-03,  1.450e-02,  6.510e-02,\n",
            "       -1.120e-02, -2.340e-02,  1.850e-02, -2.010e-02, -1.141e-01,\n",
            "       -9.280e-02, -1.534e-01,  9.620e-02,  2.830e-02, -1.450e-02,\n",
            "        7.386e-01, -5.500e-02, -1.209e-01, -1.235e-01, -1.330e-02,\n",
            "       -3.820e-02, -6.900e-03, -1.459e-01, -8.620e-02, -6.220e-02,\n",
            "       -3.640e-02, -5.780e-02,  1.219e-01, -1.340e-02,  1.200e-01,\n",
            "       -1.890e-02,  1.950e-02,  8.610e-02,  7.200e-02,  4.420e-02,\n",
            "       -1.197e-01, -3.420e-02,  2.070e-02, -3.400e-03,  2.890e-02,\n",
            "        5.470e-02,  8.840e-02,  4.170e-02,  3.180e-02,  9.300e-02,\n",
            "       -6.180e-02, -3.500e-02,  4.400e-02, -1.528e-01,  1.190e-02,\n",
            "        4.900e-03, -8.360e-02,  5.100e-03,  2.500e-03, -2.000e-03,\n",
            "       -9.840e-02, -1.070e-02,  1.650e-02,  6.450e-02,  5.370e-02,\n",
            "        4.900e-02, -3.470e-02, -1.648e-01,  2.460e-02,  1.209e-01,\n",
            "       -4.320e-02,  5.224e-01,  8.860e-02,  5.310e-02,  1.010e-02,\n",
            "       -4.630e-02,  3.660e-02,  2.530e-02,  1.061e-01, -9.400e-02,\n",
            "        3.280e-02, -4.222e-01,  4.640e-02, -4.450e-02, -9.590e-02,\n",
            "       -3.450e-02,  4.840e-02, -1.010e-02,  6.940e-02,  1.050e-02,\n",
            "       -6.320e-02,  2.330e-02,  4.000e-03,  4.870e-02,  5.900e-03,\n",
            "        1.483e-01, -5.000e-04,  1.610e-02, -2.260e-02, -2.270e-02,\n",
            "        3.700e-03, -7.440e-02, -1.320e-02, -6.740e-02, -4.370e-02,\n",
            "       -3.200e-03, -8.350e-01,  4.160e-02, -2.220e-02, -4.300e-02,\n",
            "        7.210e-02, -3.850e-02, -7.790e-02,  8.060e-02, -5.530e-02,\n",
            "        7.290e-02, -5.900e-02, -1.600e-02, -1.271e-01,  4.470e-02,\n",
            "       -4.180e-02, -4.550e-02, -5.500e-03,  5.620e-02,  9.920e-02,\n",
            "        8.950e-02, -5.790e-02, -5.430e-02, -8.090e-02,  1.197e-01,\n",
            "        6.600e-03,  1.038e-01, -3.150e-02, -3.050e-02, -6.070e-02,\n",
            "       -9.200e-02,  6.010e-02, -1.285e-01, -5.050e-02, -4.200e-02,\n",
            "        1.033e-01, -4.818e-01, -3.300e-03, -1.020e-02,  5.000e-03,\n",
            "        5.650e-02, -1.600e-02,  2.300e-03, -9.280e-02, -1.318e-01,\n",
            "        1.330e-02,  3.440e-02,  1.860e-02, -4.995e-01, -1.300e-02,\n",
            "        6.330e-02, -3.300e-02,  3.200e-03, -2.370e-02, -3.660e-02],\n",
            "      dtype=float32), array([-1.081e-01,  1.910e-02,  3.540e-02,  1.270e-02,  6.640e-02,\n",
            "       -1.260e-02, -1.882e-01,  6.310e-02, -2.306e-01,  9.500e-03,\n",
            "        9.170e-02,  1.513e-01,  5.580e-02, -6.430e-02, -2.880e-02,\n",
            "       -4.470e-02,  1.603e-01,  6.130e-02,  3.490e-02, -5.780e-02,\n",
            "        3.000e-04, -1.399e-01,  1.630e-02, -4.190e-02, -4.870e-02,\n",
            "        5.700e-03,  1.700e-03, -2.680e-02, -1.700e-02,  1.045e-01,\n",
            "        8.150e-02, -6.000e-03,  6.350e-02, -1.710e-01,  2.760e-02,\n",
            "       -2.300e-02,  8.240e-02,  3.040e-02, -1.595e-01,  8.510e-02,\n",
            "       -5.560e-02,  3.290e-02, -9.210e-02,  1.960e-02, -4.040e-02,\n",
            "        3.610e-02, -8.430e-02,  1.014e-01, -3.930e-02, -1.805e-01,\n",
            "       -7.300e-03,  2.247e-01, -6.210e-02, -5.750e-02, -3.160e-02,\n",
            "        1.980e-02,  6.020e-02,  1.312e-01, -1.278e-01,  1.770e-02,\n",
            "       -6.000e-02, -2.904e-01, -4.650e-02, -1.078e-01, -7.010e-02,\n",
            "       -4.970e-02, -1.102e-01,  1.560e-02, -5.950e-02,  7.620e-02,\n",
            "        2.638e-01,  3.530e-02,  7.450e-02,  3.610e-02, -5.610e-02,\n",
            "       -1.179e-01, -4.751e-01,  4.580e-02, -6.720e-02, -2.200e-03,\n",
            "        9.264e-01,  1.010e-02, -1.085e-01,  6.690e-02, -4.170e-02,\n",
            "       -4.280e-02,  3.910e-02, -5.810e-02,  1.070e-02, -8.730e-02,\n",
            "       -1.600e-03,  5.711e-01,  3.450e-02,  9.300e-02, -6.470e-02,\n",
            "       -8.200e-02,  5.870e-02, -1.307e-01,  4.800e-02,  2.260e-02,\n",
            "        1.150e-02,  2.960e-02, -1.675e-01,  3.990e-02,  2.511e-01,\n",
            "        4.230e-02,  1.299e-01, -3.360e-02, -9.770e-02,  1.460e-02,\n",
            "        3.527e-01,  1.220e-02,  1.031e-01, -6.570e-02, -3.704e-01,\n",
            "        4.450e-02,  1.242e-01, -3.100e-03,  2.550e-02, -1.090e-02,\n",
            "        1.575e-01,  2.880e-02,  2.549e-01, -2.800e-02,  4.620e-02,\n",
            "       -2.820e-02, -1.540e-02,  4.050e-02,  3.191e-01,  1.130e-02,\n",
            "       -1.360e-02,  1.820e-02, -3.850e-02, -4.990e-02, -8.100e-02,\n",
            "       -1.340e-02, -1.413e-01,  5.660e-02, -4.230e-02, -6.090e-02,\n",
            "        1.063e-01,  3.860e-02, -3.750e-02,  2.714e-01, -6.650e-02,\n",
            "       -2.190e-02,  5.790e-02,  5.500e-02,  7.720e-02,  2.024e-01,\n",
            "       -3.650e-02, -7.640e-02,  3.180e-02,  4.000e-04, -4.240e-02,\n",
            "       -5.790e-02,  4.280e-02,  2.110e-02, -6.100e-03,  2.430e-02,\n",
            "        5.702e-01,  7.980e-02, -5.020e-02, -1.950e-02,  2.420e-02,\n",
            "       -3.390e-02, -4.410e-02, -1.095e-01,  4.260e-02, -2.340e-02,\n",
            "       -5.700e-03, -2.090e-02, -9.030e-02, -6.540e-02,  1.559e-01,\n",
            "        5.100e-03,  3.700e-03, -8.000e-04, -5.000e-03,  5.080e-02,\n",
            "        1.050e-02,  2.510e-02,  2.700e-02,  3.900e-03,  6.210e-02,\n",
            "        5.190e-02, -1.560e-02, -1.285e-01, -5.640e-02, -2.630e-02,\n",
            "        4.010e-02,  3.720e-02, -1.040e-01, -6.740e-02,  8.660e-02,\n",
            "       -3.300e-03, -7.600e-03,  6.400e-03, -1.140e-02, -2.390e-02,\n",
            "        1.350e-02,  8.010e-02,  1.710e-02,  4.250e-02, -4.990e-02,\n",
            "        6.150e-02,  3.550e-02, -8.330e-02,  3.780e-02,  2.880e-02,\n",
            "       -3.920e-02,  5.679e-01, -1.200e-02,  6.990e-02, -8.930e-02,\n",
            "       -1.050e-02,  2.190e-02, -1.360e-02,  1.063e-01, -1.400e-03,\n",
            "       -6.200e-03, -6.170e-02,  3.570e-02,  3.000e-03, -4.140e-02,\n",
            "       -6.340e-02,  7.010e-02,  5.010e-02,  2.410e-02,  3.790e-02,\n",
            "       -2.340e-02, -1.160e-02,  3.910e-02, -1.690e-02, -1.900e-03,\n",
            "        7.310e-02,  5.970e-02,  1.910e-02, -2.620e-02, -2.560e-02,\n",
            "        6.310e-02, -2.610e-02,  1.850e-02,  1.000e-02,  1.126e-01,\n",
            "       -4.870e-02,  1.545e-01,  4.380e-02,  1.470e-02, -2.780e-02,\n",
            "       -4.930e-02, -8.660e-02, -2.810e-02, -9.550e-02,  8.790e-02,\n",
            "        1.940e-02, -3.230e-02, -1.260e-02, -8.100e-02,  8.800e-03,\n",
            "        2.000e-04,  5.900e-03,  8.580e-02,  2.400e-03,  4.010e-02,\n",
            "       -2.050e-02,  3.040e-02,  6.780e-02,  5.660e-02,  9.700e-03,\n",
            "        3.900e-03, -9.680e-02,  7.900e-03, -3.838e-01, -6.420e-02,\n",
            "       -4.170e-02, -6.660e-02,  3.370e-02,  5.300e-03, -2.630e-02,\n",
            "       -3.100e-02,  2.002e-01, -9.600e-03,  7.960e-02,  7.430e-02,\n",
            "        7.090e-02, -7.700e-03, -1.690e-02, -8.300e-03, -3.220e-02,\n",
            "        6.080e-02,  2.270e-02, -3.060e-02,  3.945e-01,  2.000e-02,\n",
            "       -6.130e-02, -6.110e-02,  1.104e-01,  4.750e-02, -5.990e-02],\n",
            "      dtype=float32), array([-1.750e-02, -2.189e-01,  3.530e-02,  3.450e-02,  9.130e-02,\n",
            "        2.690e-02, -1.670e-01, -2.759e-01,  4.283e-01,  3.490e-02,\n",
            "        1.320e-02,  1.078e-01, -5.580e-02, -2.150e-02, -3.890e-02,\n",
            "        3.790e-02, -1.180e-01,  1.640e-02,  6.310e-02,  4.590e-02,\n",
            "        3.310e-02, -1.760e-02,  1.000e-03,  1.760e-02,  3.510e-02,\n",
            "       -5.450e-02,  4.300e-02, -3.000e-02,  3.580e-02,  2.588e-01,\n",
            "        5.760e-02, -8.540e-02,  6.320e-02,  2.550e-02,  8.000e-04,\n",
            "        1.520e-02, -8.430e-02, -2.490e-02,  6.510e-02, -9.610e-02,\n",
            "       -1.070e-02,  1.649e-01,  3.000e-04, -2.875e-01, -3.880e-02,\n",
            "       -2.120e-02, -9.730e-02,  3.010e-02, -4.740e-02,  9.173e-01,\n",
            "       -2.400e-03,  4.422e-01,  1.367e-01, -1.770e-02, -5.910e-02,\n",
            "       -6.900e-03,  6.920e-02, -3.956e-01, -4.860e-02,  2.510e-02,\n",
            "        1.376e-01,  1.816e-01, -1.640e-02, -2.103e-01,  1.560e-02,\n",
            "       -4.490e-02, -1.004e-01,  3.990e-02, -8.470e-02, -5.650e-02,\n",
            "       -2.697e-01,  5.100e-03, -4.410e-02,  1.862e-01, -1.430e-02,\n",
            "       -1.114e-01, -7.036e-01,  1.620e-02,  1.955e-01, -2.250e-02,\n",
            "        6.761e-01,  2.950e-02, -3.900e-03, -4.500e-02, -3.830e-02,\n",
            "        2.580e-02,  4.640e-02, -2.430e-02,  2.430e-02,  3.700e-03,\n",
            "        3.100e-03,  6.531e-01,  3.300e-02,  9.200e-03,  2.600e-03,\n",
            "        3.510e-02, -3.749e-01, -4.020e-02, -2.596e-01,  3.260e-02,\n",
            "        5.500e-03, -3.900e-03,  1.751e-01,  4.200e-02,  1.438e-01,\n",
            "       -1.090e-02,  2.360e-02, -8.400e-03, -1.880e-02,  1.287e-01,\n",
            "        6.240e-02,  1.152e-01,  2.440e-02,  2.070e-02, -3.837e-01,\n",
            "        2.028e-01,  1.272e-01,  1.790e-02,  1.020e-01,  4.330e-02,\n",
            "       -6.500e-02,  2.810e-02,  1.151e-01, -7.400e-03,  1.740e-02,\n",
            "       -2.520e-02,  5.330e-02,  6.590e-02,  2.928e-01, -2.110e-02,\n",
            "       -2.520e-02, -4.000e-04, -4.620e-02, -6.150e-02,  3.430e-01,\n",
            "       -7.050e-02, -9.380e-02,  5.090e-02,  7.600e-03, -2.799e-01,\n",
            "        1.191e-01,  6.060e-02, -8.900e-03,  1.064e-01, -8.290e-02,\n",
            "       -4.400e-03, -1.018e-01, -1.000e-02,  7.650e-02,  1.046e-01,\n",
            "        2.960e-02, -1.440e-02, -2.410e-02,  2.030e-02, -1.770e-02,\n",
            "       -3.470e-02,  2.440e-02,  4.400e-03,  6.000e-03,  1.260e-02,\n",
            "        4.667e-01,  1.586e-01, -3.760e-02, -5.660e-02, -3.870e-02,\n",
            "        1.080e-02, -8.300e-03, -4.980e-02,  1.210e-02, -4.580e-02,\n",
            "        4.300e-02,  2.700e-03, -9.220e-02,  1.590e-02,  7.700e-02,\n",
            "       -6.380e-02,  4.350e-02, -1.520e-02,  1.800e-03, -2.400e-03,\n",
            "       -2.509e-01, -1.970e-02,  9.700e-03,  2.140e-02,  6.030e-02,\n",
            "       -1.050e-02, -9.400e-02, -4.068e-01, -2.884e-01,  4.450e-02,\n",
            "        2.000e-03,  1.260e-02, -7.440e-02, -6.780e-02,  4.830e-02,\n",
            "       -9.190e-02,  4.310e-02, -4.010e-02,  3.130e-02, -2.020e-02,\n",
            "        2.970e-02,  1.000e-03,  2.240e-02,  5.750e-02,  3.490e-02,\n",
            "       -1.030e-02,  1.500e-03,  1.220e-02,  4.150e-02, -2.559e-01,\n",
            "        1.320e-02,  5.005e-01, -7.400e-03,  4.160e-02, -8.390e-02,\n",
            "        8.600e-03, -1.550e-02, -8.900e-03, -7.600e-03,  2.950e-02,\n",
            "       -2.770e-02, -9.420e-02,  2.360e-02,  1.530e-02,  7.980e-02,\n",
            "       -4.750e-02, -4.310e-02,  3.690e-02,  3.960e-02,  1.460e-02,\n",
            "        7.900e-02,  7.160e-02,  3.340e-02, -4.620e-02,  1.210e-02,\n",
            "        6.200e-02,  9.600e-02, -3.370e-02, -6.170e-02,  4.160e-02,\n",
            "       -2.360e-02,  5.400e-03, -1.031e-01, -3.310e-02, -3.805e-01,\n",
            "       -7.500e-02,  4.056e-01,  2.090e-02,  1.060e-02, -3.440e-02,\n",
            "        1.620e-02, -1.616e-01,  3.560e-02, -1.080e-02,  1.220e-01,\n",
            "        4.840e-02,  3.530e-02,  4.620e-02, -5.300e-03,  1.450e-02,\n",
            "        6.150e-02,  7.100e-03, -2.410e-02, -3.270e-02,  7.100e-03,\n",
            "       -1.290e-01,  2.020e-02,  3.450e-02,  2.750e-02, -1.930e-02,\n",
            "       -3.300e-03, -4.104e-01,  5.050e-02, -1.699e-01, -3.980e-02,\n",
            "       -9.900e-03, -3.296e-01, -4.020e-02,  1.344e-01,  1.000e-03,\n",
            "        3.360e-02,  1.412e-01,  2.300e-03, -5.560e-02,  3.480e-02,\n",
            "       -6.250e-02, -2.280e-02, -1.500e-03,  7.320e-02,  3.620e-02,\n",
            "        6.420e-02,  1.360e-02,  3.670e-02,  1.735e-01,  2.170e-02,\n",
            "        5.910e-02,  1.660e-02, -2.846e-01,  5.090e-02,  2.290e-02],\n",
            "      dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KlY6uIVFpGM7",
        "colab_type": "code",
        "outputId": "1fc277d9-1b50-4d97-df31-ab793dda9c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 14:33:18--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.24.185\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.24.185|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  62.3MB/s    in 10s     \n",
            "\n",
            "2019-01-30 14:33:29 (63.5 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n",
            "sample_data\t small_word2vec.txt\twiki-news-300d-1M.vec.zip\n",
            "small_glove.txt  wiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W_gGAHK2Rmt_",
        "colab_type": "code",
        "outputId": "30e39788-0c34-4ad2-d6ff-64b4ba553d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!du -h wiki-news-300d-1M.vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2G\twiki-news-300d-1M.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pvLMJq5YTflf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Fasttext, small GloVe and small word2vec data"
      ]
    },
    {
      "metadata": {
        "id": "2YnZZtKhvs6Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xZ2vGm_H5RI",
        "colab_type": "code",
        "outputId": "35a6a492-9cad-4582-ee5f-7634f29eddd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fasttext2 = KeyedVectors.load_word2vec_format('/content/' + 'fasttext.bin', binary=True)\n",
        "print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The fasttext embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3CnjAlvyeXFt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fasttext = KeyedVectors.load_word2vec_format('/content/' + 'wiki-news-300d-1M.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UOIQBmZJxPZe",
        "colab_type": "code",
        "outputId": "1df61104-cbd3-43cb-fe13-eff8878314ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m gensim.scripts.glove2word2vec -i small_glove.txt -o small_glove_w2v.txt\n",
        "!python -m gensim.scripts.glove2word2vec -i small_word2vec.txt -o small_w2v_w2v.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2019-01-30 14:55:36,843 - glove2word2vec - INFO - running /usr/local/lib/python3.6/dist-packages/gensim/scripts/glove2word2vec.py -i small_glove.txt -o small_glove_w2v.txt\n",
            "2019-01-30 14:55:37,191 - glove2word2vec - INFO - converting 128607 vectors from small_glove.txt to small_glove_w2v.txt\n",
            "2019-01-30 14:55:38,871 - glove2word2vec - INFO - Converted model with 128607 vectors and 300 dimensions\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'gensim.scripts.glove2word2vec' found in sys.modules after import of package 'gensim.scripts', but prior to execution of 'gensim.scripts.glove2word2vec'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2019-01-30 14:55:40,147 - glove2word2vec - INFO - running /usr/local/lib/python3.6/dist-packages/gensim/scripts/glove2word2vec.py -i small_word2vec.txt -o small_w2v_w2v.txt\n",
            "2019-01-30 14:55:40,359 - glove2word2vec - INFO - converting 76078 vectors from small_word2vec.txt to small_w2v_w2v.txt\n",
            "2019-01-30 14:55:41,663 - glove2word2vec - INFO - Converted model with 76078 vectors and 300 dimensions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "maoP-rf9ZX29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove = KeyedVectors.load_word2vec_format('/content/' + 'small_glove_w2v.txt')\n",
        "w2v = KeyedVectors.load_word2vec_format('/content/' + 'small_w2v_w2v.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGxhPVpSwWYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Post-processing with CN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YQit52KnT-G7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Use this func for data size smaller than 1M\n",
        "def cn_mat(pre_cn_f_name, alpha):\n",
        "  pre_cn_data = eval(pre_cn_f_name)\n",
        "  #word_pairs = set(list(cn_data.keys()))\n",
        "  cn_mat = pre_cn_data.vectors\n",
        "  word_vec = np.array(cn_mat, dtype = float).T\n",
        "  num_word = word_vec.shape[1]\n",
        "  num_vec = word_vec.shape[0]\n",
        "  print(num_word, num_vec)\n",
        "  corr_mat = word_vec.dot(word_vec.T) /num_word\n",
        "  print('got corr_mat')\n",
        "  concept_mat = corr_mat @ np.linalg.inv(corr_mat + alpha ** (-2) * np.eye(num_vec))\n",
        "  print('got concep_mat')\n",
        "  new_mat = ((np.eye(num_vec)-concept_mat)@word_vec).T\n",
        "  print('got new_mat')\n",
        "  return new_mat\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ud3gBoypeSvV",
        "colab_type": "code",
        "outputId": "142ce16d-fc38-488e-9a03-6f0efa37ab59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "cell_type": "code",
      "source": [
        "cn_fasttext_mat = cn_mat('fasttext', alpha = 2)\n",
        "print('CN preprocess done for fasttext data')\n",
        "cn_glove_mat = cn_mat('glove', alpha = 2)\n",
        "print('CN preprocess done for glove data')\n",
        "cn_w2v_mat = cn_mat('w2v', alpha =2)\n",
        "print('CN preprocess done for w2v data')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999994 300\n",
            "got corr_mat\n",
            "got concep_mat\n",
            "got new_mat\n",
            "CN preprocess done for fasttext data\n",
            "128607 300\n",
            "got corr_mat\n",
            "got concep_mat\n",
            "got new_mat\n",
            "CN preprocess done for glove data\n",
            "76078 300\n",
            "got corr_mat\n",
            "got concep_mat\n",
            "got new_mat\n",
            "CN preprocess done for w2v data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nPlGIjbnPljQ",
        "colab_type": "code",
        "outputId": "c6d9116a-55ca-453d-b483-85fad79e00ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-27 15:01:16--  https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23741395 (23M) [text/plain]\n",
            "Saving to: ‘enwiki-20150602-words-frequency.txt’\n",
            "\n",
            "enwiki-20150602-wor 100%[===================>]  22.64M   130MB/s    in 0.2s    \n",
            "\n",
            "2019-01-27 15:01:16 (130 MB/s) - ‘enwiki-20150602-words-frequency.txt’ saved [23741395/23741395]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwxamGOnPv5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wikiWordsPath = '/content/' + 'enwiki-20150602-words-frequency.txt'\n",
        "wikiWords = []\n",
        "with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    for line in f_in:\n",
        "      one_line = line.split(' ')\n",
        "      if int(one_line[1]) > 200:\n",
        "        wikiWords.append(one_line[0]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67ko1fRmHq5U",
        "colab_type": "code",
        "outputId": "7f1f0c48-b7a7-407a-c4f0-b43b7c6e613d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PrincetonML/SIF"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SIF'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 2.80 MiB | 12.46 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IeVid0J7HvHA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wikiWordsPath = '/content/' + '/SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "wikiWords = []\n",
        "\n",
        "with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    for line in f_in:\n",
        "        wikiWords.append(line.split(' ')[0])   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzqdVXQALfwl",
        "colab_type": "code",
        "outputId": "e014e932-51b4-40e3-c251-eb3c0ee9d36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(wikiWords))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "188033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kBV9JQ1JScr1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm, inv, eig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZbjSeupI9KE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reduced_cn_mat(wordVecModel_str, alpha = 1):\n",
        "    # compute the prototype conceptor with alpha = 1\n",
        "    \n",
        "    \n",
        "    wordVecModel = eval(wordVecModel_str)    \n",
        "    word_in_wiki_and_model = set(list(wordVecModel.vocab)).intersection(set(wikiWords))\n",
        "\n",
        "    x_collector_indices = []\n",
        "\n",
        "\n",
        "    for word in word_in_wiki_and_model:\n",
        "        x_collector_indices.append(wordVecModel.vocab[word].index)\n",
        "\n",
        "    # put the word vectors in columns\n",
        "    x_collector = wordVecModel.vectors[x_collector_indices,:].T       \n",
        "   \n",
        "    \n",
        "    nrWords = x_collector.shape[1] # number of total words\n",
        "    print(nrWords)\n",
        "    \n",
        "    R = x_collector.dot(x_collector.T) / nrWords # calculate the correlation matrix\n",
        "    \n",
        "    concept_mat = R @ inv(R + alpha ** (-2) * np.eye(300))# calculate the conceptor matrix\n",
        "    \n",
        "    return concept_mat\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8hw27nTcC4U",
        "colab_type": "code",
        "outputId": "3501d0de-5303-4381-9465-58caf3c4ac36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "fasttext2_concept_mat = reduced_cn_mat('fasttext2', alpha = 1)\n",
        "print('CN preprocess done for fasttext2 data')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119127\n",
            "CN preprocess done for fasttext2 data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wrmIWyKcTUoD",
        "colab_type": "code",
        "outputId": "822738c1-9ecd-428b-c808-16c663c0409d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(fasttext2_concept_mat))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dpmzKwYldGmC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Word similarity evaluation\n",
        "I re-implemented word similarity task by evaluating CN post-processed word vectors with 7 standard word similarity datasets, namely the RG65 (Rubenstein and Goodenough, 1965), the WordSim-353 (WS) (Finkelstein et al., 2002), the rare- words (RW) (Luong, Socher, and Manning, 2013), the MEN dataset (Bruni, Tran, and Baroni, 2014), the MTurk (Radinsky et al., 2011), the SimLex-999 (SimLex) (Hill, Reichart, and Korhonen, 2015), and the SimVerb-3500 (Gerz et al., 2016) \\\n"
      ]
    },
    {
      "metadata": {
        "id": "VzqvIqhQdQKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Load word similarity text data"
      ]
    },
    {
      "metadata": {
        "id": "NMVIlRw8dIXP",
        "colab_type": "code",
        "outputId": "dcf8994d-f3ef-4ecd-83f7-f89b38cb5e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1475
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MEN-TR-3k.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MTurk-287.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RG-65.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RW-STANFORD.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SIMLEX-999.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SimVerb-3500.txt\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-WS-353-ALL.txt\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-27 14:53:40--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MEN-TR-3k.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53593 (52K) [text/plain]\n",
            "Saving to: ‘EN-MEN-TR-3k.txt’\n",
            "\n",
            "EN-MEN-TR-3k.txt    100%[===================>]  52.34K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-01-27 14:53:40 (2.53 MB/s) - ‘EN-MEN-TR-3k.txt’ saved [53593/53593]\n",
            "\n",
            "--2019-01-27 14:53:41--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-MTurk-287.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7218 (7.0K) [text/plain]\n",
            "Saving to: ‘EN-MTurk-287.txt’\n",
            "\n",
            "EN-MTurk-287.txt    100%[===================>]   7.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-27 14:53:41 (105 MB/s) - ‘EN-MTurk-287.txt’ saved [7218/7218]\n",
            "\n",
            "--2019-01-27 14:53:42--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RG-65.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1210 (1.2K) [text/plain]\n",
            "Saving to: ‘EN-RG-65.txt’\n",
            "\n",
            "EN-RG-65.txt        100%[===================>]   1.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-27 14:53:42 (205 MB/s) - ‘EN-RG-65.txt’ saved [1210/1210]\n",
            "\n",
            "--2019-01-27 14:53:43--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-RW-STANFORD.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49851 (49K) [text/plain]\n",
            "Saving to: ‘EN-RW-STANFORD.txt’\n",
            "\n",
            "EN-RW-STANFORD.txt  100%[===================>]  48.68K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-01-27 14:53:43 (2.36 MB/s) - ‘EN-RW-STANFORD.txt’ saved [49851/49851]\n",
            "\n",
            "--2019-01-27 14:53:44--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SIMLEX-999.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18024 (18K) [text/plain]\n",
            "Saving to: ‘EN-SIMLEX-999.txt’\n",
            "\n",
            "EN-SIMLEX-999.txt   100%[===================>]  17.60K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-01-27 14:53:44 (1.78 MB/s) - ‘EN-SIMLEX-999.txt’ saved [18024/18024]\n",
            "\n",
            "--2019-01-27 14:53:44--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-SimVerb-3500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65315 (64K) [text/plain]\n",
            "Saving to: ‘EN-SimVerb-3500.txt’\n",
            "\n",
            "EN-SimVerb-3500.txt 100%[===================>]  63.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-01-27 14:53:45 (3.02 MB/s) - ‘EN-SimVerb-3500.txt’ saved [65315/65315]\n",
            "\n",
            "--2019-01-27 14:53:45--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/wordSimData/EN-WS-353-ALL.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7405 (7.2K) [text/plain]\n",
            "Saving to: ‘EN-WS-353-ALL.txt’\n",
            "\n",
            "EN-WS-353-ALL.txt   100%[===================>]   7.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-27 14:53:45 (96.9 MB/s) - ‘EN-WS-353-ALL.txt’ saved [7405/7405]\n",
            "\n",
            "EN-MEN-TR-3k.txt    EN-SimVerb-3500.txt  small_glove_w2v.txt\n",
            "EN-MTurk-287.txt    EN-WS-353-ALL.txt\t small_w2v_w2v.txt\n",
            "EN-RG-65.txt\t    fasttext.bin\t small_word2vec.txt\n",
            "EN-RW-STANFORD.txt  sample_data\t\t wiki-news-300d-1M.vec\n",
            "EN-SIMLEX-999.txt   small_glove.txt\t wiki-news-300d-1M.vec.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kj07C41RvYgc",
        "colab_type": "code",
        "outputId": "c2b6cd69-e3ab-4223-fbd2-e7f08b3c1a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ipOWZhOTddTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Compare word similarity scores and calculate Spearman Correlation"
      ]
    },
    {
      "metadata": {
        "id": "Z58uFzFAzksh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sim(data_f_name, cn_f_name, cn_mat, alpha):\n",
        "  cn_data = eval(cn_f_name)\n",
        "  #word_pairs = set(list(cn_data.keys()))\n",
        "  fin = io.open(data_f_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  dataset = []\n",
        "  word_vec = []\n",
        "  keys = []\n",
        "  ls_word = list(cn_data.vocab)\n",
        "  #line_num = 0\n",
        "  for line in fin:\n",
        "   # if line_num > 0:\n",
        "      tokens = line.rstrip().split()\n",
        "      if tokens[0] in cn_data.vocab and tokens[1] in cn_data.vocab:\n",
        "        dataset.append(((tokens[0], tokens[1]), float(tokens[2])))\n",
        "        id1 = ls_word.index(tokens[0])\n",
        "        id2 = ls_word.index(tokens[1])\n",
        "        word_vec.append(cn_mat[id1])\n",
        "        word_vec.append(cn_mat[id2])\n",
        "        keys.append(tokens[0])\n",
        "        keys.append(tokens[1])\n",
        "    #line_num +=1\n",
        "  dataset.sort(key = lambda score: -score[1]) #sort based on score\n",
        " # print(cn_data['gem'])\n",
        "  cn_dataset = {}\n",
        "  cn_dataset_list = []\n",
        "  \n",
        "  for ((word1, word2), score) in dataset:\n",
        "    #print(word1, word2)\n",
        "    id1 = ls_word.index(word1)\n",
        "    id2 = ls_word.index(word2)\n",
        "    sim_score = 1 - cosine_similarity(cn_mat[id1].reshape(1,-1), cn_mat[id2].reshape(1,-1))\n",
        "    cn_dataset[(word1, word2)] = sim_score\n",
        "    cn_dataset_list.append(((word1, word2),sim_score))\n",
        "  cn_dataset_list.sort(key = lambda score: score[1])\n",
        "  spearman_list1=[]\n",
        "  spearman_list2=[]\n",
        "  for pos_1, (pair, score_1) in enumerate(dataset):\n",
        "    score_2 = cn_dataset[pair]\n",
        "    pos_2 = cn_dataset_list.index((pair, score_2))\n",
        "    spearman_list1.append(pos_1)\n",
        "    spearman_list2.append(pos_2)\n",
        "  rho = spearmanr(spearman_list1, spearman_list2)\n",
        "  return rho[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "weaV_H04UV8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sim_large_data(data_f_name, cn_f_name, C, alpha):\n",
        "  cn_data = eval(cn_f_name)\n",
        "  #word_pairs = set(list(cn_data.keys()))\n",
        "  fin = io.open(data_f_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  dataset = []\n",
        "  ls_word = list(cn_data.vocab)\n",
        "  #line_num = 0\n",
        "  for line in fin:\n",
        "   # if line_num > 0:\n",
        "      tokens = line.rstrip().split()\n",
        "      if tokens[0] in cn_data.vocab and tokens[1] in cn_data.vocab:\n",
        "        dataset.append(((tokens[0], tokens[1]), float(tokens[2])))\n",
        "    #line_num +=1\n",
        "  dataset.sort(key = lambda score: -score[1]) #sort based on score\n",
        " # print(cn_data['gem'])\n",
        "  cn_dataset = {}\n",
        "  cn_dataset_list = []\n",
        "  \n",
        "  for ((word1, word2), score) in dataset:\n",
        "    #print(word1, word2)\n",
        "    sim_score = 1 - cosine_similarity( cn_data[word1]  - (C @ cn_data[word1]).reshape(1,-1)  , cn_data[word2] - (C@ cn_data[word2]).reshape(1,-1))\n",
        "    cn_dataset[(word1, word2)] = sim_score\n",
        "    cn_dataset_list.append(((word1, word2),sim_score))\n",
        "  cn_dataset_list.sort(key = lambda score: score[1])\n",
        "  spearman_list1=[]\n",
        "  spearman_list2=[]\n",
        "  for pos_1, (pair, score_1) in enumerate(dataset):\n",
        "    score_2 = cn_dataset[pair]\n",
        "    pos_2 = cn_dataset_list.index((pair, score_2))\n",
        "    spearman_list1.append(pos_1)\n",
        "    spearman_list2.append(pos_2)\n",
        "  rho = spearmanr(spearman_list1, spearman_list2)\n",
        "  return rho[0] \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmzwtDc7S_99",
        "colab_type": "code",
        "outputId": "461112a6-8730-4def-d640-b37400c78a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = '/content/'+  dataset\n",
        "    print('evaluating the data set', dataSetAddress)\n",
        "    print('        Fasttext2')\n",
        "    print('With CN ', \"%.4f\" % get_sim_large_data(dataSetAddress, 'fasttext2', fasttext2_concept_mat, 1))\n",
        "    print('No   CN ', \"%.4f\" % get_sim_no_cn(dataSetAddress, 'fasttext2'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set /content/EN-RG-65.txt\n",
            "        Fasttext2\n",
            "With CN  0.8755\n",
            "No   CN  0.8526\n",
            "evaluating the data set /content/EN-WS-353-ALL.txt\n",
            "        Fasttext2\n",
            "With CN  0.7904\n",
            "No   CN  0.7921\n",
            "evaluating the data set /content/EN-RW-STANFORD.txt\n",
            "        Fasttext2\n",
            "With CN  0.6135\n",
            "No   CN  0.5949\n",
            "evaluating the data set /content/EN-MEN-TR-3k.txt\n",
            "        Fasttext2\n",
            "With CN  0.8466\n",
            "No   CN  0.8362\n",
            "evaluating the data set /content/EN-MTurk-287.txt\n",
            "        Fasttext2\n",
            "With CN  0.7323\n",
            "No   CN  0.7254\n",
            "evaluating the data set /content/EN-SIMLEX-999.txt\n",
            "        Fasttext2\n",
            "With CN  0.5168\n",
            "No   CN  0.5051\n",
            "evaluating the data set /content/EN-SimVerb-3500.txt\n",
            "        Fasttext2\n",
            "With CN  0.4348\n",
            "No   CN  0.4304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j9execaAXaqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Without CN post-processing"
      ]
    },
    {
      "metadata": {
        "id": "x8SN7qBuXZ85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sim_no_cn(data_f_name, f_name):\n",
        "  model = eval(f_name)\n",
        "  fin = io.open(data_f_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  data = []\n",
        "  #line_num = 0\n",
        "  for line in fin:\n",
        "    #if line_num > 0:\n",
        "      tokens = line.rstrip().split()\n",
        "      if tokens[0] in model.vocab and tokens[1] in model.vocab:\n",
        "        data.append(((tokens[0], tokens[1]), float(tokens[2])))\n",
        "   # line_num +=1\n",
        "  data.sort(key = lambda score: -score[1]) #sort based on score\n",
        "  dataset = {}\n",
        "  dataset_list = []\n",
        "  \n",
        "  for ((word1, word2), score) in data:\n",
        "    sim_score = 1 - cosine_similarity(model[word1].reshape(1,-1), model[word2].reshape(1,-1))\n",
        "    dataset[(word1, word2)] = sim_score\n",
        "    dataset_list.append(((word1, word2),sim_score))\n",
        "  dataset_list.sort(key = lambda score: score[1])\n",
        "  spearman_list1=[]\n",
        "  spearman_list2=[]\n",
        "  for pos_1, (pair, score_1) in enumerate(data):\n",
        "    score_2 = dataset[pair]\n",
        "    pos_2 = dataset_list.index((pair, score_2))\n",
        "    spearman_list1.append(pos_1)\n",
        "    spearman_list2.append(pos_2)\n",
        "  rho = spearmanr(spearman_list1, spearman_list2)\n",
        "  return rho[0] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIZ32onyZaeA",
        "colab_type": "code",
        "outputId": "dd4f24b4-950f-4d6b-82e1-146a414dc2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = '/content/'+  dataset\n",
        "    print('evaluating the data set', dataSetAddress)\n",
        "    print('Fasttext ', 'GloVe ', 'w2v ')\n",
        "    print(\"%.4f\" % get_sim_no_cn(dataSetAddress, 'fasttext'), \"%.4f\" % get_sim_no_cn(dataSetAddress, 'glove'), \"%.4f\" % get_sim_no_cn(dataSetAddress, 'w2v'))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set /content/EN-RG-65.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.8457 0.7603 0.7494\n",
            "evaluating the data set /content/EN-WS-353-ALL.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.7334 0.7379 0.6934\n",
            "evaluating the data set /content/EN-RW-STANFORD.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.5231 0.5101 0.5578\n",
            "evaluating the data set /content/EN-MEN-TR-3k.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.7904 0.8013 0.7707\n",
            "evaluating the data set /content/EN-MTurk-287.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.7051 0.6916 0.6831\n",
            "evaluating the data set /content/EN-SIMLEX-999.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.4503 0.4076 0.4427\n",
            "evaluating the data set /content/EN-SimVerb-3500.txt\n",
            "Fasttext  GloVe  w2v \n",
            "0.3605 0.2842 0.3654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kgXSJJaygvTx",
        "colab_type": "code",
        "outputId": "7e3b4686-56cd-4aa4-b278-dc71faf86aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = '/content/'+  dataset\n",
        "    print('evaluating the data set', dataSetAddress)\n",
        "    print('Fasttext2')\n",
        "    print(\"%.4f\" % get_sim_no_cn(dataSetAddress, 'fasttext2'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set /content/EN-RG-65.txt\n",
            "Fasttext2\n",
            "0.8587\n",
            "evaluating the data set /content/EN-WS-353-ALL.txt\n",
            "Fasttext2\n",
            "0.7915\n",
            "evaluating the data set /content/EN-RW-STANFORD.txt\n",
            "Fasttext2\n",
            "0.5948\n",
            "evaluating the data set /content/EN-MEN-TR-3k.txt\n",
            "Fasttext2\n",
            "0.8364\n",
            "evaluating the data set /content/EN-MTurk-287.txt\n",
            "Fasttext2\n",
            "0.7262\n",
            "evaluating the data set /content/EN-SIMLEX-999.txt\n",
            "Fasttext2\n",
            "0.5038\n",
            "evaluating the data set /content/EN-SimVerb-3500.txt\n",
            "Fasttext2\n",
            "0.4304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uhn9WM0dicG9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results"
      ]
    },
    {
      "metadata": {
        "id": "EsFoNZEFrPPK",
        "colab_type": "code",
        "outputId": "72af0a38-a9ba-424f-cf4f-ab4d9633a7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = '/content/'+  dataset\n",
        "    print('evaluating the data set', dataSetAddress)\n",
        "    print('         Fasttext ', 'GloVe ', 'w2v ')\n",
        "    print('With CN',\"%.4f\" % get_sim(dataSetAddress, 'fasttext',cn_fasttext_mat, alpha =2), \"%.4f\" % get_sim(dataSetAddress, 'glove', cn_glove_mat, alpha =2), \"%.4f\" % get_sim(dataSetAddress, 'w2v', cn_w2v_mat, alpha =2))\n",
        "    print('NO   CN',\"%.4f\" % get_sim_no_cn(dataSetAddress, 'fasttext'), \"%.4f\" % get_sim_no_cn(dataSetAddress, 'glove'), \"%.4f\" % get_sim_no_cn(dataSetAddress, 'w2v'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set /content/EN-RG-65.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.8621 0.7840 0.7892\n",
            "NO   CN 0.8400 0.7510 0.7391\n",
            "evaluating the data set /content/EN-WS-353-ALL.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.7336 0.7908 0.6930\n",
            "NO   CN 0.7334 0.7385 0.6935\n",
            "evaluating the data set /content/EN-RW-STANFORD.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.5369 0.5898 0.5804\n",
            "NO   CN 0.5231 0.5101 0.5578\n",
            "evaluating the data set /content/EN-MEN-TR-3k.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.8062 0.8338 0.7867\n",
            "NO   CN 0.7902 0.8011 0.7705\n",
            "evaluating the data set /content/EN-MTurk-287.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.7141 0.7107 0.6681\n",
            "NO   CN 0.7072 0.6908 0.6831\n",
            "evaluating the data set /content/EN-SIMLEX-999.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.4584 0.4853 0.4682\n",
            "NO   CN 0.4521 0.4073 0.4419\n",
            "evaluating the data set /content/EN-SimVerb-3500.txt\n",
            "         Fasttext  GloVe  w2v \n",
            "With CN 0.3652 0.3636 0.3830\n",
            "NO   CN 0.3603 0.2843 0.3654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rq4P_yGNoXoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 2: STS Benchmark\n",
        "I re-implement STS tasks by evaluating CN post-processed word vectors with 2012-2017 SemEval STS tasks."
      ]
    },
    {
      "metadata": {
        "id": "CM2WhrKpd2W1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Load STS datasets"
      ]
    },
    {
      "metadata": {
        "id": "cvypn1sHSDhN",
        "colab_type": "code",
        "outputId": "458ea594-dd1f-4fb1-ab50-16d07675ed48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-dev.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-mt.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-other.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-test.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-train.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-19 13:54:22--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-dev.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255680 (250K) [text/plain]\n",
            "Saving to: ‘sts-dev.csv’\n",
            "\n",
            "\rsts-dev.csv           0%[                    ]       0  --.-KB/s               \rsts-dev.csv         100%[===================>] 249.69K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-01-19 13:54:22 (5.94 MB/s) - ‘sts-dev.csv’ saved [255680/255680]\n",
            "\n",
            "--2019-01-19 13:54:23--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-mt.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 513141 (501K) [text/plain]\n",
            "Saving to: ‘sts-mt.csv’\n",
            "\n",
            "sts-mt.csv          100%[===================>] 501.11K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-01-19 13:54:23 (9.55 MB/s) - ‘sts-mt.csv’ saved [513141/513141]\n",
            "\n",
            "--2019-01-19 13:54:24--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-other.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 661674 (646K) [text/plain]\n",
            "Saving to: ‘sts-other.csv’\n",
            "\n",
            "sts-other.csv       100%[===================>] 646.17K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-01-19 13:54:24 (12.4 MB/s) - ‘sts-other.csv’ saved [661674/661674]\n",
            "\n",
            "--2019-01-19 13:54:25--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 282419 (276K) [text/plain]\n",
            "Saving to: ‘sts-test.csv’\n",
            "\n",
            "sts-test.csv        100%[===================>] 275.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-01-19 13:54:25 (6.64 MB/s) - ‘sts-test.csv’ saved [282419/282419]\n",
            "\n",
            "--2019-01-19 13:54:25--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 901177 (880K) [text/plain]\n",
            "Saving to: ‘sts-train.csv’\n",
            "\n",
            "sts-train.csv       100%[===================>] 880.06K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-01-19 13:54:26 (14.0 MB/s) - ‘sts-train.csv’ saved [901177/901177]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yBQJhrZ8Tocw",
        "colab_type": "code",
        "outputId": "ed6c7702-69bb-43cb-c093-b581995f9175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "EN-MEN-TR-3k.txt     sample_data\t  sts-other.csv\n",
            "EN-MTurk-287.txt     small_glove.txt\t  sts-test.csv\n",
            "EN-RG-65.txt\t     small_glove_w2v.txt  sts-train.csv\n",
            "EN-RW-STANFORD.txt   small_w2v_w2v.txt\t  wiki-news-300d-1M.vec\n",
            "EN-SIMLEX-999.txt    small_word2vec.txt   wiki-news-300d-1M.vec.zip\n",
            "EN-SimVerb-3500.txt  sts-dev.csv\n",
            "EN-WS-353-ALL.txt    sts-mt.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Oaa8E1Yzo0Tk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "def load_sts_dataset(fname):\n",
        "      fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
        "      sent_pairs = []\n",
        "      for line in fin:\n",
        "          items = line.rstrip().split('\\t')\n",
        "          if len(items) == 7 or len(items) == 9:\n",
        "              sent_pairs.append((re.sub(\"[^0-9]\", \"\", items[2]) + '-' + items[1] , items[5], items[6], float(items[4])))\n",
        "          elif len(items) == 6 or len(items) == 8:\n",
        "              sent_pairs.append((re.sub(\"[^0-9]\", \"\", items[1]) + '-' + items[0] , items[4], items[5], float(items[3])))\n",
        "          else:\n",
        "              print('data format is wrong!!!')\n",
        "      return pd.DataFrame(sent_pairs, columns=[\"year_task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_all_sts_dataset():\n",
        "    # Loads all of the STS datasets \n",
        "    resourceFile = '/content/'\n",
        "    sts_train = load_sts_dataset(resourceFile + 'sts-train.csv') \n",
        "    sts_dev = load_sts_dataset(resourceFile + \"sts-dev.csv\")\n",
        "    sts_test = load_sts_dataset(resourceFile + \"sts-test.csv\")\n",
        "    sts_other = load_sts_dataset(resourceFile + \"sts-other.csv\")\n",
        "    sts_mt = load_sts_dataset(resourceFile +\"sts-mt.csv\")\n",
        "    \n",
        "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
        "    \n",
        "    return sts_all\n",
        "\n",
        "sts_all = load_all_sts_dataset()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNWC3KTveCdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load dataset by year-task"
      ]
    },
    {
      "metadata": {
        "id": "RUvku9s5mqNW",
        "colab_type": "code",
        "outputId": "81998c34-e2a7-4a2d-9767-8c63d18f0f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "def load_by_task_year(sts_all):\n",
        "  sts_task_year = {}\n",
        "  for i in sts_all['year_task']:\n",
        "    indices = [index for index, x in enumerate(sts_all['year_task']) if x == i]\n",
        "    sts_task_year[i] = sts_all.iloc[indices]\n",
        "  return sts_task_year\n",
        "sts_year_task = load_by_task_year(sts_all)\n",
        "print(sts_year_task.keys())\n",
        "print(sts_year_task['2012-MSRvid'][0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['2012-MSRvid', '2014-images', '2015-images', '2014-deft-forum', '2012-MSRpar', '2014-deft-news', '2013-headlines', '2014-headlines', '2015-headlines', '2016-headlines', '2017-track5.en-en', '2015-answers-forums', '2016-answer-answer', '2012-surprise.OnWN', '2013-FNWN', '2013-OnWN', '2014-OnWN', '2014-tweet-news', '2015-belief', '2016-plagiarism', '2016-question-question', '2012-SMTeuroparl', '2012-surprise.SMTnews', '2016-postediting'])\n",
            "     year_task                                         sent_1  \\\n",
            "0  2012-MSRvid                         A plane is taking off.   \n",
            "1  2012-MSRvid                A man is playing a large flute.   \n",
            "2  2012-MSRvid  A man is spreading shreded cheese on a pizza.   \n",
            "3  2012-MSRvid                   Three men are playing chess.   \n",
            "4  2012-MSRvid                    A man is playing the cello.   \n",
            "\n",
            "                                              sent_2   sim  \n",
            "0                        An air plane is taking off.  5.00  \n",
            "1                          A man is playing a flute.  3.80  \n",
            "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
            "3                         Two men are playing chess.  2.60  \n",
            "4                 A man seated is playing the cello.  4.25  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "utseGdZ-eHoa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load dataset by year"
      ]
    },
    {
      "metadata": {
        "id": "ptl5qiXcotl9",
        "colab_type": "code",
        "outputId": "ce0a0269-5912-483d-ef55-74227b3d013a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "sts_year = {}\n",
        "def load_by_year(sts_all):\n",
        "  for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
        "    indices = [index for index, x in enumerate(sts_all['year_task'])if year in x]\n",
        "    # store year as dictionary, [year: year-task]\n",
        "    #year_task = sts_all.iloc[indices]\n",
        "    sts_year[year] = sts_all.iloc[indices]\n",
        "  return sts_year\n",
        "sts_year = load_by_year(sts_all)\n",
        "print(len(sts_year.keys()))\n",
        "print(sts_year['2016'][:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "           year_task                                             sent_1  \\\n",
            "5552  2016-headlines  Driver backs into stroller with child, drives off   \n",
            "5553  2016-headlines   Spain Princess Testifies in Historic Fraud Probe   \n",
            "5554  2016-headlines  Senate confirms Obama nominee to key appeals c...   \n",
            "5555  2016-headlines  U.N. rights chief presses Egypt on Mursi deten...   \n",
            "5556  2016-headlines  US Senate confirms Janet Yellen as US Federal ...   \n",
            "\n",
            "                                                 sent_2  sim  \n",
            "5552  Driver backs into mom, stroller with child the...  4.0  \n",
            "5553   Spain princess testifies in historic fraud probe  5.0  \n",
            "5554  Senate approves Obama nominee to key appeals c...  5.0  \n",
            "5555   UN Rights Chief Presses Egypt on Morsi Detention  5.0  \n",
            "5556  Senate confirms Janet Yellen as next Federal R...  5.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wmhlTYqMaIrB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparation for STS Evaluation\n",
        "\n",
        "\n",
        "*   Define Sentence class, which has raw data and tokenized data\n",
        "*   Get similarity scores based on embeddings\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GO05uTkmaNBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Sentence:\n",
        "  def __init__(self, sentence):\n",
        "    self.raw = sentence\n",
        "    normalized = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    self.tokens = [token.lower() for token in nltk.word_tokenize(normalized)]\n",
        "\n",
        "def sen_sim(sentences1, sentences2, cn_fname, cn_mat):\n",
        "  model = eval(cn_fname)\n",
        "  embeddings = []\n",
        "  ls_word = list(model.vocab)\n",
        "  for sent_1, sent_2 in zip(sentences1, sentences2):\n",
        "    tokens1 = sent_1.tokens\n",
        "    tokens2 = sent_2.tokens\n",
        "    tokens1 = [token for token in tokens1 if token in model.vocab and token.islower()]\n",
        "    tokens2 = [token for token in tokens2 if token in model.vocab and token.islower()]\n",
        "    ids1 = [ls_word.index(token) for token in tokens1 ]\n",
        "    ids2 = [ls_word.index(token) for token in tokens2 ]\n",
        "    embedding1 = np.average([cn_mat[id] for id in ids1], axis = 0)\n",
        "    embedding2 = np.average([cn_mat[id] for id in ids2], axis = 0)\n",
        "    if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
        "      embeddings.append(np.zeros(300))\n",
        "      embeddings.append(np.zeros(300))\n",
        "    else:\n",
        "      embeddings.append(embedding1)\n",
        "      embeddings.append(embedding2)\n",
        "  sim_score = [cosine_similarity(embeddings[id*2].reshape(1, -1), embeddings[id*2+1].reshape(1, -1))[0][0] for id in range(len(embeddings)//2)]\n",
        "  return sim_score\n",
        "        \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QS9nTyJd2K91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def no_cn_sen_sim(sentences1, sentences2, fname):\n",
        "  model = eval(fname)\n",
        "  embeddings = []\n",
        "  for sent_1, sent_2 in zip(sentences1, sentences2):\n",
        "    tokens1 = sent_1.tokens\n",
        "    tokens2 = sent_2.tokens\n",
        "    tokens1 = [token for token in tokens1 if token in model.vocab and token.islower()]\n",
        "    tokens2 = [token for token in tokens2 if token in model.vocab and token.islower()]\n",
        "    embedding1 = np.average([model[token] for token in tokens1], axis = 0)\n",
        "    embedding2 = np.average([model[token] for token in tokens2], axis = 0)\n",
        "    if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
        "      embeddings.append(np.zeros(300))\n",
        "      embeddings.append(np.zeros(300))\n",
        "    else:\n",
        "      embeddings.append(embedding1)\n",
        "      embeddings.append(embedding2)\n",
        "  sim_score = [cosine_similarity(embeddings[id*2].reshape(1, -1), embeddings[id*2+1].reshape(1, -1))[0][0] for id in range(len(embeddings)//2)]\n",
        "  return sim_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZoKlkxniYq1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results"
      ]
    },
    {
      "metadata": {
        "id": "WX0g9N1z9Tx4",
        "colab_type": "code",
        "outputId": "fb88269e-c75c-4391-df43-747c58841601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3281
        }
      },
      "cell_type": "code",
      "source": [
        "model_list = ['glove', 'w2v', 'fasttext']\n",
        "pearson_cors = {}\n",
        "pearson_cors_no_cn = {}\n",
        "mat = []\n",
        "for year_task in sts_all['year_task'].unique():\n",
        "  for model in model_list:\n",
        "    if model == 'glove':\n",
        "      mat = cn_glove_mat\n",
        "    elif model == 'w2v':\n",
        "      mat = cn_w2v_mat\n",
        "    elif model == 'fasttext':\n",
        "      mat = cn_fasttext_mat\n",
        "        \n",
        "    sentences1=[Sentence(sent1) for sent1 in sts_year_task[year_task]['sent_1']]\n",
        "    sentences2=[Sentence(sent2) for sent2 in sts_year_task[year_task]['sent_2']]\n",
        "    sim = sen_sim(sentences1, sentences2, model, mat)\n",
        "    pearson_correlation = round(scipy.stats.pearsonr(sim, sts_year_task[year_task]['sim'])[0] * 100,2)\n",
        "    pearson_cors[(model, year_task)] = pearson_correlation\n",
        "    sim2 = no_cn_sen_sim(sentences1, sentences2, model)\n",
        "    pearson_correlation_no_cn = round(scipy.stats.pearsonr(sim2, sts_year_task[year_task]['sim'])[0] * 100,2)\n",
        "    pearson_cors_no_cn[(model, year_task)] = pearson_correlation_no_cn\n",
        "count = 0\n",
        "for (i,j) in pearson_cors.keys():\n",
        "  if count % 3 ==0:\n",
        "    print('')\n",
        "  count +=1\n",
        "  print('With CN',i, j, pearson_cors[(i,j)])\n",
        "  print('NO   CN',i, j, pearson_cors_no_cn[(i,j)])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "With CN glove 2012-MSRvid 62.5\n",
            "NO   CN glove 2012-MSRvid 65.85\n",
            "With CN w2v 2012-MSRvid 75.22\n",
            "NO   CN w2v 2012-MSRvid 76.06\n",
            "With CN fasttext 2012-MSRvid 66.44\n",
            "NO   CN fasttext 2012-MSRvid 66.35\n",
            "\n",
            "With CN glove 2014-images 65.81\n",
            "NO   CN glove 2014-images 61.89\n",
            "With CN w2v 2014-images 78.24\n",
            "NO   CN w2v 2014-images 77.43\n",
            "With CN fasttext 2014-images 63.41\n",
            "NO   CN fasttext 2014-images 61.6\n",
            "\n",
            "With CN glove 2015-images 71.43\n",
            "NO   CN glove 2015-images 69.14\n",
            "With CN w2v 2015-images 80.48\n",
            "NO   CN w2v 2015-images 79.97\n",
            "With CN fasttext 2015-images 71.13\n",
            "NO   CN fasttext 2015-images 70.42\n",
            "\n",
            "With CN glove 2014-deft-forum 37.57\n",
            "NO   CN glove 2014-deft-forum 28.82\n",
            "With CN w2v 2014-deft-forum 42.8\n",
            "NO   CN w2v 2014-deft-forum 41.33\n",
            "With CN fasttext 2014-deft-forum 40.18\n",
            "NO   CN fasttext 2014-deft-forum 38.65\n",
            "\n",
            "With CN glove 2012-MSRpar 41.19\n",
            "NO   CN glove 2012-MSRpar 42.01\n",
            "With CN w2v 2012-MSRpar 40.3\n",
            "NO   CN w2v 2012-MSRpar 42.2\n",
            "With CN fasttext 2012-MSRpar 45.03\n",
            "NO   CN fasttext 2012-MSRpar 44.98\n",
            "\n",
            "With CN glove 2014-deft-news 69.08\n",
            "NO   CN glove 2014-deft-news 63.41\n",
            "With CN w2v 2014-deft-news 65.57\n",
            "NO   CN w2v 2014-deft-news 66.95\n",
            "With CN fasttext 2014-deft-news 64.76\n",
            "NO   CN fasttext 2014-deft-news 64.01\n",
            "\n",
            "With CN glove 2013-headlines 67.0\n",
            "NO   CN glove 2013-headlines 63.54\n",
            "With CN w2v 2013-headlines 64.78\n",
            "NO   CN w2v 2013-headlines 64.83\n",
            "With CN fasttext 2013-headlines 67.04\n",
            "NO   CN fasttext 2013-headlines 66.18\n",
            "\n",
            "With CN glove 2014-headlines 61.71\n",
            "NO   CN glove 2014-headlines 59.28\n",
            "With CN w2v 2014-headlines 61.09\n",
            "NO   CN w2v 2014-headlines 61.05\n",
            "With CN fasttext 2014-headlines 63.36\n",
            "NO   CN fasttext 2014-headlines 62.96\n",
            "\n",
            "With CN glove 2015-headlines 69.18\n",
            "NO   CN glove 2015-headlines 65.41\n",
            "With CN w2v 2015-headlines 68.88\n",
            "NO   CN w2v 2015-headlines 68.53\n",
            "With CN fasttext 2015-headlines 69.84\n",
            "NO   CN fasttext 2015-headlines 69.36\n",
            "\n",
            "With CN glove 2016-headlines 67.19\n",
            "NO   CN glove 2016-headlines 61.6\n",
            "With CN w2v 2016-headlines 65.13\n",
            "NO   CN w2v 2016-headlines 64.34\n",
            "With CN fasttext 2016-headlines 66.05\n",
            "NO   CN fasttext 2016-headlines 64.78\n",
            "\n",
            "With CN glove 2017-track5.en-en 65.42\n",
            "NO   CN glove 2017-track5.en-en 57.85\n",
            "With CN w2v 2017-track5.en-en 73.44\n",
            "NO   CN w2v 2017-track5.en-en 72.03\n",
            "With CN fasttext 2017-track5.en-en 61.34\n",
            "NO   CN fasttext 2017-track5.en-en 58.77\n",
            "\n",
            "With CN glove 2015-answers-forums 48.62\n",
            "NO   CN glove 2015-answers-forums 36.86\n",
            "With CN w2v 2015-answers-forums 53.66\n",
            "NO   CN w2v 2015-answers-forums 52.99\n",
            "With CN fasttext 2015-answers-forums 45.04\n",
            "NO   CN fasttext 2015-answers-forums 44.98\n",
            "\n",
            "With CN glove 2016-answer-answer 39.59\n",
            "NO   CN glove 2016-answer-answer 37.36\n",
            "With CN w2v 2016-answer-answer 42.22\n",
            "NO   CN w2v 2016-answer-answer 40.27\n",
            "With CN fasttext 2016-answer-answer 39.2\n",
            "NO   CN fasttext 2016-answer-answer 37.65\n",
            "\n",
            "With CN glove 2012-surprise.OnWN 67.96\n",
            "NO   CN glove 2012-surprise.OnWN 60.82\n",
            "With CN w2v 2012-surprise.OnWN 70.82\n",
            "NO   CN w2v 2012-surprise.OnWN 70.93\n",
            "With CN fasttext 2012-surprise.OnWN 65.97\n",
            "NO   CN fasttext 2012-surprise.OnWN 63.72\n",
            "\n",
            "With CN glove 2013-FNWN 42.07\n",
            "NO   CN glove 2013-FNWN 39.5\n",
            "With CN w2v 2013-FNWN 43.99\n",
            "NO   CN w2v 2013-FNWN 40.12\n",
            "With CN fasttext 2013-FNWN 36.48\n",
            "NO   CN fasttext 2013-FNWN 33.85\n",
            "\n",
            "With CN glove 2013-OnWN 57.45\n",
            "NO   CN glove 2013-OnWN 53.75\n",
            "With CN w2v 2013-OnWN 68.76\n",
            "NO   CN w2v 2013-OnWN 67.92\n",
            "With CN fasttext 2013-OnWN 58.88\n",
            "NO   CN fasttext 2013-OnWN 58.02\n",
            "\n",
            "With CN glove 2014-OnWN 66.43\n",
            "NO   CN glove 2014-OnWN 61.91\n",
            "With CN w2v 2014-OnWN 75.08\n",
            "NO   CN w2v 2014-OnWN 74.94\n",
            "With CN fasttext 2014-OnWN 67.01\n",
            "NO   CN fasttext 2014-OnWN 66.1\n",
            "\n",
            "With CN glove 2014-tweet-news 75.37\n",
            "NO   CN glove 2014-tweet-news 62.37\n",
            "With CN w2v 2014-tweet-news 74.55\n",
            "NO   CN w2v 2014-tweet-news 72.84\n",
            "With CN fasttext 2014-tweet-news 69.55\n",
            "NO   CN fasttext 2014-tweet-news 67.38\n",
            "\n",
            "With CN glove 2015-belief 59.77\n",
            "NO   CN glove 2015-belief 44.2\n",
            "With CN w2v 2015-belief 61.29\n",
            "NO   CN w2v 2015-belief 60.04\n",
            "With CN fasttext 2015-belief 50.73\n",
            "NO   CN fasttext 2015-belief 48.3\n",
            "\n",
            "With CN glove 2016-plagiarism 70.28\n",
            "NO   CN glove 2016-plagiarism 54.43\n",
            "With CN w2v 2016-plagiarism 73.5\n",
            "NO   CN w2v 2016-plagiarism 74.32\n",
            "With CN fasttext 2016-plagiarism 63.57\n",
            "NO   CN fasttext 2016-plagiarism 62.39\n",
            "\n",
            "With CN glove 2016-question-question 60.74\n",
            "NO   CN glove 2016-question-question 53.74\n",
            "With CN w2v 2016-question-question 64.46\n",
            "NO   CN w2v 2016-question-question 63.8\n",
            "With CN fasttext 2016-question-question 49.69\n",
            "NO   CN fasttext 2016-question-question 48.19\n",
            "\n",
            "With CN glove 2012-SMTeuroparl 52.58\n",
            "NO   CN glove 2012-SMTeuroparl 51.97\n",
            "With CN w2v 2012-SMTeuroparl 35.14\n",
            "NO   CN w2v 2012-SMTeuroparl 31.45\n",
            "With CN fasttext 2012-SMTeuroparl 53.52\n",
            "NO   CN fasttext 2012-SMTeuroparl 50.18\n",
            "\n",
            "With CN glove 2012-surprise.SMTnews 47.69\n",
            "NO   CN glove 2012-surprise.SMTnews 46.35\n",
            "With CN w2v 2012-surprise.SMTnews 50.08\n",
            "NO   CN w2v 2012-surprise.SMTnews 53.75\n",
            "With CN fasttext 2012-surprise.SMTnews 53.76\n",
            "NO   CN fasttext 2012-surprise.SMTnews 55.3\n",
            "\n",
            "With CN glove 2016-postediting 68.9\n",
            "NO   CN glove 2016-postediting 55.88\n",
            "With CN w2v 2016-postediting 69.54\n",
            "NO   CN w2v 2016-postediting 68.22\n",
            "With CN fasttext 2016-postediting 62.13\n",
            "NO   CN fasttext 2016-postediting 60.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IXtXl2A7Dg8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PaKOcJ30iNDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://github.com/liutianlin0121/Conceptor-Negation-WV\n",
        "2. Unsupervised Post-processing of Word Vectors via Conceptor Negation. Tianlin Liu, João Sedoc, and Lyle Ungar, Unsupervised Post-processing of Word Vectors via Conceptor Negation, AAAI 2019.\n"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/CN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hRbl49z3h8a7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import tensorflow as tf\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "import functools as ft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpTZDQjsYIRs",
        "colab_type": "code",
        "outputId": "def09e2a-c29f-40fb-9e7c-f1cdb4994afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "conceptor  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OiAFjzhh2TEC",
        "colab_type": "code",
        "outputId": "d9c11616-8515-4318-fa55-8e8f4fc84e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "KLCNa8TNMMhE",
        "colab_type": "code",
        "outputId": "1b5a6b7b-0323-4654-9ce5-c9f0c90cbd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Glove embeddings\n",
        "!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "#import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "\n",
        "#glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
            "To: /content/gensim_glove.840B.300d.txt.bin\n",
            "2.65GB [00:17, 151MB/s]\n",
            "The glove embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sQGMyhSgNbT8",
        "colab_type": "code",
        "outputId": "5c03dc99-a92b-4a4c-de12-2348983f1707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "\n",
        "#word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "1.65GB [00:10, 157MB/s]\n",
            "The glove embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p-LZaFTviA62",
        "colab_type": "code",
        "outputId": "8106942a-8bd5-425a-a7c8-7d1562021695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!gdown https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM\n",
        "!gdown https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM\n",
            "To: /content/small_glove.txt\n",
            "333MB [00:02, 160MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB\n",
            "To: /content/small_word2vec.txt\n",
            "267MB [00:01, 164MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jf_LsdSXiP9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_embd(model_str):\n",
        "  embeddings = {}\n",
        "  f = codecs.open(\"/content/small_\" + model_str + \".txt\",\"r\",\"utf-8\")\n",
        "  \n",
        "  for line in f:\n",
        "    line = line.split(\" \",1)\n",
        "    embeddings[line[0].lower()] = np.fromstring(line[1],sep=\" \",dtype=\"float32\")\n",
        "  return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6B-DWPy4jaQ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_embd = load_embd(\"glove\")\n",
        "#w2v_embd = load_embd(\"word2vec\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KD8kGGmLjneq",
        "colab_type": "code",
        "outputId": "bc255dec-a038-4815-e8a3-38a53819ff82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.8722813232690143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "odA7DO84tQQe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.array(list(glove_embd.values())).T\n",
        "for ind in np.arange(0,x.shape[1]):\n",
        "  curr_word = list(glove_embd.keys())[ind]\n",
        "  print(curr_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97fCjKCelTkW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Hello\")\n",
        "def post_process_cn(orig_embd, saveto, alpha = 2):\n",
        "  print(\"starting...\")\n",
        "  #x = orig_embd.vectors\n",
        "  print(orig_embd.vectors.shape)\n",
        "  #Calculate the correlation matrix\n",
        "  R = orig_embd.vectors.dot(orig_embd.vectors.T)/(orig_embd.vectors.shape[1])\n",
        "  \n",
        "  #Calculate the conceptor matrix\n",
        "  C = R @ (np.linalg.inv(R + alpha ** (-2) * np.eye(orig_embd.vectors.shape[0])))\n",
        "  \n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = np.eye(orig_embd.vectors.shape[0]) - C\n",
        "  \n",
        "  #Post-process the vocab matrix\n",
        "  newX = (negC @ orig_embd.vectors).T\n",
        "  print(newX.shape)\n",
        "  new_embd = {}\n",
        "  count = 0\n",
        "  #Convert to dictonary\n",
        "  for ind in np.arange(0,orig_embd.vectors.shape[1]):\n",
        "    #curr_word = list(orig_embd.keys())[ind]\n",
        "    #new_embd[curr_word] = newX[ind,:]\n",
        "    #count = count + 1\n",
        "    \n",
        "    #Directly store post-processed embeddings to file\n",
        "    f = open(\"/content/\" + saveto, \"a+\")\n",
        "    val = ' '.join((np.array2string(newX[ind,:]).replace(\"[\",\"\").replace(\"]\",\"\")).split()) + \"\\n\"\n",
        "    f.write(curr_word + \" \" + val)\n",
        "    count = count + 1\n",
        "    print(count)\n",
        "    \n",
        "  \n",
        "  return new_embd\n",
        "\n",
        "print(\"Begin\")\n",
        "glove_cn = post_process_cn(glove_embd, \"full_glove_cn.txt\")\n",
        "print(\"Glove done\")\n",
        "#w2v_cn = post_process_cn(word2vec, \"full_word2vec_cn.txt\")\n",
        "#print(\"Word2vec done\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pVFd5YAQa3qs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def post_process_cn_matrix(x, alpha = 2):\n",
        "  print(\"starting...\")\n",
        "  #x = orig_embd.vectors\n",
        "  print(x.shape)\n",
        "  \n",
        "  #Calculate the correlation matrix\n",
        "  R = x.dot(x.T)/(x.shape[1])\n",
        "  print(\"R calculated\")\n",
        "  print('Memory', psutil.virtual_memory())\n",
        "  #Calculate the conceptor matrix\n",
        "  C = R @ (np.linalg.inv(R + alpha ** (-2) * np.eye(x.shape[0])))\n",
        "  print(\"C calculated\")\n",
        "  print('Memory', psutil.virtual_memory())\n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = np.eye(x.shape[0]) - C\n",
        "  print(\"negC calculated\")\n",
        "  print('Memory', psutil.virtual_memory())\n",
        "  #Post-process the vocab matrix\n",
        "  newX = (negC @ x).T\n",
        "  print(newX.shape)\n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GH_ve-0MkAbT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Save post-processed embeddings to text file"
      ]
    },
    {
      "metadata": {
        "id": "hny7gKvIj_ED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_text(curr_embd, saveto):\n",
        "  for key, value in curr_embd.items():\n",
        "    #print(\"key: \", key, \" value: \", np.array_str(value).replace(\"[\",\"\").replace(\"]\",\"\"))\n",
        "    f = open(\"/content/\" + saveto, \"a+\")\n",
        "    val = ' '.join((np.array2string(value).replace(\"[\",\"\").replace(\"]\",\"\")).split()) + \"\\n\"\n",
        "    f.write(key + \" \" + val)\n",
        "    #with open(\"/content/\" + saveto) as f:\n",
        "      #f.write(key + \" \" + np.array_str(value).replace(\"[\",\"\").replace(\"]\",\"\"))\n",
        "\n",
        "convert_to_text(glove_cn, \"glove_cn.txt\")\n",
        "convert_to_text(w2v_cn, \"w2v_cn.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeZmkl9hkT7F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Intrinsic Evaluation**\n",
        "Evaluate embeddings for word similarity "
      ]
    },
    {
      "metadata": {
        "id": "May82bhhyJxo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity_eval(dataSetAddress, embd):\n",
        "    wordVecModel = eval(embd)\n",
        "    vocab = set(list(wordVecModel.keys()))\n",
        "    \n",
        "    fread_simlex = open(dataSetAddress, \"r\")\n",
        "    \n",
        "    pair_list = []\n",
        "\n",
        "    line_number = 0\n",
        "    for line in fread_simlex:\n",
        "      #print(line)\n",
        "      if line_number > 0:\n",
        "          tokens = line.split()\n",
        "          word_i = tokens[0]\n",
        "          word_j = tokens[1]\n",
        "          score = float(tokens[2])\n",
        "          if word_i in vocab and word_j in vocab:\n",
        "              pair_list.append( ((word_i, word_j), score) )\n",
        "      line_number += 1\n",
        "\n",
        "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
        "\n",
        "\n",
        "    extracted_scores = {}\n",
        "\n",
        "    extracted_list = []\n",
        "    \n",
        "               \n",
        "    for (x,y) in pair_list:\n",
        "        (word_i, word_j) = x\n",
        "        \n",
        "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
        "\n",
        "        extracted_scores[(word_i, word_j)] = current_distance\n",
        "        extracted_list.append(((word_i, word_j), current_distance))\n",
        "\n",
        "    extracted_list.sort(key=lambda x: x[1])\n",
        "\n",
        "    spearman_original_list = []\n",
        "    spearman_target_list = []\n",
        "\n",
        "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
        "        score_2 = extracted_scores[word_pair]\n",
        "        position_2 = extracted_list.index((word_pair, score_2))\n",
        "        spearman_original_list.append(position_1)\n",
        "        spearman_target_list.append(position_2)\n",
        "\n",
        "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
        "    \n",
        "    return spearman_rho[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECR7t3R3OHB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-RG-65.txt https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-WS-353-ALL.txt https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-RW-STANFORD.txt https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-MEN-TR-3k.txt https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-MTurk-287.txt https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-SIMLEX-999.txt https://raw.githubusercontent.com/mfaruqui/eval-word-vectors/master/data/word-sim/EN-SimVerb-3500.txt -P wordSimData1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ep13J2_KaVP7",
        "colab_type": "code",
        "outputId": "98b70c69-f7eb-40d4-9e78-880e4ef746d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!cd wordSimData/\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-RG-65.txt  sample_data  small_glove.txt  small_word2vec.txt\twordSimData\n",
            "EN-RG-65.txt  sample_data  small_glove.txt  small_word2vec.txt\twordSimData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zjlnJaDjuEYs",
        "colab_type": "code",
        "outputId": "5d4b2659-3dd0-4a72-990d-beee9cd75684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt', 'EN-RG-65.txt']\n",
        "\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = 'wordSimData1/' +  dataset\n",
        "    print('evaluating the data set', dataset)\n",
        "    \n",
        "    print('Word2Vec + CN : %.4f' %  similarity_eval(dataSetAddress, 'w2v_cn'))\n",
        "    print('Glove + CN : %.4f' %  similarity_eval(dataSetAddress, 'glove_cn'))\n",
        "        \n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set EN-WS-353-ALL.txt\n",
            "Word2Vec + CN : 0.6930\n",
            "Glove + CN : 0.7908\n",
            "\n",
            "\n",
            "evaluating the data set EN-RW-STANFORD.txt\n",
            "Word2Vec + CN : 0.5804\n",
            "Glove + CN : 0.5898\n",
            "\n",
            "\n",
            "evaluating the data set EN-MEN-TR-3k.txt\n",
            "Word2Vec + CN : 0.7867\n",
            "Glove + CN : 0.8338\n",
            "\n",
            "\n",
            "evaluating the data set EN-MTurk-287.txt\n",
            "Word2Vec + CN : 0.6681\n",
            "Glove + CN : 0.7107\n",
            "\n",
            "\n",
            "evaluating the data set EN-SIMLEX-999.txt\n",
            "Word2Vec + CN : 0.4682\n",
            "Glove + CN : 0.4853\n",
            "\n",
            "\n",
            "evaluating the data set EN-SimVerb-3500.txt\n",
            "Word2Vec + CN : 0.3830\n",
            "Glove + CN : 0.3636\n",
            "\n",
            "\n",
            "evaluating the data set EN-RG-65.txt\n",
            "Word2Vec + CN : 0.7892\n",
            "Glove + CN : 0.7840\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2y3tXCFUkoFj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Extrinsic Evaluation\n"
      ]
    },
    {
      "metadata": {
        "id": "mHMm9YCEAyBH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To download post-processed embeddings, run the following cell"
      ]
    },
    {
      "metadata": {
        "id": "SE_ES2bwA5Va",
        "colab_type": "code",
        "outputId": "d68691e6-5ea5-4437-a8c4-beb5100f9434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1p7ThmH_Qzk8AsfHU-8Mh15nUU6MpSclE\n",
        "!gdown https://drive.google.com/uc?id=16fRNeAD0kmsXB9GVEZ-i0xH9IkRn9TTV"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1p7ThmH_Qzk8AsfHU-8Mh15nUU6MpSclE\n",
            "To: /content/glove_cn.txt\n",
            "524MB [00:04, 108MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16fRNeAD0kmsXB9GVEZ-i0xH9IkRn9TTV\n",
            "To: /content/w2v_cn.txt\n",
            "310MB [00:01, 156MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aIAl5mMVfOop",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clone git repository for extrinsic evaluation"
      ]
    },
    {
      "metadata": {
        "id": "WhhluA18fVjN",
        "colab_type": "code",
        "outputId": "ff9f6e3a-c079-488e-c4dd-a0673d9cbc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karvesaket/conceptor.git\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'conceptor'...\n",
            "remote: Enumerating objects: 279, done.\u001b[K\n",
            "remote: Counting objects: 100% (279/279), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 279 (delta 120), reused 35 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (279/279), 3.96 MiB | 3.52 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MwHFiMsigQ4q",
        "colab_type": "code",
        "outputId": "028c703a-08a4-4e63-fc55-4151543db9a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd ..\n",
        "%cd ..\n",
        "!rm -r conceptor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master\n",
            "/content/conceptor\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VFoeDLNmJpep",
        "colab_type": "code",
        "outputId": "cdc4cbeb-49ab-4175-e535-2a91aece55ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "%cd content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'content'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KxP8-JHYgFA8",
        "colab_type": "code",
        "outputId": "f13b9edd-4d4b-412e-8392-8257010f8609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!ls conceptor/Extrinsic-Evaluation-tasks-master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md\t\t\t  sentence_polarity_classification.zip\n",
            "Relation_extraction\t\t  sentiment_classification\n",
            "Relation_extraction.zip\t\t  snli\n",
            "requirements.txt\t\t  subjectivity_classification\n",
            "sentence_polarity_classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5jePKOTpkxmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Upload modified files for all tasks"
      ]
    },
    {
      "metadata": {
        "id": "2O7Q3dXok4WT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Relation Extraction"
      ]
    },
    {
      "metadata": {
        "id": "ktT5XAewhTTJ",
        "colab_type": "code",
        "outputId": "ae88ebeb-65b1-43b5-977e-50851e464350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "%cd content\n",
        "!unzip Relation_extraction.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'content'\n",
            "/content\n",
            "Archive:  Relation_extraction.zip\n",
            "   creating: Relation_extraction/\n",
            "   creating: Relation_extraction/embeddings/\n",
            "  inflating: Relation_extraction/embeddings/.gitignore  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/Relation_extraction/\n",
            "   creating: __MACOSX/Relation_extraction/embeddings/\n",
            "  inflating: __MACOSX/Relation_extraction/embeddings/._.gitignore  \n",
            "  inflating: __MACOSX/Relation_extraction/._embeddings  \n",
            "  inflating: Relation_extraction/requirements.txt  \n",
            "  inflating: __MACOSX/Relation_extraction/._requirements.txt  \n",
            "  inflating: Relation_extraction/preprocess.py  \n",
            "  inflating: __MACOSX/Relation_extraction/._preprocess.py  \n",
            "   creating: Relation_extraction/dataset/\n",
            "  inflating: Relation_extraction/dataset/train.txt  \n",
            "   creating: __MACOSX/Relation_extraction/dataset/\n",
            "  inflating: __MACOSX/Relation_extraction/dataset/._train.txt  \n",
            "  inflating: Relation_extraction/dataset/test.txt  \n",
            "  inflating: __MACOSX/Relation_extraction/dataset/._test.txt  \n",
            "  inflating: __MACOSX/Relation_extraction/._dataset  \n",
            "  inflating: Relation_extraction/train_cnn.py  \n",
            "  inflating: __MACOSX/Relation_extraction/._train_cnn.py  \n",
            "  inflating: __MACOSX/._Relation_extraction  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SPG5IvIY8sfD",
        "colab_type": "code",
        "outputId": "ecfc0b31-1797-47bf-e092-a54875fefc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd conceptor/Extrinsic-Evaluation-tasks-master/Relation_extraction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master/Relation_extraction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "04XLQxtF80Yq",
        "colab_type": "code",
        "outputId": "7bf08cb1-ecec-4d24-f5f5-c2b720565d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "aaa = [0.7650, 0.7490, 0.7409, 0.7512, 0.7460, 0.7505, 0.7446, 0.7541, 0.7516, 0.7519]\n",
        "bbb = [0.7266, 0.7250, 0.7072, 0.7144, 0.7138, 0.7195, 0.7235, 0.7222, 0.7221, 0.7198]\n",
        "print(np.mean(aaa))\n",
        "print(np.std(aaa))\n",
        "print(np.mean(bbb))\n",
        "print(np.std(bbb))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7504799999999999\n",
            "0.006134623052804467\n",
            "0.7194100000000001\n",
            "0.005661351428766797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p0fhhOkk-Hh0",
        "colab_type": "code",
        "outputId": "4ea9a622-45cf-4ec6-b1ad-8195e6f1263c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 preprocess.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PADDING': 0, 'LowerMin': 1, 'GreaterMax': 2, -30: 3, -29: 4, -28: 5, -27: 6, -26: 7, -25: 8, -24: 9, -23: 10, -22: 11, -21: 12, -20: 13, -19: 14, -18: 15, -17: 16, -16: 17, -15: 18, -14: 19, -13: 20, -12: 21, -11: 22, -10: 23, -9: 24, -8: 25, -7: 26, -6: 27, -5: 28, -4: 29, -3: 30, -2: 31, -1: 32, 0: 33, 1: 34, 2: 35, 3: 36, 4: 37, 5: 38, 6: 39, 7: 40, 8: 41, 9: 42, 10: 43, 11: 44, 12: 45, 13: 46, 14: 47, 15: 48, 16: 49, 17: 50, 18: 51, 19: 52, 20: 53, 21: 54, 22: 55, 23: 56, 24: 57, 25: 58, 26: 59, 27: 60, 28: 61, 29: 62, 30: 63}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1vFxmb3F-N32",
        "colab_type": "code",
        "outputId": "138aed29-89af-4531-8783-418e88da54cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35788
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 train_cnn.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PADDING': 0, 'LowerMin': 1, 'GreaterMax': 2, -30: 3, -29: 4, -28: 5, -27: 6, -26: 7, -25: 8, -24: 9, -23: 10, -22: 11, -21: 12, -20: 13, -19: 14, -18: 15, -17: 16, -16: 17, -15: 18, -14: 19, -13: 20, -12: 21, -11: 22, -10: 23, -9: 24, -8: 25, -7: 26, -6: 27, -5: 28, -4: 29, -3: 30, -2: 31, -1: 32, 0: 33, 1: 34, 2: 35, 3: 36, 4: 37, 5: 38, 6: 39, 7: 40, 8: 41, 9: 42, 10: 43, 11: 44, 12: 45, 13: 46, 14: 47, 15: 48, 16: 49, 17: 50, 18: 51, 19: 52, 20: 53, 21: 54, 22: 55, 23: 56, 24: 57, 25: 58, 26: 59, 27: 60, 28: 61, 29: 62, 30: 63}\n",
            "Using TensorFlow backend.\n",
            "Enter embedding typeglove\n",
            "tcmalloc: large alloc 2635227136 bytes == 0x49ac000 @  0x7faa38737001 0x7faa3646bb85 0x7faa364ceb43 0x7faa364d0a86 0x7faa36568868 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7faa38332b97 0x5afa0a\n",
            "300\n",
            "The embedding has been loaded from gensim!\n",
            "Load dataset\n",
            "Max Sentence Lengths:  [97, 67]\n",
            "Conceptor? n\n",
            "Embeddings shape:  (22108, 300)\n",
            "Len words:  23503\n",
            "[  2 112 113 114  11  56 115 116  35 117  15   2 118 119  43 120   1  61\n",
            " 121 122  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0]\n",
            "sentenceTrain:  (8000, 97)\n",
            "positionTrain1:  (8000, 97)\n",
            "yTrain:  (8000,)\n",
            "sentenceTest:  (2717, 97)\n",
            "positionTest1:  (2717, 97)\n",
            "yTest:  (2717,)\n",
            "Embeddings:  (22108, 300)\n",
            "2019-01-31 02:57:26.368456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-01-31 02:57:26.368925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-01-31 02:57:26.368969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-31 02:57:27.217987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-31 02:57:27.218043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-31 02:57:27.218064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-31 02:57:27.218358: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-01-31 02:57:27.218447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "words_input (InputLayer)        (None, 97)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "distance1_input (InputLayer)    (None, 97)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "distance2_input (InputLayer)    (None, 97)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 97, 300)      6632400     words_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 97, 50)       3200        distance1_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 97, 50)       3200        distance2_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 97, 400)      0           embedding_1[0][0]                \n",
            "                                                                 embedding_2[0][0]                \n",
            "                                                                 embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 97, 100)      120100      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100)          0           global_max_pooling1d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 19)           1919        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,760,819\n",
            "Trainable params: 128,419\n",
            "Non-trainable params: 6,632,400\n",
            "__________________________________________________________________________________________________\n",
            "Start training\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 6s 752us/step - loss: 1.7584 - acc: 0.4688\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 401us/step - loss: 0.8759 - acc: 0.7225\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 394us/step - loss: 0.6063 - acc: 0.8121\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 391us/step - loss: 0.4437 - acc: 0.8685\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 390us/step - loss: 0.3259 - acc: 0.9079\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 389us/step - loss: 0.2459 - acc: 0.9347\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 388us/step - loss: 0.1738 - acc: 0.9584\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 392us/step - loss: 0.1441 - acc: 0.9646\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 389us/step - loss: 0.1049 - acc: 0.9779\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0854 - acc: 0.9794\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 389us/step - loss: 0.0757 - acc: 0.9834\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0639 - acc: 0.9853\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0586 - acc: 0.9864\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0627 - acc: 0.9838\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0568 - acc: 0.9856\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0460 - acc: 0.9885\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0480 - acc: 0.9878\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0437 - acc: 0.9881\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0330 - acc: 0.9932\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0378 - acc: 0.9902\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0401 - acc: 0.9875\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0414 - acc: 0.9886\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0424 - acc: 0.9873\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0324 - acc: 0.9908\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0325 - acc: 0.9895\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0341 - acc: 0.9890\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0352 - acc: 0.9898\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 388us/step - loss: 0.0336 - acc: 0.9901\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0302 - acc: 0.9911\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0279 - acc: 0.9921\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0254 - acc: 0.9929\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0243 - acc: 0.9935\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0302 - acc: 0.9895\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0344 - acc: 0.9882\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0276 - acc: 0.9914\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0328 - acc: 0.9885\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0270 - acc: 0.9924\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0280 - acc: 0.9900\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0254 - acc: 0.9930\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0316 - acc: 0.9902\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0256 - acc: 0.9926\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0220 - acc: 0.9939\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0178 - acc: 0.9946\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0241 - acc: 0.9915\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0210 - acc: 0.9932\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0196 - acc: 0.9937\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 389us/step - loss: 0.0234 - acc: 0.9925\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0275 - acc: 0.9919\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0258 - acc: 0.9919\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 391us/step - loss: 0.0303 - acc: 0.9893\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0210 - acc: 0.9926\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0222 - acc: 0.9930\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0197 - acc: 0.9940\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0200 - acc: 0.9931\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0174 - acc: 0.9943\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0189 - acc: 0.9936\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0240 - acc: 0.9919\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0242 - acc: 0.9918\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0255 - acc: 0.9919\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0201 - acc: 0.9934\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0158 - acc: 0.9951\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0166 - acc: 0.9943\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0202 - acc: 0.9929\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0205 - acc: 0.9937\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0229 - acc: 0.9920\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0140 - acc: 0.9951\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0179 - acc: 0.9939\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0233 - acc: 0.9925\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0182 - acc: 0.9940\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0138 - acc: 0.9954\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0167 - acc: 0.9949\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0252 - acc: 0.9915\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0206 - acc: 0.9931\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0225 - acc: 0.9923\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0175 - acc: 0.9946\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0159 - acc: 0.9959\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0174 - acc: 0.9935\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0166 - acc: 0.9940\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0167 - acc: 0.9946\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0154 - acc: 0.9949\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0140 - acc: 0.9952\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0169 - acc: 0.9943\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0162 - acc: 0.9939\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0178 - acc: 0.9935\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0222 - acc: 0.9918\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 388us/step - loss: 0.0153 - acc: 0.9950\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0197 - acc: 0.9934\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0138 - acc: 0.9955\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0150 - acc: 0.9941\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0130 - acc: 0.9962\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0144 - acc: 0.9955\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0210 - acc: 0.9923\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0194 - acc: 0.9937\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0140 - acc: 0.9956\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0147 - acc: 0.9951\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 386us/step - loss: 0.0207 - acc: 0.9931\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0183 - acc: 0.9931\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0153 - acc: 0.9944\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0168 - acc: 0.9944\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 385us/step - loss: 0.0139 - acc: 0.9952\n",
            "Accuracy: 0.7431 (max: 0.7431)\n",
            "Non-other Macro-Averaged F1: 0.7082 (max: 0.7082)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0120 - acc: 0.9959\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0176 - acc: 0.9932\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0140 - acc: 0.9950\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0138 - acc: 0.9949\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0135 - acc: 0.9955\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0136 - acc: 0.9955\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0137 - acc: 0.9955\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0145 - acc: 0.9950\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0160 - acc: 0.9948\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0160 - acc: 0.9936\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0163 - acc: 0.9946\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0169 - acc: 0.9941\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0140 - acc: 0.9951\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0133 - acc: 0.9955\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0200 - acc: 0.9944\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0150 - acc: 0.9946\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0148 - acc: 0.9952\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0128 - acc: 0.9951\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0129 - acc: 0.9952\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0167 - acc: 0.9937\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0136 - acc: 0.9949\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0131 - acc: 0.9964\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0130 - acc: 0.9955\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0143 - acc: 0.9948\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0145 - acc: 0.9949\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0136 - acc: 0.9959\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0126 - acc: 0.9955\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0109 - acc: 0.9962\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0200 - acc: 0.9928\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0150 - acc: 0.9950\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0119 - acc: 0.9950\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0117 - acc: 0.9960\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0157 - acc: 0.9944\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0164 - acc: 0.9943\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0139 - acc: 0.9951\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0106 - acc: 0.9966\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0125 - acc: 0.9955\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0152 - acc: 0.9951\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0162 - acc: 0.9939\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0144 - acc: 0.9952\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0121 - acc: 0.9966\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0140 - acc: 0.9948\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0152 - acc: 0.9945\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0103 - acc: 0.9965\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0141 - acc: 0.9952\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0192 - acc: 0.9940\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0178 - acc: 0.9944\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0117 - acc: 0.9959\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0099 - acc: 0.9970\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0108 - acc: 0.9964\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0117 - acc: 0.9958\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0126 - acc: 0.9954\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0133 - acc: 0.9951\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0126 - acc: 0.9959\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0196 - acc: 0.9930\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0167 - acc: 0.9948\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0096 - acc: 0.9974\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0149 - acc: 0.9948\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0162 - acc: 0.9945\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0155 - acc: 0.9949\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0127 - acc: 0.9952\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0132 - acc: 0.9962\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0103 - acc: 0.9966\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0137 - acc: 0.9951\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0158 - acc: 0.9944\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0115 - acc: 0.9968\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0095 - acc: 0.9968\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0101 - acc: 0.9962\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0130 - acc: 0.9962\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0117 - acc: 0.9956\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0134 - acc: 0.9955\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0123 - acc: 0.9956\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0112 - acc: 0.9960\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0115 - acc: 0.9964\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0117 - acc: 0.9960\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0119 - acc: 0.9951\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0089 - acc: 0.9970\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0086 - acc: 0.9969\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0118 - acc: 0.9958\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0135 - acc: 0.9952\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0104 - acc: 0.9969\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0116 - acc: 0.9955\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0113 - acc: 0.9960\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0106 - acc: 0.9960\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0081 - acc: 0.9970\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0098 - acc: 0.9966\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0107 - acc: 0.9966\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0111 - acc: 0.9962\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0120 - acc: 0.9956\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0129 - acc: 0.9952\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0129 - acc: 0.9960\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0129 - acc: 0.9956\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0134 - acc: 0.9950\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0127 - acc: 0.9960\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0081 - acc: 0.9971\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0085 - acc: 0.9975\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0098 - acc: 0.9971\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0080 - acc: 0.9968\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0089 - acc: 0.9970\n",
            "Accuracy: 0.7413 (max: 0.7431)\n",
            "Non-other Macro-Averaged F1: 0.7090 (max: 0.7090)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0125 - acc: 0.9958\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0089 - acc: 0.9968\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0125 - acc: 0.9962\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0123 - acc: 0.9951\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0112 - acc: 0.9962\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0117 - acc: 0.9962\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0075 - acc: 0.9965\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0079 - acc: 0.9975\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0120 - acc: 0.9960\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0067 - acc: 0.9979\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0140 - acc: 0.9949\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0116 - acc: 0.9961\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0175 - acc: 0.9945\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0133 - acc: 0.9962\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0139 - acc: 0.9958\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0093 - acc: 0.9976\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0102 - acc: 0.9968\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0099 - acc: 0.9958\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0121 - acc: 0.9956\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0111 - acc: 0.9968\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0085 - acc: 0.9969\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0119 - acc: 0.9959\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0084 - acc: 0.9970\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0079 - acc: 0.9969\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0122 - acc: 0.9964\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0096 - acc: 0.9970\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0113 - acc: 0.9955\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0101 - acc: 0.9965\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0115 - acc: 0.9962\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0134 - acc: 0.9950\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0122 - acc: 0.9956\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0085 - acc: 0.9969\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0105 - acc: 0.9960\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0103 - acc: 0.9959\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0147 - acc: 0.9952\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0111 - acc: 0.9965\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0105 - acc: 0.9961\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0078 - acc: 0.9971\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0082 - acc: 0.9975\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0111 - acc: 0.9969\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0146 - acc: 0.9949\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0098 - acc: 0.9974\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0095 - acc: 0.9971\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0133 - acc: 0.9960\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0098 - acc: 0.9965\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0091 - acc: 0.9964\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0057 - acc: 0.9979\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0074 - acc: 0.9972\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0074 - acc: 0.9974\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0049 - acc: 0.9987\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0135 - acc: 0.9958\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0125 - acc: 0.9961\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0109 - acc: 0.9960\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0088 - acc: 0.9974\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0081 - acc: 0.9965\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0126 - acc: 0.9951\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0094 - acc: 0.9971\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0079 - acc: 0.9970\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0078 - acc: 0.9981\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0084 - acc: 0.9970\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 369us/step - loss: 0.0115 - acc: 0.9958\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0093 - acc: 0.9968\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0095 - acc: 0.9958\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0122 - acc: 0.9956\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0111 - acc: 0.9969\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0117 - acc: 0.9960\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0112 - acc: 0.9959\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0096 - acc: 0.9964\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0089 - acc: 0.9974\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0079 - acc: 0.9972\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0093 - acc: 0.9975\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0054 - acc: 0.9982\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0100 - acc: 0.9969\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0099 - acc: 0.9964\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0123 - acc: 0.9965\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0079 - acc: 0.9980\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0072 - acc: 0.9971\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0093 - acc: 0.9961\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0113 - acc: 0.9960\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0100 - acc: 0.9962\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0086 - acc: 0.9972\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0113 - acc: 0.9958\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0061 - acc: 0.9978\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0117 - acc: 0.9959\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0073 - acc: 0.9975\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0102 - acc: 0.9969\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0133 - acc: 0.9952\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0085 - acc: 0.9972\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0081 - acc: 0.9975\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0066 - acc: 0.9975\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0079 - acc: 0.9972\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0099 - acc: 0.9966\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0089 - acc: 0.9968\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0118 - acc: 0.9958\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0075 - acc: 0.9975\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0085 - acc: 0.9971\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0099 - acc: 0.9969\n",
            "Accuracy: 0.7438 (max: 0.7438)\n",
            "Non-other Macro-Averaged F1: 0.7000 (max: 0.7090)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 360us/step - loss: 0.0084 - acc: 0.9972\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 360us/step - loss: 0.0091 - acc: 0.9968\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0108 - acc: 0.9958\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0083 - acc: 0.9970\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0072 - acc: 0.9972\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0086 - acc: 0.9970\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 366us/step - loss: 0.0081 - acc: 0.9974\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 366us/step - loss: 0.0093 - acc: 0.9966\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0123 - acc: 0.9950\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0066 - acc: 0.9974\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0076 - acc: 0.9974\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0108 - acc: 0.9965\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0072 - acc: 0.9969\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0058 - acc: 0.9978\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0102 - acc: 0.9965\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0144 - acc: 0.9952\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0110 - acc: 0.9972\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0106 - acc: 0.9960\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0074 - acc: 0.9974\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0061 - acc: 0.9972\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 365us/step - loss: 0.0110 - acc: 0.9955\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0103 - acc: 0.9965\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0136 - acc: 0.9952\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0066 - acc: 0.9980\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0073 - acc: 0.9975\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0063 - acc: 0.9974\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0082 - acc: 0.9970\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0085 - acc: 0.9972\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0115 - acc: 0.9960\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 361us/step - loss: 0.0087 - acc: 0.9974\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0092 - acc: 0.9965\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0104 - acc: 0.9955\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0086 - acc: 0.9972\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0111 - acc: 0.9974\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0107 - acc: 0.9966\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0112 - acc: 0.9961\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0152 - acc: 0.9939\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0117 - acc: 0.9959\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0100 - acc: 0.9969\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 366us/step - loss: 0.0104 - acc: 0.9969\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0079 - acc: 0.9980\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0042 - acc: 0.9986\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 360us/step - loss: 0.0077 - acc: 0.9972\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 365us/step - loss: 0.0061 - acc: 0.9980\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 365us/step - loss: 0.0102 - acc: 0.9962\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0088 - acc: 0.9976\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 361us/step - loss: 0.0069 - acc: 0.9972\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 361us/step - loss: 0.0065 - acc: 0.9974\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0058 - acc: 0.9978\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0059 - acc: 0.9978\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0096 - acc: 0.9968\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 361us/step - loss: 0.0093 - acc: 0.9969\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0132 - acc: 0.9952\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 360us/step - loss: 0.0104 - acc: 0.9969\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0100 - acc: 0.9966\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 363us/step - loss: 0.0101 - acc: 0.9960\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 362us/step - loss: 0.0056 - acc: 0.9981\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 361us/step - loss: 0.0086 - acc: 0.9974\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0111 - acc: 0.9960\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 4s 476us/step - loss: 0.0062 - acc: 0.9980\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 4s 472us/step - loss: 0.0075 - acc: 0.9971\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0069 - acc: 0.9972\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0059 - acc: 0.9976\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0102 - acc: 0.9961\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0110 - acc: 0.9966\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0073 - acc: 0.9976\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0074 - acc: 0.9974\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0077 - acc: 0.9972\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0061 - acc: 0.9980\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0063 - acc: 0.9976\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0087 - acc: 0.9966\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0074 - acc: 0.9975\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0099 - acc: 0.9962\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0077 - acc: 0.9975\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0065 - acc: 0.9978\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0071 - acc: 0.9975\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0047 - acc: 0.9985\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0080 - acc: 0.9969\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0090 - acc: 0.9975\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0111 - acc: 0.9962\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0096 - acc: 0.9968\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0064 - acc: 0.9979\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0043 - acc: 0.9986\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0057 - acc: 0.9978\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0045 - acc: 0.9986\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0064 - acc: 0.9979\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0065 - acc: 0.9982\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0084 - acc: 0.9971\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0083 - acc: 0.9972\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0095 - acc: 0.9966\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0054 - acc: 0.9976\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0112 - acc: 0.9966\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0086 - acc: 0.9970\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0080 - acc: 0.9978\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0072 - acc: 0.9974\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0057 - acc: 0.9987\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0063 - acc: 0.9976\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0083 - acc: 0.9979\n",
            "Accuracy: 0.7416 (max: 0.7438)\n",
            "Non-other Macro-Averaged F1: 0.7145 (max: 0.7145)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0115 - acc: 0.9962\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0082 - acc: 0.9969\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0101 - acc: 0.9958\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0094 - acc: 0.9965\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0075 - acc: 0.9976\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0075 - acc: 0.9976\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0090 - acc: 0.9972\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0078 - acc: 0.9966\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0061 - acc: 0.9980\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0064 - acc: 0.9976\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9989\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0098 - acc: 0.9971\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0064 - acc: 0.9979\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0061 - acc: 0.9979\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0105 - acc: 0.9966\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0089 - acc: 0.9971\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0083 - acc: 0.9974\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0069 - acc: 0.9975\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0051 - acc: 0.9981\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0080 - acc: 0.9970\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0082 - acc: 0.9972\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0072 - acc: 0.9979\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0080 - acc: 0.9966\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0055 - acc: 0.9984\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0058 - acc: 0.9979\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0065 - acc: 0.9978\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0107 - acc: 0.9962\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0073 - acc: 0.9975\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0097 - acc: 0.9969\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0084 - acc: 0.9974\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0104 - acc: 0.9968\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0082 - acc: 0.9969\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0088 - acc: 0.9970\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0091 - acc: 0.9968\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0070 - acc: 0.9971\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0048 - acc: 0.9987\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0055 - acc: 0.9981\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0082 - acc: 0.9975\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0048 - acc: 0.9987\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0097 - acc: 0.9965\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0073 - acc: 0.9971\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0096 - acc: 0.9961\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0099 - acc: 0.9968\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0074 - acc: 0.9978\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0056 - acc: 0.9978\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0081 - acc: 0.9970\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0095 - acc: 0.9970\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0046 - acc: 0.9987\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0074 - acc: 0.9972\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0105 - acc: 0.9969\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0056 - acc: 0.9978\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0098 - acc: 0.9965\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0095 - acc: 0.9966\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0082 - acc: 0.9961\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0086 - acc: 0.9964\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0069 - acc: 0.9978\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 371us/step - loss: 0.0069 - acc: 0.9975\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0067 - acc: 0.9974\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0064 - acc: 0.9974\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0055 - acc: 0.9978\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0071 - acc: 0.9975\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0076 - acc: 0.9976\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0045 - acc: 0.9989\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0042 - acc: 0.9985\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0032 - acc: 0.9987\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0055 - acc: 0.9980\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0064 - acc: 0.9975\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0060 - acc: 0.9980\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0084 - acc: 0.9964\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0096 - acc: 0.9970\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0045 - acc: 0.9984\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0069 - acc: 0.9976\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0057 - acc: 0.9978\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0104 - acc: 0.9961\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0081 - acc: 0.9971\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0055 - acc: 0.9976\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0061 - acc: 0.9982\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0096 - acc: 0.9970\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0107 - acc: 0.9964\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0074 - acc: 0.9975\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 387us/step - loss: 0.0070 - acc: 0.9974\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0080 - acc: 0.9974\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0055 - acc: 0.9982\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0097 - acc: 0.9965\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0078 - acc: 0.9971\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0072 - acc: 0.9976\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0077 - acc: 0.9975\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0103 - acc: 0.9968\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0072 - acc: 0.9976\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0080 - acc: 0.9970\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0058 - acc: 0.9982\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0060 - acc: 0.9981\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0073 - acc: 0.9970\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0043 - acc: 0.9985\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0067 - acc: 0.9976\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0062 - acc: 0.9974\n",
            "Accuracy: 0.7394 (max: 0.7438)\n",
            "Non-other Macro-Averaged F1: 0.6968 (max: 0.7145)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0087 - acc: 0.9964\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0051 - acc: 0.9981\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0085 - acc: 0.9971\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0072 - acc: 0.9979\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9981\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0059 - acc: 0.9982\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0077 - acc: 0.9980\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0055 - acc: 0.9981\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0079 - acc: 0.9975\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0084 - acc: 0.9976\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0081 - acc: 0.9976\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0061 - acc: 0.9978\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0087 - acc: 0.9974\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0095 - acc: 0.9969\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0070 - acc: 0.9981\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0057 - acc: 0.9980\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0056 - acc: 0.9978\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0077 - acc: 0.9971\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0068 - acc: 0.9974\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0093 - acc: 0.9975\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0074 - acc: 0.9974\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0063 - acc: 0.9981\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0033 - acc: 0.9990\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0063 - acc: 0.9979\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0104 - acc: 0.9968\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0056 - acc: 0.9979\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0066 - acc: 0.9980\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0069 - acc: 0.9980\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0089 - acc: 0.9975\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0043 - acc: 0.9981\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0073 - acc: 0.9972\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0041 - acc: 0.9984\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0061 - acc: 0.9975\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0051 - acc: 0.9982\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0084 - acc: 0.9968\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0067 - acc: 0.9972\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0087 - acc: 0.9974\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0067 - acc: 0.9979\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0054 - acc: 0.9979\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0093 - acc: 0.9970\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0139 - acc: 0.9955\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0067 - acc: 0.9971\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0068 - acc: 0.9974\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0068 - acc: 0.9976\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0065 - acc: 0.9976\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0074 - acc: 0.9979\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0078 - acc: 0.9970\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0062 - acc: 0.9979\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0072 - acc: 0.9970\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0050 - acc: 0.9985\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0125 - acc: 0.9962\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0066 - acc: 0.9976\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0074 - acc: 0.9970\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0053 - acc: 0.9985\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0045 - acc: 0.9984\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0095 - acc: 0.9974\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0064 - acc: 0.9979\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0054 - acc: 0.9979\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0061 - acc: 0.9979\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0080 - acc: 0.9971\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0081 - acc: 0.9979\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0080 - acc: 0.9981\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0064 - acc: 0.9975\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0059 - acc: 0.9982\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0054 - acc: 0.9979\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0055 - acc: 0.9982\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0069 - acc: 0.9979\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0073 - acc: 0.9975\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0056 - acc: 0.9974\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0078 - acc: 0.9976\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0099 - acc: 0.9965\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0064 - acc: 0.9982\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0046 - acc: 0.9986\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0078 - acc: 0.9980\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0079 - acc: 0.9974\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0082 - acc: 0.9969\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0055 - acc: 0.9981\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0093 - acc: 0.9972\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0052 - acc: 0.9985\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0062 - acc: 0.9972\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0078 - acc: 0.9978\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0064 - acc: 0.9970\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0079 - acc: 0.9968\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9985\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0034 - acc: 0.9990\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0056 - acc: 0.9979\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0065 - acc: 0.9975\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0048 - acc: 0.9986\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0081 - acc: 0.9975\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0082 - acc: 0.9976\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0074 - acc: 0.9976\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0075 - acc: 0.9971\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0045 - acc: 0.9985\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0070 - acc: 0.9975\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0047 - acc: 0.9986\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0069 - acc: 0.9979\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0059 - acc: 0.9975\n",
            "Accuracy: 0.7479 (max: 0.7479)\n",
            "Non-other Macro-Averaged F1: 0.7124 (max: 0.7145)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0056 - acc: 0.9979\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0072 - acc: 0.9975\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0047 - acc: 0.9982\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0056 - acc: 0.9980\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0088 - acc: 0.9968\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0080 - acc: 0.9974\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0080 - acc: 0.9970\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0044 - acc: 0.9984\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0081 - acc: 0.9974\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0065 - acc: 0.9978\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0059 - acc: 0.9981\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0062 - acc: 0.9982\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0073 - acc: 0.9980\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0053 - acc: 0.9981\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0069 - acc: 0.9976\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0077 - acc: 0.9978\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0041 - acc: 0.9978\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0066 - acc: 0.9978\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0046 - acc: 0.9986\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0058 - acc: 0.9982\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0069 - acc: 0.9975\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0091 - acc: 0.9976\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0059 - acc: 0.9980\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0054 - acc: 0.9982\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0081 - acc: 0.9976\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0056 - acc: 0.9981\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0050 - acc: 0.9984\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0047 - acc: 0.9982\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0044 - acc: 0.9986\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0074 - acc: 0.9978\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0065 - acc: 0.9980\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0050 - acc: 0.9979\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0083 - acc: 0.9972\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0043 - acc: 0.9985\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0062 - acc: 0.9978\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0072 - acc: 0.9976\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0098 - acc: 0.9960\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0081 - acc: 0.9975\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0067 - acc: 0.9976\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0056 - acc: 0.9975\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0041 - acc: 0.9985\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0057 - acc: 0.9980\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0036 - acc: 0.9989\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0087 - acc: 0.9979\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0052 - acc: 0.9979\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0074 - acc: 0.9975\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0043 - acc: 0.9985\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0036 - acc: 0.9986\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0051 - acc: 0.9985\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0059 - acc: 0.9978\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0052 - acc: 0.9984\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0082 - acc: 0.9978\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0060 - acc: 0.9982\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0095 - acc: 0.9971\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0035 - acc: 0.9990\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 366us/step - loss: 0.0052 - acc: 0.9984\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0064 - acc: 0.9981\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0054 - acc: 0.9986\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0035 - acc: 0.9990\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0068 - acc: 0.9984\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0056 - acc: 0.9981\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0094 - acc: 0.9974\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0045 - acc: 0.9984\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0089 - acc: 0.9972\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0050 - acc: 0.9979\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0047 - acc: 0.9987\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0031 - acc: 0.9990\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0063 - acc: 0.9979\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0061 - acc: 0.9981\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0044 - acc: 0.9984\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0054 - acc: 0.9972\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0072 - acc: 0.9974\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0055 - acc: 0.9980\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0069 - acc: 0.9975\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0091 - acc: 0.9974\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0071 - acc: 0.9976\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0048 - acc: 0.9978\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0052 - acc: 0.9982\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0053 - acc: 0.9985\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0063 - acc: 0.9975\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0074 - acc: 0.9974\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0066 - acc: 0.9978\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0059 - acc: 0.9980\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0048 - acc: 0.9985\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0045 - acc: 0.9985\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0044 - acc: 0.9986\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0051 - acc: 0.9984\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0069 - acc: 0.9974\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0050 - acc: 0.9980\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0036 - acc: 0.9989\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0070 - acc: 0.9974\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 383us/step - loss: 0.0058 - acc: 0.9981\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0051 - acc: 0.9982\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0085 - acc: 0.9975\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0060 - acc: 0.9981\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0050 - acc: 0.9984\n",
            "Accuracy: 0.7508 (max: 0.7508)\n",
            "Non-other Macro-Averaged F1: 0.7196 (max: 0.7196)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0040 - acc: 0.9985\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0030 - acc: 0.9989\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0031 - acc: 0.9993\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0068 - acc: 0.9971\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0038 - acc: 0.9989\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0063 - acc: 0.9984\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0053 - acc: 0.9980\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0073 - acc: 0.9979\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0080 - acc: 0.9974\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0049 - acc: 0.9984\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0066 - acc: 0.9972\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0067 - acc: 0.9979\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0054 - acc: 0.9982\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0068 - acc: 0.9972\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0097 - acc: 0.9966\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0067 - acc: 0.9971\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0049 - acc: 0.9982\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0058 - acc: 0.9978\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0050 - acc: 0.9980\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0041 - acc: 0.9986\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0045 - acc: 0.9987\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0056 - acc: 0.9981\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0085 - acc: 0.9971\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0051 - acc: 0.9982\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0068 - acc: 0.9974\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0045 - acc: 0.9986\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0068 - acc: 0.9980\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0052 - acc: 0.9978\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0083 - acc: 0.9978\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0032 - acc: 0.9987\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0050 - acc: 0.9979\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0057 - acc: 0.9980\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0041 - acc: 0.9986\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0036 - acc: 0.9993\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0050 - acc: 0.9987\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0044 - acc: 0.9985\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0043 - acc: 0.9985\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0039 - acc: 0.9987\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0050 - acc: 0.9987\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0044 - acc: 0.9986\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0063 - acc: 0.9975\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0100 - acc: 0.9965\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0062 - acc: 0.9981\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0073 - acc: 0.9976\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0054 - acc: 0.9985\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0078 - acc: 0.9975\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0075 - acc: 0.9975\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0053 - acc: 0.9984\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0054 - acc: 0.9982\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0053 - acc: 0.9978\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0055 - acc: 0.9978\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 384us/step - loss: 0.0038 - acc: 0.9981\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0030 - acc: 0.9991\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0088 - acc: 0.9971\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0060 - acc: 0.9984\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0051 - acc: 0.9981\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0071 - acc: 0.9976\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0054 - acc: 0.9978\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0056 - acc: 0.9981\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0049 - acc: 0.9984\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0056 - acc: 0.9985\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0058 - acc: 0.9976\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0068 - acc: 0.9979\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0045 - acc: 0.9980\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0051 - acc: 0.9979\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0046 - acc: 0.9981\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0049 - acc: 0.9982\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0042 - acc: 0.9985\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0067 - acc: 0.9984\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0030 - acc: 0.9991\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0029 - acc: 0.9987\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0056 - acc: 0.9984\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0032 - acc: 0.9993\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0051 - acc: 0.9986\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0056 - acc: 0.9976\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0061 - acc: 0.9979\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0033 - acc: 0.9989\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 369us/step - loss: 0.0041 - acc: 0.9981\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0039 - acc: 0.9985\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0031 - acc: 0.9991\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0054 - acc: 0.9979\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0071 - acc: 0.9981\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0034 - acc: 0.9987\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 380us/step - loss: 0.0066 - acc: 0.9978\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0054 - acc: 0.9982\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0048 - acc: 0.9986\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0030 - acc: 0.9986\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0050 - acc: 0.9984\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0057 - acc: 0.9985\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0064 - acc: 0.9985\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 382us/step - loss: 0.0091 - acc: 0.9980\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0068 - acc: 0.9976\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0090 - acc: 0.9975\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9989\n",
            "Accuracy: 0.7391 (max: 0.7508)\n",
            "Non-other Macro-Averaged F1: 0.7081 (max: 0.7196)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0057 - acc: 0.9978\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0073 - acc: 0.9969\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0055 - acc: 0.9985\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0043 - acc: 0.9981\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0071 - acc: 0.9971\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0046 - acc: 0.9982\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0052 - acc: 0.9982\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9987\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0053 - acc: 0.9985\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0050 - acc: 0.9984\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0050 - acc: 0.9986\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0035 - acc: 0.9987\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0046 - acc: 0.9984\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0077 - acc: 0.9970\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0039 - acc: 0.9990\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0066 - acc: 0.9978\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0033 - acc: 0.9991\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0052 - acc: 0.9980\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0081 - acc: 0.9981\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0066 - acc: 0.9981\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0045 - acc: 0.9981\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0081 - acc: 0.9975\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0056 - acc: 0.9979\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0087 - acc: 0.9975\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0036 - acc: 0.9990\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0038 - acc: 0.9990\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0040 - acc: 0.9986\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0061 - acc: 0.9980\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0029 - acc: 0.9993\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0053 - acc: 0.9981\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0047 - acc: 0.9984\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0049 - acc: 0.9979\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0023 - acc: 0.9995\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0091 - acc: 0.9969\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0056 - acc: 0.9978\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0086 - acc: 0.9970\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0023 - acc: 0.9993\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0043 - acc: 0.9987\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0040 - acc: 0.9984\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0078 - acc: 0.9980\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0045 - acc: 0.9984\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0049 - acc: 0.9984\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0053 - acc: 0.9985\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0068 - acc: 0.9975\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0056 - acc: 0.9987\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0044 - acc: 0.9984\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0063 - acc: 0.9982\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0060 - acc: 0.9981\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0067 - acc: 0.9976\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0033 - acc: 0.9987\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0052 - acc: 0.9984\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0038 - acc: 0.9990\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0043 - acc: 0.9985\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 365us/step - loss: 0.0051 - acc: 0.9981\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 364us/step - loss: 0.0037 - acc: 0.9989\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0049 - acc: 0.9976\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0062 - acc: 0.9978\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0067 - acc: 0.9978\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0065 - acc: 0.9980\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0033 - acc: 0.9990\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0051 - acc: 0.9979\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0038 - acc: 0.9989\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0092 - acc: 0.9976\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0062 - acc: 0.9978\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0065 - acc: 0.9974\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0036 - acc: 0.9985\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0052 - acc: 0.9987\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0023 - acc: 0.9994\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0046 - acc: 0.9979\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0070 - acc: 0.9974\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0118 - acc: 0.9966\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0024 - acc: 0.9989\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0028 - acc: 0.9990\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0030 - acc: 0.9987\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0079 - acc: 0.9979\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0039 - acc: 0.9981\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0078 - acc: 0.9974\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0051 - acc: 0.9982\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0060 - acc: 0.9978\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0055 - acc: 0.9982\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 370us/step - loss: 0.0057 - acc: 0.9981\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 369us/step - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 371us/step - loss: 0.0025 - acc: 0.9993\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0025 - acc: 0.9991\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0020 - acc: 0.9994\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0043 - acc: 0.9986\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0064 - acc: 0.9976\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0049 - acc: 0.9984\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0051 - acc: 0.9980\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0051 - acc: 0.9980\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0042 - acc: 0.9982\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0034 - acc: 0.9985\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 370us/step - loss: 0.0027 - acc: 0.9993\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0060 - acc: 0.9978\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0031 - acc: 0.9986\n",
            "Accuracy: 0.7490 (max: 0.7508)\n",
            "Non-other Macro-Averaged F1: 0.7059 (max: 0.7196)\n",
            "\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0063 - acc: 0.9978\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0040 - acc: 0.9987\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0073 - acc: 0.9980\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0037 - acc: 0.9987\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0023 - acc: 0.9993\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0060 - acc: 0.9980\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0048 - acc: 0.9979\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9984\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0035 - acc: 0.9987\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0048 - acc: 0.9984\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0079 - acc: 0.9970\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0053 - acc: 0.9981\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0061 - acc: 0.9981\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0056 - acc: 0.9978\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0084 - acc: 0.9975\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0067 - acc: 0.9981\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0057 - acc: 0.9975\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0049 - acc: 0.9985\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0024 - acc: 0.9994\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 371us/step - loss: 0.0048 - acc: 0.9982\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0047 - acc: 0.9982\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0030 - acc: 0.9985\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0027 - acc: 0.9990\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0042 - acc: 0.9984\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0027 - acc: 0.9989\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0047 - acc: 0.9985\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0055 - acc: 0.9976\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0061 - acc: 0.9979\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0035 - acc: 0.9989\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0063 - acc: 0.9978\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0052 - acc: 0.9982\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0075 - acc: 0.9980\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0046 - acc: 0.9984\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0079 - acc: 0.9976\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0047 - acc: 0.9984\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 381us/step - loss: 0.0062 - acc: 0.9981\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0048 - acc: 0.9985\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0045 - acc: 0.9981\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0063 - acc: 0.9980\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0048 - acc: 0.9982\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0067 - acc: 0.9974\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0074 - acc: 0.9972\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0038 - acc: 0.9989\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0062 - acc: 0.9979\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0038 - acc: 0.9989\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0045 - acc: 0.9985\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0052 - acc: 0.9981\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0054 - acc: 0.9980\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 372us/step - loss: 0.0036 - acc: 0.9990\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 371us/step - loss: 0.0031 - acc: 0.9990\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0032 - acc: 0.9991\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0059 - acc: 0.9985\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0055 - acc: 0.9980\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0040 - acc: 0.9984\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0034 - acc: 0.9986\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0064 - acc: 0.9972\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0046 - acc: 0.9987\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0058 - acc: 0.9979\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0038 - acc: 0.9987\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0055 - acc: 0.9980\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0033 - acc: 0.9989\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0058 - acc: 0.9976\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0049 - acc: 0.9979\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0068 - acc: 0.9981\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0050 - acc: 0.9980\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0050 - acc: 0.9980\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0035 - acc: 0.9990\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0037 - acc: 0.9986\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0027 - acc: 0.9986\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 373us/step - loss: 0.0025 - acc: 0.9989\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0032 - acc: 0.9990\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0025 - acc: 0.9989\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0027 - acc: 0.9991\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0043 - acc: 0.9985\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 379us/step - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0029 - acc: 0.9989\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0019 - acc: 0.9993\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0056 - acc: 0.9985\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0027 - acc: 0.9994\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0053 - acc: 0.9985\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0044 - acc: 0.9981\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0048 - acc: 0.9984\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 369us/step - loss: 0.0033 - acc: 0.9989\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 374us/step - loss: 0.0041 - acc: 0.9984\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0050 - acc: 0.9982\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0064 - acc: 0.9982\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0030 - acc: 0.9991\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 377us/step - loss: 0.0023 - acc: 0.9993\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 375us/step - loss: 0.0037 - acc: 0.9985\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0029 - acc: 0.9991\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0043 - acc: 0.9987\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 376us/step - loss: 0.0040 - acc: 0.9989\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 378us/step - loss: 0.0062 - acc: 0.9981\n",
            "Accuracy: 0.7545 (max: 0.7545)\n",
            "Non-other Macro-Averaged F1: 0.7198 (max: 0.7198)\n",
            "\n",
            "F1: [0.7082089225809326, 0.7090304349807406, 0.7000313129773705, 0.7145204476172503, 0.696836022057299, 0.7124122249604946, 0.7196116502310831, 0.7081461587936199, 0.7059316507468598, 0.7198158565427079]\n",
            "Accuracy: [0.743099006256901, 0.7412587412587412, 0.7438351122561649, 0.7416267942583732, 0.7394184762605815, 0.7478836952521163, 0.7508281192491719, 0.7390504232609496, 0.7489878542510121, 0.7545086492454913]\n",
            "F1 mean:  0.7094544681488357\n",
            "Accuracy mean:  0.7450496871549503\n",
            "F1 SD:  0.0071331152345130025\n",
            "Accuracy SD:  0.004954518355966027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "elB2ltIklCNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentence Polarity classification"
      ]
    },
    {
      "metadata": {
        "id": "GGw7AL5Jm9d5",
        "colab_type": "code",
        "outputId": "a24edc06-c5e4-4f81-9bac-05acfb2cbe57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a = [1,2,3,4]\n",
        "print(np.multiply(a,10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10 20 30 40]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "98PPQLk0NRIM",
        "colab_type": "code",
        "outputId": "a8df10b4-46ba-489e-bf3a-cfbcf62d909f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "%cd conceptor/Extrinsic-Evaluation-tasks-master\n",
        "%cd sentence_polarity_classification\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master\n",
            "/content/conceptor/Extrinsic-Evaluation-tasks-master/sentence_polarity_classification\n",
            "data  pkl  preprocess.py  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j9BO3mo0Nchb",
        "colab_type": "code",
        "outputId": "db15a1b7-f4f6-4d74-d973-beaa306360c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9642
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 preprocess.py\n",
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/train.txt 5000 sentences\n",
            "data/dev.txt 2500 sentences\n",
            "data/test.txt 2500 sentences\n",
            "Enter embedding typeword2vec\n",
            "tcmalloc: large alloc 3600007168 bytes == 0x3842000 @  0x7f10af779001 0x7f10ad4adb85 0x7f10ad510b43 0x7f10ad512a86 0x7f10ad5aa868 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f10af374b97 0x5afa0a\n",
            "The embedding has been loaded from gensim!\n",
            "Load dataset\n",
            "Load pre-trained embeddings file\n",
            "Conceptor? n\n",
            "Embeddings shape:  (17469, 300)\n",
            "Len words:  23908\n",
            "Unknown tokens: 26.88%\n",
            "Unknown tokens: 26.75%\n",
            "Unknown tokens: 26.84%\n",
            "Data stored in pkl folder\n",
            "Using TensorFlow backend.\n",
            "data loaded!\n",
            "Longest sentence: 120\n",
            "X_train shape: (5000, 120)\n",
            "X_dev shape: (2500, 120)\n",
            "X_test shape: (2500, 120)\n",
            "y_train shape: (5000,)\n",
            "Build model...\n",
            "2019-02-01 00:35:39.846246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-02-01 00:35:39.846764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-02-01 00:35:39.846804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-02-01 00:35:40.196730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-02-01 00:35:40.196843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-02-01 00:35:40.196862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-02-01 00:35:40.197223: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-02-01 00:35:40.197310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "words_input (InputLayer)     (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 120, 300)          5240700   \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 5,241,001\n",
            "Trainable params: 301\n",
            "Non-trainable params: 5,240,700\n",
            "_________________________________________________________________\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 1s 132us/step - loss: 0.6279 - acc: 0.6640 - val_loss: 0.4396 - val_acc: 0.8116\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.3974 - acc: 0.8344 - val_loss: 0.3378 - val_acc: 0.8720\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.3374 - acc: 0.8652 - val_loss: 0.2959 - val_acc: 0.8904\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.3084 - acc: 0.8800 - val_loss: 0.2787 - val_acc: 0.8936\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2911 - acc: 0.8882 - val_loss: 0.2607 - val_acc: 0.9000\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2801 - acc: 0.8926 - val_loss: 0.2516 - val_acc: 0.9040\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2720 - acc: 0.8908 - val_loss: 0.2518 - val_acc: 0.8980\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2651 - acc: 0.8958 - val_loss: 0.2407 - val_acc: 0.9076\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2615 - acc: 0.8990 - val_loss: 0.2525 - val_acc: 0.8960\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2566 - acc: 0.9028 - val_loss: 0.2346 - val_acc: 0.9072\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2520 - acc: 0.9024 - val_loss: 0.2321 - val_acc: 0.9096\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2508 - acc: 0.9034 - val_loss: 0.2292 - val_acc: 0.9072\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2466 - acc: 0.9058 - val_loss: 0.2310 - val_acc: 0.9100\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2434 - acc: 0.9066 - val_loss: 0.2301 - val_acc: 0.9116\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2425 - acc: 0.9060 - val_loss: 0.2261 - val_acc: 0.9116\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2396 - acc: 0.9070 - val_loss: 0.2245 - val_acc: 0.9108\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2364 - acc: 0.9084 - val_loss: 0.2343 - val_acc: 0.9080\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2359 - acc: 0.9112 - val_loss: 0.2259 - val_acc: 0.9132\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2341 - acc: 0.9104 - val_loss: 0.2243 - val_acc: 0.9148\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2334 - acc: 0.9096 - val_loss: 0.2224 - val_acc: 0.9128\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2337 - acc: 0.9094 - val_loss: 0.2276 - val_acc: 0.9124\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2308 - acc: 0.9104 - val_loss: 0.2221 - val_acc: 0.9136\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2287 - acc: 0.9120 - val_loss: 0.2250 - val_acc: 0.9140\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2278 - acc: 0.9128 - val_loss: 0.2267 - val_acc: 0.9104\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2281 - acc: 0.9116 - val_loss: 0.2224 - val_acc: 0.9160\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2270 - acc: 0.9112 - val_loss: 0.2238 - val_acc: 0.9116\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2274 - acc: 0.9118 - val_loss: 0.2224 - val_acc: 0.9160\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2252 - acc: 0.9126 - val_loss: 0.2230 - val_acc: 0.9128\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2235 - acc: 0.9152 - val_loss: 0.2221 - val_acc: 0.9148\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2241 - acc: 0.9156 - val_loss: 0.2246 - val_acc: 0.9128\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2224 - acc: 0.9148 - val_loss: 0.2220 - val_acc: 0.9148\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2215 - acc: 0.9154 - val_loss: 0.2232 - val_acc: 0.9132\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2225 - acc: 0.9134 - val_loss: 0.2223 - val_acc: 0.9124\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2208 - acc: 0.9146 - val_loss: 0.2216 - val_acc: 0.9140\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2200 - acc: 0.9158 - val_loss: 0.2216 - val_acc: 0.9128\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2200 - acc: 0.9150 - val_loss: 0.2221 - val_acc: 0.9124\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2192 - acc: 0.9164 - val_loss: 0.2260 - val_acc: 0.9112\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2204 - acc: 0.9174 - val_loss: 0.2219 - val_acc: 0.9132\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2192 - acc: 0.9154 - val_loss: 0.2220 - val_acc: 0.9144\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2181 - acc: 0.9180 - val_loss: 0.2221 - val_acc: 0.9128\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2192 - acc: 0.9160 - val_loss: 0.2315 - val_acc: 0.9076\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2222 - acc: 0.9152 - val_loss: 0.2222 - val_acc: 0.9132\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2173 - acc: 0.9168 - val_loss: 0.2227 - val_acc: 0.9120\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2166 - acc: 0.9162 - val_loss: 0.2231 - val_acc: 0.9144\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2184 - acc: 0.9140 - val_loss: 0.2226 - val_acc: 0.9132\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2159 - acc: 0.9218 - val_loss: 0.2226 - val_acc: 0.9128\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2186 - acc: 0.9162 - val_loss: 0.2238 - val_acc: 0.9128\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2158 - acc: 0.9170 - val_loss: 0.2230 - val_acc: 0.9152\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2159 - acc: 0.9162 - val_loss: 0.2238 - val_acc: 0.9124\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2151 - acc: 0.9180 - val_loss: 0.2238 - val_acc: 0.9116\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2140 - acc: 0.9198 - val_loss: 0.2239 - val_acc: 0.9140\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2142 - acc: 0.9184 - val_loss: 0.2245 - val_acc: 0.9128\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2162 - acc: 0.9156 - val_loss: 0.2302 - val_acc: 0.9092\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2142 - acc: 0.9178 - val_loss: 0.2241 - val_acc: 0.9128\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2136 - acc: 0.9182 - val_loss: 0.2244 - val_acc: 0.9108\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2132 - acc: 0.9186 - val_loss: 0.2286 - val_acc: 0.9108\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2131 - acc: 0.9174 - val_loss: 0.2334 - val_acc: 0.9068\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2133 - acc: 0.9184 - val_loss: 0.2257 - val_acc: 0.9120\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2124 - acc: 0.9208 - val_loss: 0.2249 - val_acc: 0.9128\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2134 - acc: 0.9174 - val_loss: 0.2262 - val_acc: 0.9108\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2124 - acc: 0.9198 - val_loss: 0.2263 - val_acc: 0.9124\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2111 - acc: 0.9212 - val_loss: 0.2260 - val_acc: 0.9136\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2112 - acc: 0.9194 - val_loss: 0.2264 - val_acc: 0.9124\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2120 - acc: 0.9208 - val_loss: 0.2355 - val_acc: 0.9040\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2142 - acc: 0.9160 - val_loss: 0.2280 - val_acc: 0.9116\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2121 - acc: 0.9210 - val_loss: 0.2260 - val_acc: 0.9120\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2116 - acc: 0.9190 - val_loss: 0.2329 - val_acc: 0.9056\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2121 - acc: 0.9198 - val_loss: 0.2304 - val_acc: 0.9088\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2106 - acc: 0.9200 - val_loss: 0.2264 - val_acc: 0.9112\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2115 - acc: 0.9236 - val_loss: 0.2264 - val_acc: 0.9132\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2119 - acc: 0.9190 - val_loss: 0.2271 - val_acc: 0.9124\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2110 - acc: 0.9196 - val_loss: 0.2270 - val_acc: 0.9120\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2108 - acc: 0.9212 - val_loss: 0.2277 - val_acc: 0.9124\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2104 - acc: 0.9210 - val_loss: 0.2277 - val_acc: 0.9096\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2103 - acc: 0.9196 - val_loss: 0.2278 - val_acc: 0.9108\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2108 - acc: 0.9226 - val_loss: 0.2292 - val_acc: 0.9104\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2106 - acc: 0.9212 - val_loss: 0.2278 - val_acc: 0.9108\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2126 - acc: 0.9200 - val_loss: 0.2310 - val_acc: 0.9088\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2096 - acc: 0.9210 - val_loss: 0.2283 - val_acc: 0.9124\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2095 - acc: 0.9234 - val_loss: 0.2309 - val_acc: 0.9080\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2098 - acc: 0.9212 - val_loss: 0.2283 - val_acc: 0.9124\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2095 - acc: 0.9216 - val_loss: 0.2315 - val_acc: 0.9076\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2107 - acc: 0.9218 - val_loss: 0.2306 - val_acc: 0.9088\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2081 - acc: 0.9222 - val_loss: 0.2341 - val_acc: 0.9068\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2096 - acc: 0.9210 - val_loss: 0.2312 - val_acc: 0.9076\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2100 - acc: 0.9220 - val_loss: 0.2352 - val_acc: 0.9072\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2102 - acc: 0.9196 - val_loss: 0.2290 - val_acc: 0.9116\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2092 - acc: 0.9200 - val_loss: 0.2302 - val_acc: 0.9080\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2087 - acc: 0.9210 - val_loss: 0.2293 - val_acc: 0.9100\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2086 - acc: 0.9206 - val_loss: 0.2306 - val_acc: 0.9084\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2086 - acc: 0.9206 - val_loss: 0.2336 - val_acc: 0.9072\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2079 - acc: 0.9216 - val_loss: 0.2301 - val_acc: 0.9104\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2090 - acc: 0.9214 - val_loss: 0.2305 - val_acc: 0.9092\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2085 - acc: 0.9200 - val_loss: 0.2311 - val_acc: 0.9096\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2084 - acc: 0.9212 - val_loss: 0.2341 - val_acc: 0.9064\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2074 - acc: 0.9228 - val_loss: 0.2311 - val_acc: 0.9092\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2088 - acc: 0.9218 - val_loss: 0.2311 - val_acc: 0.9092\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2081 - acc: 0.9194 - val_loss: 0.2315 - val_acc: 0.9092\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2087 - acc: 0.9202 - val_loss: 0.2326 - val_acc: 0.9088\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2085 - acc: 0.9210 - val_loss: 0.2319 - val_acc: 0.9096\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2074 - acc: 0.9210 - val_loss: 0.2373 - val_acc: 0.9076\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2076 - acc: 0.9216 - val_loss: 0.2324 - val_acc: 0.9068\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2087 - acc: 0.9224 - val_loss: 0.2349 - val_acc: 0.9064\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2072 - acc: 0.9240 - val_loss: 0.2326 - val_acc: 0.9072\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2087 - acc: 0.9214 - val_loss: 0.2332 - val_acc: 0.9100\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2077 - acc: 0.9202 - val_loss: 0.2384 - val_acc: 0.9084\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2064 - acc: 0.9222 - val_loss: 0.2347 - val_acc: 0.9076\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2070 - acc: 0.9212 - val_loss: 0.2351 - val_acc: 0.9072\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2072 - acc: 0.9226 - val_loss: 0.2340 - val_acc: 0.9080\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2073 - acc: 0.9238 - val_loss: 0.2340 - val_acc: 0.9072\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2065 - acc: 0.9214 - val_loss: 0.2342 - val_acc: 0.9064\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2067 - acc: 0.9226 - val_loss: 0.2347 - val_acc: 0.9072\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2069 - acc: 0.9230 - val_loss: 0.2372 - val_acc: 0.9068\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2069 - acc: 0.9228 - val_loss: 0.2354 - val_acc: 0.9068\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2070 - acc: 0.9210 - val_loss: 0.2345 - val_acc: 0.9064\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2082 - acc: 0.9230 - val_loss: 0.2352 - val_acc: 0.9060\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2060 - acc: 0.9234 - val_loss: 0.2339 - val_acc: 0.9068\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2071 - acc: 0.9226 - val_loss: 0.2347 - val_acc: 0.9084\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2058 - acc: 0.9208 - val_loss: 0.2342 - val_acc: 0.9052\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2085 - acc: 0.9206 - val_loss: 0.2351 - val_acc: 0.9072\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2079 - acc: 0.9208 - val_loss: 0.2349 - val_acc: 0.9064\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2067 - acc: 0.9210 - val_loss: 0.2413 - val_acc: 0.9052\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2055 - acc: 0.9228 - val_loss: 0.2379 - val_acc: 0.9076\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2059 - acc: 0.9224 - val_loss: 0.2357 - val_acc: 0.9088\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2057 - acc: 0.9212 - val_loss: 0.2430 - val_acc: 0.9052\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2054 - acc: 0.9230 - val_loss: 0.2346 - val_acc: 0.9056\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2066 - acc: 0.9196 - val_loss: 0.2347 - val_acc: 0.9060\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2061 - acc: 0.9224 - val_loss: 0.2348 - val_acc: 0.9072\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2065 - acc: 0.9214 - val_loss: 0.2351 - val_acc: 0.9032\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2062 - acc: 0.9212 - val_loss: 0.2359 - val_acc: 0.9064\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2073 - acc: 0.9212 - val_loss: 0.2379 - val_acc: 0.9072\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2072 - acc: 0.9232 - val_loss: 0.2398 - val_acc: 0.9064\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2069 - acc: 0.9238 - val_loss: 0.2361 - val_acc: 0.9048\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2064 - acc: 0.9220 - val_loss: 0.2358 - val_acc: 0.9056\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2059 - acc: 0.9238 - val_loss: 0.2361 - val_acc: 0.9056\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2073 - acc: 0.9220 - val_loss: 0.2395 - val_acc: 0.9060\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2068 - acc: 0.9216 - val_loss: 0.2393 - val_acc: 0.9056\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2057 - acc: 0.9224 - val_loss: 0.2366 - val_acc: 0.9064\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2068 - acc: 0.9172 - val_loss: 0.2369 - val_acc: 0.9068\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2059 - acc: 0.9210 - val_loss: 0.2391 - val_acc: 0.9060\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2083 - acc: 0.9186 - val_loss: 0.2366 - val_acc: 0.9048\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2071 - acc: 0.9174 - val_loss: 0.2375 - val_acc: 0.9048\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2058 - acc: 0.9222 - val_loss: 0.2429 - val_acc: 0.9044\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2059 - acc: 0.9210 - val_loss: 0.2392 - val_acc: 0.9064\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2066 - acc: 0.9216 - val_loss: 0.2392 - val_acc: 0.9060\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2050 - acc: 0.9216 - val_loss: 0.2372 - val_acc: 0.9040\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2054 - acc: 0.9220 - val_loss: 0.2488 - val_acc: 0.9024\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2058 - acc: 0.9236 - val_loss: 0.2373 - val_acc: 0.9044\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2060 - acc: 0.9212 - val_loss: 0.2408 - val_acc: 0.9052\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 71us/step - loss: 0.2062 - acc: 0.9212 - val_loss: 0.2399 - val_acc: 0.9056\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2054 - acc: 0.9212 - val_loss: 0.2409 - val_acc: 0.9056\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2057 - acc: 0.9212 - val_loss: 0.2399 - val_acc: 0.9056\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2068 - acc: 0.9222 - val_loss: 0.2380 - val_acc: 0.9036\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2046 - acc: 0.9234 - val_loss: 0.2391 - val_acc: 0.9060\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2055 - acc: 0.9218 - val_loss: 0.2386 - val_acc: 0.9028\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2051 - acc: 0.9246 - val_loss: 0.2383 - val_acc: 0.9024\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2074 - acc: 0.9194 - val_loss: 0.2435 - val_acc: 0.9048\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2061 - acc: 0.9234 - val_loss: 0.2395 - val_acc: 0.9056\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2064 - acc: 0.9224 - val_loss: 0.2383 - val_acc: 0.9056\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2048 - acc: 0.9246 - val_loss: 0.2394 - val_acc: 0.9032\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2051 - acc: 0.9218 - val_loss: 0.2387 - val_acc: 0.9040\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2062 - acc: 0.9202 - val_loss: 0.2386 - val_acc: 0.9028\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2055 - acc: 0.9228 - val_loss: 0.2407 - val_acc: 0.9028\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2057 - acc: 0.9226 - val_loss: 0.2472 - val_acc: 0.9044\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2052 - acc: 0.9232 - val_loss: 0.2397 - val_acc: 0.9044\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2047 - acc: 0.9228 - val_loss: 0.2398 - val_acc: 0.9048\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2053 - acc: 0.9204 - val_loss: 0.2395 - val_acc: 0.9040\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2054 - acc: 0.9226 - val_loss: 0.2404 - val_acc: 0.9064\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2047 - acc: 0.9224 - val_loss: 0.2395 - val_acc: 0.9040\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2052 - acc: 0.9212 - val_loss: 0.2406 - val_acc: 0.9052\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2045 - acc: 0.9236 - val_loss: 0.2423 - val_acc: 0.9056\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2043 - acc: 0.9234 - val_loss: 0.2435 - val_acc: 0.9044\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2058 - acc: 0.9210 - val_loss: 0.2401 - val_acc: 0.9036\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 71us/step - loss: 0.2053 - acc: 0.9214 - val_loss: 0.2418 - val_acc: 0.9060\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2042 - acc: 0.9228 - val_loss: 0.2399 - val_acc: 0.9024\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2055 - acc: 0.9230 - val_loss: 0.2448 - val_acc: 0.9056\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2089 - acc: 0.9198 - val_loss: 0.2434 - val_acc: 0.9052\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2041 - acc: 0.9216 - val_loss: 0.2494 - val_acc: 0.9024\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2054 - acc: 0.9210 - val_loss: 0.2443 - val_acc: 0.9048\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2051 - acc: 0.9232 - val_loss: 0.2410 - val_acc: 0.9024\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2056 - acc: 0.9206 - val_loss: 0.2409 - val_acc: 0.9040\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2045 - acc: 0.9214 - val_loss: 0.2412 - val_acc: 0.9040\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2057 - acc: 0.9220 - val_loss: 0.2421 - val_acc: 0.9032\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2046 - acc: 0.9214 - val_loss: 0.2411 - val_acc: 0.9044\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2047 - acc: 0.9238 - val_loss: 0.2416 - val_acc: 0.9036\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2044 - acc: 0.9220 - val_loss: 0.2410 - val_acc: 0.9040\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2046 - acc: 0.9226 - val_loss: 0.2479 - val_acc: 0.9048\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2067 - acc: 0.9228 - val_loss: 0.2442 - val_acc: 0.9056\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2046 - acc: 0.9216 - val_loss: 0.2456 - val_acc: 0.9044\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2046 - acc: 0.9206 - val_loss: 0.2415 - val_acc: 0.9036\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2050 - acc: 0.9196 - val_loss: 0.2495 - val_acc: 0.9032\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2048 - acc: 0.9214 - val_loss: 0.2449 - val_acc: 0.9044\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2046 - acc: 0.9222 - val_loss: 0.2432 - val_acc: 0.9036\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2043 - acc: 0.9226 - val_loss: 0.2513 - val_acc: 0.9020\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2067 - acc: 0.9206 - val_loss: 0.2497 - val_acc: 0.9032\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2047 - acc: 0.9218 - val_loss: 0.2459 - val_acc: 0.9040\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2038 - acc: 0.9230 - val_loss: 0.2431 - val_acc: 0.9036\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2065 - acc: 0.9232 - val_loss: 0.2447 - val_acc: 0.9052\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2058 - acc: 0.9242 - val_loss: 0.2442 - val_acc: 0.9048\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2042 - acc: 0.9224 - val_loss: 0.2417 - val_acc: 0.9040\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2046 - acc: 0.9214 - val_loss: 0.2434 - val_acc: 0.9028\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2084 - acc: 0.9218 - val_loss: 0.2460 - val_acc: 0.9040\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2057 - acc: 0.9232 - val_loss: 0.2425 - val_acc: 0.9048\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2063 - acc: 0.9218 - val_loss: 0.2435 - val_acc: 0.9024\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2045 - acc: 0.9208 - val_loss: 0.2422 - val_acc: 0.9028\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2046 - acc: 0.9228 - val_loss: 0.2422 - val_acc: 0.9044\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2042 - acc: 0.9230 - val_loss: 0.2444 - val_acc: 0.9052\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2061 - acc: 0.9204 - val_loss: 0.2423 - val_acc: 0.9032\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2047 - acc: 0.9220 - val_loss: 0.2505 - val_acc: 0.9024\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2041 - acc: 0.9226 - val_loss: 0.2431 - val_acc: 0.9044\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2039 - acc: 0.9222 - val_loss: 0.2431 - val_acc: 0.9028\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2040 - acc: 0.9236 - val_loss: 0.2469 - val_acc: 0.9048\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2044 - acc: 0.9214 - val_loss: 0.2469 - val_acc: 0.9044\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2054 - acc: 0.9220 - val_loss: 0.2452 - val_acc: 0.9020\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2061 - acc: 0.9216 - val_loss: 0.2460 - val_acc: 0.9048\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2049 - acc: 0.9216 - val_loss: 0.2452 - val_acc: 0.9020\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2038 - acc: 0.9224 - val_loss: 0.2452 - val_acc: 0.9040\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 70us/step - loss: 0.2058 - acc: 0.9216 - val_loss: 0.2526 - val_acc: 0.9020\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2049 - acc: 0.9228 - val_loss: 0.2449 - val_acc: 0.9036\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2050 - acc: 0.9202 - val_loss: 0.2451 - val_acc: 0.9040\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2056 - acc: 0.9228 - val_loss: 0.2446 - val_acc: 0.9044\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2045 - acc: 0.9228 - val_loss: 0.2443 - val_acc: 0.9028\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2046 - acc: 0.9214 - val_loss: 0.2474 - val_acc: 0.9048\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2055 - acc: 0.9202 - val_loss: 0.2446 - val_acc: 0.9024\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2045 - acc: 0.9212 - val_loss: 0.2439 - val_acc: 0.9032\n",
            "Train on 5000 samples, validate on 2500 samples\n",
            "Epoch 1/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2040 - acc: 0.9234 - val_loss: 0.2441 - val_acc: 0.9024\n",
            "Epoch 2/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2055 - acc: 0.9220 - val_loss: 0.2443 - val_acc: 0.9032\n",
            "Epoch 3/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2039 - acc: 0.9214 - val_loss: 0.2448 - val_acc: 0.9032\n",
            "Epoch 4/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2047 - acc: 0.9214 - val_loss: 0.2466 - val_acc: 0.9024\n",
            "Epoch 5/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2045 - acc: 0.9222 - val_loss: 0.2446 - val_acc: 0.9044\n",
            "Epoch 6/25\n",
            "5000/5000 [==============================] - 0s 68us/step - loss: 0.2036 - acc: 0.9234 - val_loss: 0.2484 - val_acc: 0.9016\n",
            "Epoch 7/25\n",
            "5000/5000 [==============================] - 0s 64us/step - loss: 0.2055 - acc: 0.9216 - val_loss: 0.2468 - val_acc: 0.9048\n",
            "Epoch 8/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2046 - acc: 0.9232 - val_loss: 0.2490 - val_acc: 0.9024\n",
            "Epoch 9/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2042 - acc: 0.9228 - val_loss: 0.2449 - val_acc: 0.9036\n",
            "Epoch 10/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2042 - acc: 0.9212 - val_loss: 0.2478 - val_acc: 0.9040\n",
            "Epoch 11/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2048 - acc: 0.9238 - val_loss: 0.2480 - val_acc: 0.9036\n",
            "Epoch 12/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2038 - acc: 0.9240 - val_loss: 0.2494 - val_acc: 0.9032\n",
            "Epoch 13/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2053 - acc: 0.9206 - val_loss: 0.2447 - val_acc: 0.9044\n",
            "Epoch 14/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2044 - acc: 0.9242 - val_loss: 0.2447 - val_acc: 0.9052\n",
            "Epoch 15/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2041 - acc: 0.9228 - val_loss: 0.2444 - val_acc: 0.9048\n",
            "Epoch 16/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2038 - acc: 0.9214 - val_loss: 0.2450 - val_acc: 0.9028\n",
            "Epoch 17/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2037 - acc: 0.9226 - val_loss: 0.2448 - val_acc: 0.9052\n",
            "Epoch 18/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2046 - acc: 0.9214 - val_loss: 0.2471 - val_acc: 0.9036\n",
            "Epoch 19/25\n",
            "5000/5000 [==============================] - 0s 65us/step - loss: 0.2054 - acc: 0.9236 - val_loss: 0.2450 - val_acc: 0.9040\n",
            "Epoch 20/25\n",
            "5000/5000 [==============================] - 0s 69us/step - loss: 0.2041 - acc: 0.9216 - val_loss: 0.2451 - val_acc: 0.9032\n",
            "Epoch 21/25\n",
            "5000/5000 [==============================] - 0s 66us/step - loss: 0.2037 - acc: 0.9230 - val_loss: 0.2454 - val_acc: 0.9024\n",
            "Epoch 22/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2030 - acc: 0.9216 - val_loss: 0.2449 - val_acc: 0.9052\n",
            "Epoch 23/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2035 - acc: 0.9212 - val_loss: 0.2458 - val_acc: 0.9020\n",
            "Epoch 24/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2040 - acc: 0.9200 - val_loss: 0.2460 - val_acc: 0.9032\n",
            "Epoch 25/25\n",
            "5000/5000 [==============================] - 0s 67us/step - loss: 0.2032 - acc: 0.9226 - val_loss: 0.2471 - val_acc: 0.9032\n",
            "Dev-Accuracy:  [0.916, 0.9116, 0.9108, 0.9096, 0.9052, 0.9056, 0.9024, 0.904, 0.9032, 0.9032]\n",
            "Test-Accuracy: [0.9068, 0.904, 0.9016, 0.9016, 0.9028, 0.9032, 0.9016, 0.9028, 0.9008, 0.904]\n",
            "Dev-Accuracy mean: 0.90716\n",
            "Test-Accuracy mean: 0.90292\n",
            "Dev-Accuracy SD: 0.0043283253112491515\n",
            "Test-Accuracy SD: 0.0016497272501841216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rngu3mz03O1P",
        "colab_type": "code",
        "outputId": "7fbb9d2b-c750-4624-970a-493240baa5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0m7As-NC2s64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentiment Classification"
      ]
    },
    {
      "metadata": {
        "id": "1tVwr4EO3RUj",
        "colab_type": "code",
        "outputId": "9ef2b672-c44d-4712-cbd1-9470d1b0cbef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1F67jJJVCXjXUxPzQ2Q9ug1c5lMvu9Yqz   sample_data\t 'view?usp=sharing'\n",
            " conceptor\t\t\t     small_glove.txt\t  w2v_cn.txt\n",
            " glove_cn.txt\t\t\t     small_word2vec.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x_qqi-9MKOO-",
        "colab_type": "code",
        "outputId": "69cd7850-aeb1-4be2-cf85-13217f91d86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd conceptor/Extrinsic-Evaluation-tasks-master/sentiment_classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master/sentiment_classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mHUdYifb7E3K",
        "colab_type": "code",
        "outputId": "40f31498-9231-4851-cc56-9790335140bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8129
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Memory svmem(total=13655322624, available=12675645440, percent=7.2, used=683270144, free=1344483328, active=900517888, inactive=10868092928, buffers=59322368, cached=11568246784, shared=905216, slab=421666816)\n",
            "Enter embedding typeword2vec\n",
            "Conceptor?y\n",
            "tcmalloc: large alloc 3600007168 bytes == 0x5ea2000 @  0x7f3d8f0dc001 0x7f3d8ce10b85 0x7f3d8ce73b43 0x7f3d8ce75a86 0x7f3d8cf0d868 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f3d8ecd7b97 0x5afa0a\n",
            "The embedding has been loaded from gensim!\n",
            "Memory svmem(total=13655322624, available=7991984128, percent=41.5, used=5536538624, free=152301568, active=6191370240, inactive=6901071872, buffers=59506688, cached=7906975744, shared=905216, slab=283123712)\n",
            "300\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "n_vocab 88586\n",
            "Memory svmem(total=13655322624, available=12421451776, percent=9.0, used=4875046912, free=1038204928, active=1758855168, inactive=10450653184, buffers=59518976, cached=7682551808, shared=905216, slab=277151744)\n",
            "conceptoring\n",
            "starting...\n",
            "(300, 88586)\n",
            "R calculated\n",
            "Memory svmem(total=13655322624, available=12420390912, percent=9.0, used=4876095488, free=1036046336, active=1760837632, inactive=10451750912, buffers=59518976, cached=7683661824, shared=905216, slab=277151744)\n",
            "C calculated\n",
            "Memory svmem(total=13655322624, available=12419596288, percent=9.0, used=4876943360, free=1034366976, active=1761886208, inactive=10452635648, buffers=59518976, cached=7684493312, shared=905216, slab=277151744)\n",
            "negC calculated\n",
            "Memory svmem(total=13655322624, available=12419596288, percent=9.0, used=4876943360, free=1034366976, active=1761886208, inactive=10452635648, buffers=59518976, cached=7684493312, shared=905216, slab=277151744)\n",
            "(88586, 300)\n",
            "(88586, 300)\n",
            "conceptored!!\n",
            "(88586, 300)\n",
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Build model...\n",
            "2019-01-30 02:51:43.264567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-01-30 02:51:43.264997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-01-30 02:51:43.265049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-30 02:51:44.087707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-30 02:51:44.087766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-30 02:51:44.087795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-30 02:51:44.090881: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-01-30 02:51:44.091556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2019-01-30 02:51:44.436051: W tensorflow/core/framework/allocator.cc:122] Allocation of 106303200 exceeds 10% of system memory.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 300)         26575800  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 26,736,301\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 26,575,800\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 30s 1ms/step - loss: 0.6679 - acc: 0.5804\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.6341 - acc: 0.6325\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.6074 - acc: 0.6605\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.6004 - acc: 0.6692\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.5748 - acc: 0.6850\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.5512 - acc: 0.7092\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.5296 - acc: 0.7234\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.5026 - acc: 0.7453\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.4804 - acc: 0.7589\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.4612 - acc: 0.7752\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.4412 - acc: 0.7863\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.4214 - acc: 0.8008\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.4010 - acc: 0.8113\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3846 - acc: 0.8242\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3704 - acc: 0.8296\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3590 - acc: 0.8378\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3450 - acc: 0.8462\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3285 - acc: 0.8529\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3182 - acc: 0.8600\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3010 - acc: 0.8688\n",
            "25000/25000 [==============================] - 8s 332us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2921 - acc: 0.8734\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2824 - acc: 0.8776\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2707 - acc: 0.8856\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2585 - acc: 0.8910\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 28s 1ms/step - loss: 0.2517 - acc: 0.8936\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 28s 1ms/step - loss: 0.2435 - acc: 0.8982\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 28s 1ms/step - loss: 0.2287 - acc: 0.9055\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2211 - acc: 0.9080\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2126 - acc: 0.9134\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2053 - acc: 0.9159\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1930 - acc: 0.9220\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1853 - acc: 0.9239\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1780 - acc: 0.9285\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1724 - acc: 0.9303\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1654 - acc: 0.9356\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1560 - acc: 0.9367\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1537 - acc: 0.9394\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1453 - acc: 0.9433\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1352 - acc: 0.9488\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 28s 1ms/step - loss: 0.1352 - acc: 0.9451\n",
            "25000/25000 [==============================] - 8s 325us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 30s 1ms/step - loss: 0.1245 - acc: 0.9523\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1214 - acc: 0.9537\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1210 - acc: 0.9536\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1140 - acc: 0.9560\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1082 - acc: 0.9588\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1067 - acc: 0.9605\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1038 - acc: 0.9604\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 28s 1ms/step - loss: 0.1017 - acc: 0.9615\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1018 - acc: 0.9621\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0989 - acc: 0.9641\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0882 - acc: 0.9669\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0871 - acc: 0.9676\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0855 - acc: 0.9687\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0833 - acc: 0.9694\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0803 - acc: 0.9694\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0792 - acc: 0.9718\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0737 - acc: 0.9716\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0771 - acc: 0.9711\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0705 - acc: 0.9737\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0705 - acc: 0.9740\n",
            "25000/25000 [==============================] - 8s 330us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0729 - acc: 0.9722\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0669 - acc: 0.9756\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0706 - acc: 0.9740\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0645 - acc: 0.9767\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0617 - acc: 0.9781\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0614 - acc: 0.9775\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0618 - acc: 0.9779\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0619 - acc: 0.9776\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0559 - acc: 0.9809\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0533 - acc: 0.9813\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0554 - acc: 0.9803\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0536 - acc: 0.9808\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0521 - acc: 0.9814\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0503 - acc: 0.9813\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0541 - acc: 0.9807\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0512 - acc: 0.9818\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0497 - acc: 0.9826\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0482 - acc: 0.9829\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0520 - acc: 0.9812\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0481 - acc: 0.9839\n",
            "25000/25000 [==============================] - 8s 327us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0492 - acc: 0.9829\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0424 - acc: 0.9851\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0442 - acc: 0.9847\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0415 - acc: 0.9851\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0439 - acc: 0.9842\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0424 - acc: 0.9849\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0411 - acc: 0.9856\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0396 - acc: 0.9864\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0421 - acc: 0.9851\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0422 - acc: 0.9852\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0399 - acc: 0.9860\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0370 - acc: 0.9870\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0374 - acc: 0.9872\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0370 - acc: 0.9861\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0359 - acc: 0.9872\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0414 - acc: 0.9846\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0354 - acc: 0.9873\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0360 - acc: 0.9884\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0352 - acc: 0.9872\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0345 - acc: 0.9881\n",
            "25000/25000 [==============================] - 8s 328us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0332 - acc: 0.9884\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0357 - acc: 0.9876\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0325 - acc: 0.9893\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0331 - acc: 0.9885\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0304 - acc: 0.9897\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0311 - acc: 0.9892\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0305 - acc: 0.9892\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0390 - acc: 0.9858\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0318 - acc: 0.9882\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0292 - acc: 0.9906\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0288 - acc: 0.9907\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0330 - acc: 0.9885\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0324 - acc: 0.9892\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0312 - acc: 0.9903\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0324 - acc: 0.9888\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0299 - acc: 0.9894\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0318 - acc: 0.9893\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0267 - acc: 0.9909\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0275 - acc: 0.9902\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0282 - acc: 0.9902\n",
            "25000/25000 [==============================] - 8s 327us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0262 - acc: 0.9914\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0287 - acc: 0.9899\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0269 - acc: 0.9910\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0271 - acc: 0.9901\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0295 - acc: 0.9892\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0274 - acc: 0.9899\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0268 - acc: 0.9905\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0265 - acc: 0.9904\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0244 - acc: 0.9918\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0264 - acc: 0.9909\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0238 - acc: 0.9922\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0246 - acc: 0.9910\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0258 - acc: 0.9911\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0253 - acc: 0.9915\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0239 - acc: 0.9915\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0254 - acc: 0.9913\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0254 - acc: 0.9909\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0213 - acc: 0.9926\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0259 - acc: 0.9910\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 33s 1ms/step - loss: 0.0233 - acc: 0.9917\n",
            "25000/25000 [==============================] - 8s 328us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0239 - acc: 0.9914\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0234 - acc: 0.9919\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0245 - acc: 0.9912\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0239 - acc: 0.9918\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0222 - acc: 0.9924\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0241 - acc: 0.9915\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0235 - acc: 0.9926\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0223 - acc: 0.9924\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0208 - acc: 0.9932\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0243 - acc: 0.9914\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0227 - acc: 0.9920\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0219 - acc: 0.9931\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0226 - acc: 0.9925\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0232 - acc: 0.9925\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0217 - acc: 0.9925\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0213 - acc: 0.9929\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0236 - acc: 0.9918\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0223 - acc: 0.9922\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0226 - acc: 0.9923\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0212 - acc: 0.9929\n",
            "25000/25000 [==============================] - 8s 327us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0237 - acc: 0.9920\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0208 - acc: 0.9926\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0225 - acc: 0.9924\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0171 - acc: 0.9940\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0209 - acc: 0.9929\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0211 - acc: 0.9920\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0227 - acc: 0.9920\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0205 - acc: 0.9932\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0213 - acc: 0.9932\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0196 - acc: 0.9934\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0183 - acc: 0.9938\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0189 - acc: 0.9934\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0185 - acc: 0.9938\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0203 - acc: 0.9933\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0191 - acc: 0.9929\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0206 - acc: 0.9930\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0209 - acc: 0.9928\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0195 - acc: 0.9932\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0186 - acc: 0.9944\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0185 - acc: 0.9941\n",
            "25000/25000 [==============================] - 8s 326us/step\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0179 - acc: 0.9937\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0201 - acc: 0.9932\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0200 - acc: 0.9930\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0190 - acc: 0.9936\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0195 - acc: 0.9936\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0182 - acc: 0.9935\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0202 - acc: 0.9929\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0188 - acc: 0.9932\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0201 - acc: 0.9925\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0195 - acc: 0.9934\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0190 - acc: 0.9933\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0176 - acc: 0.9938\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0168 - acc: 0.9949\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0173 - acc: 0.9941\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0156 - acc: 0.9943\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0166 - acc: 0.9943\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0187 - acc: 0.9935\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0164 - acc: 0.9938\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0160 - acc: 0.9944\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 29s 1ms/step - loss: 0.0190 - acc: 0.9931\n",
            "25000/25000 [==============================] - 8s 331us/step\n",
            "Test score: [0.4032600853919983, 0.4948718743610382, 0.6464655741977692, 0.6807749705410003, 0.7514424202919007, 0.7754366998291016, 0.8285912659358978, 0.8568158986377716, 0.8895747180843353, 0.9141797792243957]\n",
            "Test accuracy: [0.81392, 0.8220800000190734, 0.816919999961853, 0.8189600000190735, 0.824560000038147, 0.82244, 0.82616, 0.823759999961853, 0.8226400000190734, 0.82472]\n",
            "Score mean:  0.7241413286495209\n",
            "Accuracy mean:  0.8216160000019073\n",
            "Score SD:  0.1608773239012507\n",
            "Accuracy SD:  0.00365820502944758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hoPvautz3YAE",
        "colab_type": "code",
        "outputId": "68a8c1aa-0ba6-4ad9-8e10-2e9d293e25b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2d0_msUG26zs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SNLI"
      ]
    },
    {
      "metadata": {
        "id": "yTgvluYf3bW4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd conceptor/Extrinsic-Evaluation-tasks-master/snli\n",
        "#Download the SNLI data\n",
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip\n",
        "!mv snli_1.0/* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "arYwqT9H29-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2boCxxCF3drB",
        "colab_type": "code",
        "outputId": "b868e833-3e8f-4565-d716-1b983245461e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ETdZzdNL3BAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Subjectivity Classification"
      ]
    },
    {
      "metadata": {
        "id": "la-mzz4W3EOL",
        "colab_type": "code",
        "outputId": "5154d3b8-1f96-43c9-a09a-61dd1280422f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd conceptor/Extrinsic-Evaluation-tasks-master/subjectivity_classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/conceptor/Extrinsic-Evaluation-tasks-master/subjectivity_classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3SicNLtl3jY5",
        "colab_type": "code",
        "outputId": "8e724e37-cb1c-4478-f0c6-b119d010836d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9931
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 preprocess.py\n",
        "!python3 cnn.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter embeddings file name: j\n",
            "data/train.txt 5330 sentences\n",
            "data/dev.txt 2664 sentences\n",
            "data/test.txt 2668 sentences\n",
            "Enter embedding typeword2vec\n",
            "tcmalloc: large alloc 3600007168 bytes == 0x4436000 @  0x7f7e3d5bb001 0x7f7e3b2efb85 0x7f7e3b352b43 0x7f7e3b354a86 0x7f7e3b3ec868 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7f7e3d1b6b97 0x5afa0a\n",
            "The embedding has been loaded from gensim!\n",
            "Load pre-trained embeddings file\n",
            "Embeddings shape:  (15881, 300)\n",
            "Len words:  21347\n",
            "Conceptor? n\n",
            "Unknown tokens: 26.72%\n",
            "Unknown tokens: 26.46%\n",
            "Unknown tokens: 26.43%\n",
            "Data stored in pkl folder\n",
            "Using TensorFlow backend.\n",
            "data loaded!\n",
            "Longest sentence: 59\n",
            "X_train shape: (5330, 59)\n",
            "X_dev shape: (2664, 59)\n",
            "X_test shape: (2668, 59)\n",
            "Build model...\n",
            "2019-02-01 02:38:46.070770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-02-01 02:38:46.071246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-02-01 02:38:46.071285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-02-01 02:38:46.425947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-02-01 02:38:46.426021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-02-01 02:38:46.426048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-02-01 02:38:46.426438: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-02-01 02:38:46.426522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "words_input (InputLayer)        (None, 59)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 59, 300)      4764300     words_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 59, 50)       15050       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 59, 50)       30050       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 59, 50)       45050       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 50)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 50)           0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 50)           0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 150)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          15100       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,869,651\n",
            "Trainable params: 105,351\n",
            "Non-trainable params: 4,764,300\n",
            "__________________________________________________________________________________________________\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 3s 489us/step - loss: 1.4443 - acc: 0.6647 - val_loss: 1.0201 - val_acc: 0.7643\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.8056 - acc: 0.7983 - val_loss: 0.7157 - val_acc: 0.7740\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.5551 - acc: 0.8484 - val_loss: 0.5871 - val_acc: 0.7973\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.4166 - acc: 0.8938 - val_loss: 0.5598 - val_acc: 0.7917\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.3255 - acc: 0.9351 - val_loss: 0.5629 - val_acc: 0.7965\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.2590 - acc: 0.9711 - val_loss: 0.5846 - val_acc: 0.7905\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.2152 - acc: 0.9899 - val_loss: 0.6320 - val_acc: 0.7815\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1894 - acc: 0.9964 - val_loss: 0.6254 - val_acc: 0.7879\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1752 - acc: 0.9987 - val_loss: 0.6196 - val_acc: 0.7864\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.1647 - acc: 0.9992 - val_loss: 0.6201 - val_acc: 0.7883\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1570 - acc: 0.9996 - val_loss: 0.6062 - val_acc: 0.7890\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1521 - acc: 0.9996 - val_loss: 0.6048 - val_acc: 0.7894\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1462 - acc: 0.9998 - val_loss: 0.6091 - val_acc: 0.7883\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.1422 - acc: 0.9998 - val_loss: 0.6014 - val_acc: 0.7894\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.1391 - acc: 0.9998 - val_loss: 0.6013 - val_acc: 0.7887\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.1344 - acc: 0.9998 - val_loss: 0.6012 - val_acc: 0.7902\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.1307 - acc: 0.9998 - val_loss: 0.5996 - val_acc: 0.7909\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 153us/step - loss: 0.1276 - acc: 0.9998 - val_loss: 0.6064 - val_acc: 0.7917\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 153us/step - loss: 0.1253 - acc: 0.9998 - val_loss: 0.6051 - val_acc: 0.7845\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1231 - acc: 0.9998 - val_loss: 0.5987 - val_acc: 0.7872\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1195 - acc: 0.9998 - val_loss: 0.5945 - val_acc: 0.7875\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.1180 - acc: 0.9998 - val_loss: 0.6089 - val_acc: 0.7909\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1153 - acc: 0.9998 - val_loss: 0.5884 - val_acc: 0.7853\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1133 - acc: 0.9998 - val_loss: 0.6058 - val_acc: 0.7849\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1112 - acc: 0.9998 - val_loss: 0.5930 - val_acc: 0.7887\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1087 - acc: 0.9998 - val_loss: 0.5889 - val_acc: 0.7857\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1069 - acc: 0.9998 - val_loss: 0.5914 - val_acc: 0.7872\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.1047 - acc: 0.9998 - val_loss: 0.5946 - val_acc: 0.7909\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.1032 - acc: 0.9998 - val_loss: 0.6142 - val_acc: 0.7834\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1013 - acc: 0.9998 - val_loss: 0.5906 - val_acc: 0.7868\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.1008 - acc: 0.9998 - val_loss: 0.5937 - val_acc: 0.7872\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0988 - acc: 0.9998 - val_loss: 0.5923 - val_acc: 0.7857\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0965 - acc: 0.9998 - val_loss: 0.5862 - val_acc: 0.7872\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0951 - acc: 0.9998 - val_loss: 0.5856 - val_acc: 0.7875\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0954 - acc: 0.9998 - val_loss: 0.5902 - val_acc: 0.7849\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0928 - acc: 0.9998 - val_loss: 0.5897 - val_acc: 0.7890\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0919 - acc: 0.9998 - val_loss: 0.6227 - val_acc: 0.7823\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0901 - acc: 0.9998 - val_loss: 0.5862 - val_acc: 0.7834\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0891 - acc: 0.9998 - val_loss: 0.5893 - val_acc: 0.7883\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0875 - acc: 0.9998 - val_loss: 0.5879 - val_acc: 0.7872\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0868 - acc: 0.9998 - val_loss: 0.5823 - val_acc: 0.7842\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0853 - acc: 0.9998 - val_loss: 0.6094 - val_acc: 0.7842\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0839 - acc: 0.9998 - val_loss: 0.5871 - val_acc: 0.7857\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0824 - acc: 0.9998 - val_loss: 0.5876 - val_acc: 0.7860\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0817 - acc: 0.9998 - val_loss: 0.5899 - val_acc: 0.7864\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0806 - acc: 0.9998 - val_loss: 0.5956 - val_acc: 0.7857\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0798 - acc: 0.9998 - val_loss: 0.5898 - val_acc: 0.7838\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0791 - acc: 0.9998 - val_loss: 0.5913 - val_acc: 0.7875\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0779 - acc: 0.9998 - val_loss: 0.6008 - val_acc: 0.7902\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0778 - acc: 0.9998 - val_loss: 0.5963 - val_acc: 0.7905\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0769 - acc: 0.9998 - val_loss: 0.5893 - val_acc: 0.7883\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0751 - acc: 0.9998 - val_loss: 0.5878 - val_acc: 0.7868\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0742 - acc: 0.9998 - val_loss: 0.6004 - val_acc: 0.7950\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0734 - acc: 0.9998 - val_loss: 0.5928 - val_acc: 0.7872\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0721 - acc: 0.9998 - val_loss: 0.5968 - val_acc: 0.7924\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0716 - acc: 0.9998 - val_loss: 0.5828 - val_acc: 0.7887\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0709 - acc: 0.9998 - val_loss: 0.5881 - val_acc: 0.7902\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0703 - acc: 0.9998 - val_loss: 0.5901 - val_acc: 0.7879\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0696 - acc: 0.9998 - val_loss: 0.5968 - val_acc: 0.7902\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0696 - acc: 0.9998 - val_loss: 0.5969 - val_acc: 0.7857\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 144us/step - loss: 0.0678 - acc: 0.9998 - val_loss: 0.5871 - val_acc: 0.7875\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0678 - acc: 0.9998 - val_loss: 0.5889 - val_acc: 0.7864\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0663 - acc: 0.9998 - val_loss: 0.5872 - val_acc: 0.7883\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0652 - acc: 0.9998 - val_loss: 0.5915 - val_acc: 0.7875\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0647 - acc: 0.9998 - val_loss: 0.5967 - val_acc: 0.7879\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0647 - acc: 0.9998 - val_loss: 0.5909 - val_acc: 0.7887\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0632 - acc: 0.9998 - val_loss: 0.5847 - val_acc: 0.7890\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0628 - acc: 0.9998 - val_loss: 0.5862 - val_acc: 0.7917\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0618 - acc: 0.9998 - val_loss: 0.5930 - val_acc: 0.7883\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0623 - acc: 0.9998 - val_loss: 0.5993 - val_acc: 0.7898\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0609 - acc: 0.9998 - val_loss: 0.5946 - val_acc: 0.7872\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0600 - acc: 0.9998 - val_loss: 0.5867 - val_acc: 0.7879\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0606 - acc: 0.9998 - val_loss: 0.5932 - val_acc: 0.7864\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0594 - acc: 0.9998 - val_loss: 0.5924 - val_acc: 0.7879\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0585 - acc: 0.9998 - val_loss: 0.5950 - val_acc: 0.7890\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0579 - acc: 0.9998 - val_loss: 0.5890 - val_acc: 0.7864\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0574 - acc: 0.9998 - val_loss: 0.5998 - val_acc: 0.7849\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0572 - acc: 0.9998 - val_loss: 0.5930 - val_acc: 0.7887\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0561 - acc: 0.9998 - val_loss: 0.6001 - val_acc: 0.7909\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0564 - acc: 0.9998 - val_loss: 0.6037 - val_acc: 0.7872\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0560 - acc: 0.9998 - val_loss: 0.5918 - val_acc: 0.7913\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0548 - acc: 0.9998 - val_loss: 0.5944 - val_acc: 0.7838\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0547 - acc: 0.9998 - val_loss: 0.6073 - val_acc: 0.7845\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0536 - acc: 0.9998 - val_loss: 0.5909 - val_acc: 0.7838\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0532 - acc: 0.9998 - val_loss: 0.6235 - val_acc: 0.7845\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0527 - acc: 0.9998 - val_loss: 0.5962 - val_acc: 0.7875\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0523 - acc: 0.9998 - val_loss: 0.6041 - val_acc: 0.7909\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0519 - acc: 0.9998 - val_loss: 0.6089 - val_acc: 0.7849\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0515 - acc: 0.9998 - val_loss: 0.5930 - val_acc: 0.7860\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0509 - acc: 0.9998 - val_loss: 0.6067 - val_acc: 0.7823\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0502 - acc: 0.9998 - val_loss: 0.6057 - val_acc: 0.7834\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0501 - acc: 0.9998 - val_loss: 0.5900 - val_acc: 0.7879\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0504 - acc: 0.9998 - val_loss: 0.6277 - val_acc: 0.7924\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0490 - acc: 0.9998 - val_loss: 0.5991 - val_acc: 0.7838\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0491 - acc: 0.9998 - val_loss: 0.6327 - val_acc: 0.7894\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0483 - acc: 0.9998 - val_loss: 0.6127 - val_acc: 0.7913\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0484 - acc: 0.9998 - val_loss: 0.5939 - val_acc: 0.7913\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0474 - acc: 0.9998 - val_loss: 0.6090 - val_acc: 0.7913\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0469 - acc: 0.9998 - val_loss: 0.6203 - val_acc: 0.7849\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0466 - acc: 0.9998 - val_loss: 0.5990 - val_acc: 0.7917\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0461 - acc: 0.9998 - val_loss: 0.6176 - val_acc: 0.7838\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0463 - acc: 0.9998 - val_loss: 0.6050 - val_acc: 0.7909\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 143us/step - loss: 0.0456 - acc: 0.9998 - val_loss: 0.5985 - val_acc: 0.7928\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0450 - acc: 0.9998 - val_loss: 0.6196 - val_acc: 0.7838\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0449 - acc: 0.9998 - val_loss: 0.6302 - val_acc: 0.7849\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0449 - acc: 0.9998 - val_loss: 0.6056 - val_acc: 0.7872\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0447 - acc: 0.9998 - val_loss: 0.6095 - val_acc: 0.7898\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0439 - acc: 0.9998 - val_loss: 0.6038 - val_acc: 0.7853\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0443 - acc: 0.9998 - val_loss: 0.6035 - val_acc: 0.7872\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0430 - acc: 0.9998 - val_loss: 0.6049 - val_acc: 0.7913\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0431 - acc: 0.9998 - val_loss: 0.6099 - val_acc: 0.7883\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0423 - acc: 0.9998 - val_loss: 0.5950 - val_acc: 0.7898\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0423 - acc: 0.9998 - val_loss: 0.6010 - val_acc: 0.7860\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0422 - acc: 0.9998 - val_loss: 0.6166 - val_acc: 0.7928\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0414 - acc: 0.9998 - val_loss: 0.6128 - val_acc: 0.7857\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0410 - acc: 0.9998 - val_loss: 0.6363 - val_acc: 0.7823\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0409 - acc: 0.9998 - val_loss: 0.6073 - val_acc: 0.7909\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0404 - acc: 0.9998 - val_loss: 0.6038 - val_acc: 0.7898\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0403 - acc: 0.9998 - val_loss: 0.5996 - val_acc: 0.7924\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0401 - acc: 0.9998 - val_loss: 0.6260 - val_acc: 0.7838\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0398 - acc: 0.9998 - val_loss: 0.6123 - val_acc: 0.7898\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0392 - acc: 0.9998 - val_loss: 0.6118 - val_acc: 0.7849\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0391 - acc: 0.9998 - val_loss: 0.6110 - val_acc: 0.7932\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0385 - acc: 0.9998 - val_loss: 0.6216 - val_acc: 0.7823\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0391 - acc: 0.9998 - val_loss: 0.6110 - val_acc: 0.7894\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 154us/step - loss: 0.0382 - acc: 0.9998 - val_loss: 0.6115 - val_acc: 0.7890\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0379 - acc: 0.9998 - val_loss: 0.6272 - val_acc: 0.7834\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0383 - acc: 0.9998 - val_loss: 0.6226 - val_acc: 0.7917\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 153us/step - loss: 0.0390 - acc: 0.9998 - val_loss: 0.6289 - val_acc: 0.7879\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0373 - acc: 0.9998 - val_loss: 0.6125 - val_acc: 0.7898\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0371 - acc: 0.9998 - val_loss: 0.6111 - val_acc: 0.7905\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0368 - acc: 0.9998 - val_loss: 0.6059 - val_acc: 0.7894\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0366 - acc: 0.9998 - val_loss: 0.6185 - val_acc: 0.7883\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0362 - acc: 0.9998 - val_loss: 0.6241 - val_acc: 0.7928\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0360 - acc: 0.9998 - val_loss: 0.6138 - val_acc: 0.7890\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0357 - acc: 0.9998 - val_loss: 0.6141 - val_acc: 0.7913\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0354 - acc: 0.9998 - val_loss: 0.6174 - val_acc: 0.7913\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0354 - acc: 0.9998 - val_loss: 0.6155 - val_acc: 0.7883\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0349 - acc: 0.9998 - val_loss: 0.6080 - val_acc: 0.7890\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0351 - acc: 0.9998 - val_loss: 0.6177 - val_acc: 0.7913\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0346 - acc: 0.9998 - val_loss: 0.6210 - val_acc: 0.7883\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0351 - acc: 0.9998 - val_loss: 0.6661 - val_acc: 0.7920\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0348 - acc: 0.9998 - val_loss: 0.6117 - val_acc: 0.7890\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0338 - acc: 0.9998 - val_loss: 0.6129 - val_acc: 0.7864\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 144us/step - loss: 0.0337 - acc: 0.9998 - val_loss: 0.6182 - val_acc: 0.7864\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0337 - acc: 0.9998 - val_loss: 0.6119 - val_acc: 0.7890\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0335 - acc: 0.9998 - val_loss: 0.6208 - val_acc: 0.7838\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0331 - acc: 0.9998 - val_loss: 0.6159 - val_acc: 0.7913\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0333 - acc: 0.9998 - val_loss: 0.6135 - val_acc: 0.7857\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0329 - acc: 0.9998 - val_loss: 0.6423 - val_acc: 0.7834\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0329 - acc: 0.9998 - val_loss: 0.6252 - val_acc: 0.7894\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0326 - acc: 0.9998 - val_loss: 0.6661 - val_acc: 0.7920\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0324 - acc: 0.9998 - val_loss: 0.6144 - val_acc: 0.7890\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0321 - acc: 0.9998 - val_loss: 0.6195 - val_acc: 0.7857\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0317 - acc: 0.9998 - val_loss: 0.6179 - val_acc: 0.7924\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0316 - acc: 0.9998 - val_loss: 0.6163 - val_acc: 0.7913\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0315 - acc: 0.9998 - val_loss: 0.6491 - val_acc: 0.7838\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0316 - acc: 0.9998 - val_loss: 0.6244 - val_acc: 0.7872\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0310 - acc: 0.9998 - val_loss: 0.6273 - val_acc: 0.7935\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0309 - acc: 0.9998 - val_loss: 0.6588 - val_acc: 0.7823\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0310 - acc: 0.9998 - val_loss: 0.6333 - val_acc: 0.7853\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0314 - acc: 0.9998 - val_loss: 0.6186 - val_acc: 0.7868\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0308 - acc: 0.9998 - val_loss: 0.6330 - val_acc: 0.7947\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0315 - acc: 0.9998 - val_loss: 0.6718 - val_acc: 0.7823\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 153us/step - loss: 0.0304 - acc: 0.9998 - val_loss: 0.6273 - val_acc: 0.7917\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0299 - acc: 0.9998 - val_loss: 0.6275 - val_acc: 0.7890\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0298 - acc: 0.9998 - val_loss: 0.6373 - val_acc: 0.7902\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0297 - acc: 0.9998 - val_loss: 0.6243 - val_acc: 0.7917\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0295 - acc: 0.9998 - val_loss: 0.6205 - val_acc: 0.7920\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0298 - acc: 0.9998 - val_loss: 0.6347 - val_acc: 0.7857\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0296 - acc: 0.9998 - val_loss: 0.6225 - val_acc: 0.7932\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0290 - acc: 0.9998 - val_loss: 0.6264 - val_acc: 0.7913\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0293 - acc: 0.9998 - val_loss: 0.6323 - val_acc: 0.7883\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0289 - acc: 0.9998 - val_loss: 0.6427 - val_acc: 0.7853\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0286 - acc: 0.9998 - val_loss: 0.6221 - val_acc: 0.7883\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0285 - acc: 0.9998 - val_loss: 0.6210 - val_acc: 0.7879\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0284 - acc: 0.9998 - val_loss: 0.6729 - val_acc: 0.7823\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0282 - acc: 0.9998 - val_loss: 0.6249 - val_acc: 0.7890\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0279 - acc: 0.9998 - val_loss: 0.6271 - val_acc: 0.7909\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0281 - acc: 0.9998 - val_loss: 0.6260 - val_acc: 0.7883\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0282 - acc: 0.9998 - val_loss: 0.6435 - val_acc: 0.7890\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0280 - acc: 0.9998 - val_loss: 0.6255 - val_acc: 0.7879\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0276 - acc: 0.9998 - val_loss: 0.6348 - val_acc: 0.7920\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0274 - acc: 0.9998 - val_loss: 0.6311 - val_acc: 0.7868\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0272 - acc: 0.9998 - val_loss: 0.6214 - val_acc: 0.7935\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0271 - acc: 0.9998 - val_loss: 0.6294 - val_acc: 0.7883\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0271 - acc: 0.9998 - val_loss: 0.6242 - val_acc: 0.7905\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 145us/step - loss: 0.0268 - acc: 0.9998 - val_loss: 0.6272 - val_acc: 0.7917\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0267 - acc: 0.9998 - val_loss: 0.6342 - val_acc: 0.7868\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0265 - acc: 0.9998 - val_loss: 0.6371 - val_acc: 0.7913\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0266 - acc: 0.9998 - val_loss: 0.6319 - val_acc: 0.7868\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0268 - acc: 0.9998 - val_loss: 0.6376 - val_acc: 0.7913\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0262 - acc: 0.9998 - val_loss: 0.6272 - val_acc: 0.7924\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0262 - acc: 0.9998 - val_loss: 0.6277 - val_acc: 0.7883\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0260 - acc: 0.9998 - val_loss: 0.6803 - val_acc: 0.7823\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0263 - acc: 0.9998 - val_loss: 0.6336 - val_acc: 0.7943\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0258 - acc: 0.9998 - val_loss: 0.6461 - val_acc: 0.7872\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0259 - acc: 0.9998 - val_loss: 0.7143 - val_acc: 0.7894\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0262 - acc: 0.9998 - val_loss: 0.6471 - val_acc: 0.7868\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0256 - acc: 0.9998 - val_loss: 0.6506 - val_acc: 0.7920\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 153us/step - loss: 0.0254 - acc: 0.9998 - val_loss: 0.6244 - val_acc: 0.7917\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0257 - acc: 0.9998 - val_loss: 0.6268 - val_acc: 0.7913\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0256 - acc: 0.9998 - val_loss: 0.6815 - val_acc: 0.7905\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0257 - acc: 0.9998 - val_loss: 0.6468 - val_acc: 0.7860\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0251 - acc: 0.9998 - val_loss: 0.6319 - val_acc: 0.7890\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0248 - acc: 0.9998 - val_loss: 0.6337 - val_acc: 0.7857\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0247 - acc: 0.9998 - val_loss: 0.6483 - val_acc: 0.7898\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0247 - acc: 0.9998 - val_loss: 0.6410 - val_acc: 0.7924\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0250 - acc: 0.9998 - val_loss: 0.6456 - val_acc: 0.7894\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0248 - acc: 0.9998 - val_loss: 0.6321 - val_acc: 0.7902\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0244 - acc: 0.9998 - val_loss: 0.6435 - val_acc: 0.7905\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0243 - acc: 0.9998 - val_loss: 0.6346 - val_acc: 0.7875\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0242 - acc: 0.9998 - val_loss: 0.6425 - val_acc: 0.7928\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0241 - acc: 0.9998 - val_loss: 0.6303 - val_acc: 0.7898\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0239 - acc: 0.9998 - val_loss: 0.6456 - val_acc: 0.7883\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0239 - acc: 0.9998 - val_loss: 0.6470 - val_acc: 0.7932\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0240 - acc: 0.9998 - val_loss: 0.6871 - val_acc: 0.7902\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0237 - acc: 0.9998 - val_loss: 0.7090 - val_acc: 0.7804\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0238 - acc: 0.9998 - val_loss: 0.6417 - val_acc: 0.7857\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0236 - acc: 0.9998 - val_loss: 0.6919 - val_acc: 0.7800\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0236 - acc: 0.9998 - val_loss: 0.6501 - val_acc: 0.7905\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0234 - acc: 0.9998 - val_loss: 0.6532 - val_acc: 0.7932\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0234 - acc: 0.9998 - val_loss: 0.6374 - val_acc: 0.7924\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0233 - acc: 0.9998 - val_loss: 0.6365 - val_acc: 0.7868\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0231 - acc: 0.9998 - val_loss: 0.6321 - val_acc: 0.7894\n",
            "Train on 5330 samples, validate on 2664 samples\n",
            "Epoch 1/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0232 - acc: 0.9998 - val_loss: 0.6922 - val_acc: 0.7812\n",
            "Epoch 2/25\n",
            "5330/5330 [==============================] - 1s 143us/step - loss: 0.0228 - acc: 0.9998 - val_loss: 0.6472 - val_acc: 0.7875\n",
            "Epoch 3/25\n",
            "5330/5330 [==============================] - 1s 147us/step - loss: 0.0233 - acc: 0.9998 - val_loss: 0.6702 - val_acc: 0.7905\n",
            "Epoch 4/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0229 - acc: 0.9998 - val_loss: 0.6540 - val_acc: 0.7883\n",
            "Epoch 5/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0229 - acc: 0.9998 - val_loss: 0.6465 - val_acc: 0.7902\n",
            "Epoch 6/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0226 - acc: 0.9998 - val_loss: 0.6481 - val_acc: 0.7920\n",
            "Epoch 7/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0228 - acc: 0.9998 - val_loss: 0.6452 - val_acc: 0.7917\n",
            "Epoch 8/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0225 - acc: 0.9998 - val_loss: 0.6637 - val_acc: 0.7845\n",
            "Epoch 9/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0223 - acc: 0.9998 - val_loss: 0.6595 - val_acc: 0.7857\n",
            "Epoch 10/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0224 - acc: 0.9998 - val_loss: 0.6471 - val_acc: 0.7879\n",
            "Epoch 11/25\n",
            "5330/5330 [==============================] - 1s 146us/step - loss: 0.0221 - acc: 0.9998 - val_loss: 0.6488 - val_acc: 0.7905\n",
            "Epoch 12/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0224 - acc: 0.9998 - val_loss: 0.6610 - val_acc: 0.7853\n",
            "Epoch 13/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0223 - acc: 0.9998 - val_loss: 0.6322 - val_acc: 0.7864\n",
            "Epoch 14/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0226 - acc: 0.9998 - val_loss: 0.6794 - val_acc: 0.7902\n",
            "Epoch 15/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0218 - acc: 0.9998 - val_loss: 0.6450 - val_acc: 0.7894\n",
            "Epoch 16/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0218 - acc: 0.9998 - val_loss: 0.6551 - val_acc: 0.7875\n",
            "Epoch 17/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0217 - acc: 0.9998 - val_loss: 0.6551 - val_acc: 0.7898\n",
            "Epoch 18/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0219 - acc: 0.9998 - val_loss: 0.6578 - val_acc: 0.7845\n",
            "Epoch 19/25\n",
            "5330/5330 [==============================] - 1s 148us/step - loss: 0.0215 - acc: 0.9998 - val_loss: 0.6451 - val_acc: 0.7860\n",
            "Epoch 20/25\n",
            "5330/5330 [==============================] - 1s 152us/step - loss: 0.0214 - acc: 0.9998 - val_loss: 0.6467 - val_acc: 0.7917\n",
            "Epoch 21/25\n",
            "5330/5330 [==============================] - 1s 151us/step - loss: 0.0217 - acc: 0.9998 - val_loss: 0.6958 - val_acc: 0.7853\n",
            "Epoch 22/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0216 - acc: 0.9998 - val_loss: 0.6573 - val_acc: 0.7868\n",
            "Epoch 23/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0212 - acc: 0.9998 - val_loss: 0.6553 - val_acc: 0.7932\n",
            "Epoch 24/25\n",
            "5330/5330 [==============================] - 1s 149us/step - loss: 0.0217 - acc: 0.9998 - val_loss: 0.6660 - val_acc: 0.7905\n",
            "Epoch 25/25\n",
            "5330/5330 [==============================] - 1s 150us/step - loss: 0.0212 - acc: 0.9998 - val_loss: 0.6851 - val_acc: 0.7913\n",
            "Dev-Accuracy:  [0.7886636636636637, 0.7905405405405406, 0.789039039039039, 0.7916666666666666, 0.7894144144144144, 0.7834084084084084, 0.7882882882882883, 0.7920420420420421, 0.7894144144144144, 0.7912912912912913]\n",
            "Test-Accuracy: [0.7822338830584707, 0.7833583208395802, 0.7833583207502179, 0.7852323837187337, 0.788605697062062, 0.7781109444383738, 0.7882308844683589, 0.7856071963124368, 0.7871064466872494, 0.7814842578710645]\n",
            "Dev-Accuracy mean: 0.789376876876877\n",
            "Test-Accuracy mean: 0.7843328335206549\n",
            "Dev-Accuracy SD: 0.002340910184334661\n",
            "Test-Accuracy SD: 0.003109815352201815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x3cWYG1B3oa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xlgp8p9W3rwH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "## Small Dataset\n",
        "\n",
        "1.   Relation extraction\n",
        "\n",
        ">*   **Glove**\n",
        ">> Accuracy = 0.7210 \\\\\n",
        ">> F1 = 0.6978\n",
        "*   **Glove + CN**\n",
        ">> Accuracy = 0.7379 \\\\\n",
        ">> F1 = 0.7010\n",
        "*   **Word2vec**\n",
        ">> Accuracy = 0.7530 \\\\\n",
        ">> F1 = 0.7213\n",
        "*   **Word2vec + CN**\n",
        ">> Accuracy = 0.7483 \\\\\n",
        ">> F1 = 0.7162\n",
        "\n",
        "2.   Sentence Polarity Classification\n",
        "\n",
        ">*   **Glove**\n",
        ">> Dev-Accuracy = 0.9108 \\\\\n",
        ">> Test-Accuracy = 0.8912\n",
        "*   **Glove + CN**\n",
        ">> Dev-Accuracy = 0.9060  \\\\\n",
        ">> Test-Accuracy = 0.8936\n",
        "*   **Word2vec**\n",
        ">> Dev-Accuracy = 0.9004 \\\\\n",
        ">> Test-Accuracy = 0.8980\n",
        "*   **Word2vec + CN**\n",
        ">> Dev-Accuracy = 0.9 \\\\\n",
        ">> Test-Accuracy = 0.9068\n",
        "\n",
        "3.   Sentiment Classification\n",
        "\n",
        ">*   **Glove**\n",
        ">> Test-Accuracy = 0.8096\n",
        "*   **Glove + CN** \n",
        ">> Test-Accuracy = 0.8051\n",
        "*   **Word2vec**\n",
        ">> Test-Accurracy = 0.8111\n",
        "*   **Word2vec + CN**\n",
        ">> Test-Accurracy = 0.8130\n",
        "\n",
        "4.   SNLI\n",
        "\n",
        ">*   **Glove**\n",
        ">> Test-Loss = 0.7576 \\\\\n",
        ">> Test-Accuracy = 0.6868\n",
        "*   **Glove + CN**\n",
        ">> Test-Loss = 0.7643 \\\\\n",
        ">> Test-Accuracy = 0.6822\n",
        "*   **Word2vec**\n",
        ">> Test-Loss = 0.7732 \\\\\n",
        ">> Test-Accuracy = 0.6818\n",
        "*   **Word2vec + CN**\n",
        ">> Test-Loss = 0.7576 \\\\\n",
        ">> Test-Accuracy = 0.6868\n",
        "\n",
        "5.   Subjectivity Classification\n",
        "\n",
        ">*   **Glove**\n",
        ">> Dev-Accuracy = 0.7913 \\\\\n",
        ">> Test-Accuracy = 0.7770\n",
        "*   **Glove + CN**\n",
        ">> Dev-Accuracy = 0.7932  \\\\\n",
        ">> Test-Accuracy = 0.7837\n",
        "*   **Word2vec**\n",
        ">> Dev-Accuracy = 0.7834 \\\\\n",
        ">> Test-Accuracy = 0.7811\n",
        "*   **Word2vec + CN**\n",
        ">> Dev-Accuracy = 0.7868 \\\\\n",
        ">> Test-Accuracy = 0.7796\n",
        "\n",
        "\n",
        "## Full Dataset\n",
        "\n",
        "1.   Relation extraction\n",
        "\n",
        ">*   **Glove**\n",
        ">> Accuracy: Mean = 0.7505 | SD = 0.00613 \\\\\n",
        ">> F1: Mean = 0.7194 | SD = 0.00566\n",
        "*   **Glove + CN**\n",
        ">> Accuracy: Mean = 0.7417 | SD = 0.00508 \\\\\n",
        ">> F1: Mean = 0.7121 | SD = 0.00521\n",
        "*   **Word2vec**\n",
        ">> Accuracy: Mean = 0.7534 | SD = 0.00459 \\\\\n",
        ">> F1: Mean = 0.7179 | SD = 0.00637\n",
        "*   **Word2vec + CN**\n",
        ">> Accuracy: Mean = 0.7450 | SD = 0.00495 \\\\\n",
        ">> F1: Mean = 0.7095 | SD = 0.00713\n",
        "\n",
        "2.   Sentence Polarity Classification\n",
        "\n",
        ">*   **Glove**\n",
        ">> Dev-Accuracy = Mean = 0.9108 | SD = 0.00259  \\\\\n",
        ">> Test-Accuracy = Mean = 0.9023 | SD = 0.00230\n",
        "*   **Glove + CN**\n",
        ">> Dev-Accuracy = Mean = 0.9110 | SD = 0.00149 \\\\\n",
        ">> Test-Accuracy = Mean = 0.9055 | SD = 0.00147\n",
        "*   **Word2vec**\n",
        ">> Dev-Accuracy = Mean = 0.9074 | SD = 0.00433 \\\\\n",
        ">> Test-Accuracy = Mean = 0.9029 | SD = 0.00165\n",
        "*   **Word2vec + CN**\n",
        ">> Dev-Accuracy = Mean = 0.9053 | SD = 0.00350 \\\\\n",
        ">> Test-Accuracy = Mean = 0.9026 | SD = 0.00294\n",
        "\n",
        "3.   Sentiment Classification\n",
        "\n",
        ">*   **Glove**\n",
        ">> Test-Accuracy: Mean = 0.8068 | SD = 0.00562 \\\\\n",
        ">> Test-Score: Mean = 0.8778 | SD = 0.1511\n",
        "*   **Glove + CN** \n",
        ">> Test-Accuracy: Mean = 0.8159 | SD = 0.00512 \\\\\n",
        ">> Test-Score: Mean = 0.8065 | SD = 0.1531\n",
        "*   **Word2vec**\n",
        ">> Test-Accuracy: Mean = 0.8216 | SD = 0.00366 \\\\\n",
        ">> Test-Score: Mean = 0.7241 | SD = 0.1609\n",
        "*   **Word2vec + CN**\n",
        ">> Test-Accuracy: Mean = 0.8216 | SD = 0.00366 \\\\\n",
        ">> Test-Score: Mean = 0.7241 | SD = 0.1609\n",
        "\n",
        "4.   SNLI\n",
        "\n",
        ">*   **Glove**\n",
        ">> Test-Loss = 0.7576 \\\\\n",
        ">> Test-Accuracy = 0.6868\n",
        "*   **Glove + CN**\n",
        ">> Test-Loss = 0.7643 \\\\\n",
        ">> Test-Accuracy = 0.6822\n",
        "*   **Word2vec**\n",
        ">> Test-Loss = 0.7732 \\\\\n",
        ">> Test-Accuracy = 0.6818\n",
        "*   **Word2vec + CN**\n",
        ">> Test-Loss = 0.7576 \\\\\n",
        ">> Test-Accuracy = 0.6868\n",
        "\n",
        "5.   Subjectivity Classification\n",
        "\n",
        ">*   **Glove**\n",
        ">> Dev-Accuracy = Mean = 0.7950 | SD = 0.00236 \\\\\n",
        ">> Test-Accuracy = Mean = 0.7897 | SD = 0.00355\n",
        "*   **Glove + CN**\n",
        ">> Dev-Accuracy = Mean = 0.7991 | SD = 0.00252 \\\\\n",
        ">> Test-Accuracy = Mean = 0.7903 | SD = 0.00411\n",
        "*   **Word2vec**\n",
        ">> Dev-Accuracy = Mean = 0.7894 | SD = 0.00234 \\\\\n",
        ">> Test-Accuracy = Mean = 0.7843 | SD = 0.00311\n",
        "*   **Word2vec + CN**\n",
        ">> Dev-Accuracy = Mean = 0.7863 | SD = 0.00254 \\\\\n",
        ">> Test-Accuracy = Mean = 0.7849 | SD = 0.00488\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
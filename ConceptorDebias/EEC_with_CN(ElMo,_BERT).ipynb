{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEC with CN(ElMo, BERT).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/EEC_with_CN(ElMo%2C_BERT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4wlukDN_-5XU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "t = np.transpose\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kH_k0zp7wid",
        "colab_type": "code",
        "outputId": "20167a77-c89e-4305-e97e-48fbe463b83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "cell_type": "code",
      "source": [
        "##Install flair\n",
        "!pip install -q flair allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 81kB 4.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 5.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 798kB 21.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 122kB 28.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 552kB 22.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 245kB 28.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 133kB 29.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 512kB 21.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 4.5MB 7.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 358kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 12.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 143kB 31.4MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 18.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 102kB 27.1MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 204kB 21.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 92kB 25.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 9.7MB 3.4MB/s \n",
            "\u001b[?25h  Building wheel for flair (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for segtok (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mawscli 1.16.130 has requirement botocore==1.12.120, but you'll have botocore 1.12.115 which is incompatible.\u001b[0m\n",
            "\u001b[31mallennlp 0.8.2 has requirement matplotlib==2.2.3, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WeGiuPg07zwF",
        "colab_type": "code",
        "outputId": "8ba4a2c4-0c65-4b46-c8bf-e1d8a14d3abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "cell_type": "code",
      "source": [
        "##Using flair to load emlo small embeddinngs, which is in 768 dimensions\n",
        "from flair.embeddings import BertEmbeddings, ELMoEmbeddings\n",
        "from flair.data import Sentence\n",
        "\n",
        "\n",
        "# init embedding\n",
        "elmo_embedding = ELMoEmbeddings('small')\n",
        "bert_embedding = BertEmbeddings('bert-large-uncased')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 161337.85B/s]\n",
            "100%|██████████| 54402456/54402456 [00:00<00:00, 59729002.57B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1134933.35B/s]\n",
            "100%|██████████| 1248501532/1248501532 [00:41<00:00, 30353723.35B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytQwbLLuS20p",
        "colab_type": "code",
        "outputId": "15002c3c-093a-4a20-a02a-d56771a9e369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "##Load EEC Data\n",
        "!wget http://www.saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip\n",
        "!unzip Equity-Evaluation-Corpus.zip\n",
        "##Load Packages\n",
        "import numpy as np\n",
        "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import tensorflow as tf\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "import functools as ft\n",
        "import os\n",
        "import io\n",
        "nltk.download('punkt')\n",
        "\n",
        "##Read EEC\n",
        "EEC = pd.read_csv('/content/Equity-Evaluation-Corpus.csv', header=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-23 21:52:58--  http://www.saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip\n",
            "Resolving www.saifmohammad.com (www.saifmohammad.com)... 192.185.17.122\n",
            "Connecting to www.saifmohammad.com (www.saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102568 (100K) [application/zip]\n",
            "Saving to: ‘Equity-Evaluation-Corpus.zip’\n",
            "\n",
            "Equity-Evaluation-C 100%[===================>] 100.16K   549KB/s    in 0.2s    \n",
            "\n",
            "2019-03-23 21:53:01 (549 KB/s) - ‘Equity-Evaluation-Corpus.zip’ saved [102568/102568]\n",
            "\n",
            "Archive:  Equity-Evaluation-Corpus.zip\n",
            "  inflating: Equity-Evaluation-Corpus.csv  \n",
            "  inflating: README.txt              \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DBjgXTl96jln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load BERT"
      ]
    },
    {
      "metadata": {
        "id": "asGSO5rlBHfF",
        "colab_type": "code",
        "outputId": "455919e2-40b7-40e4-c832-bd1920248e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1JyRVLZb7IdiMGGwPfXuA1f-CmCvxHDXx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JyRVLZb7IdiMGGwPfXuA1f-CmCvxHDXx\n",
            "To: /content/bert_conceptors.tar.bz2\n",
            "656MB [00:04, 148MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0mROFJ1SGRot",
        "colab_type": "code",
        "outputId": "9fb79606-0fbe-430b-90b8-b2215fcc9d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "cell_type": "code",
      "source": [
        "#Bert embeddings for certain word lists\n",
        "!tar xvjf /content/bert_conceptors.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert/\n",
            "bert/bert_WEFAT_1_Male_attributes.pkl\n",
            "bert/big_bert_gender_list_propernouns.pkl\n",
            "bert/bert_W_5_African_American_names.pkl\n",
            "bert/bert_W_2_Weapons.pkl\n",
            "bert/bert_W_1_Pleasant.pkl\n",
            "bert/bert_W_5_European_American_names.pkl\n",
            "bert/bert_W_5_Unpleasant.pkl\n",
            "bert/bert_W_6_Male_names.pkl\n",
            "bert/big_bert_gender_list_all.pkl\n",
            "bert/bert_W_10_Pleasant.pkl\n",
            "bert/bert_W_1_Insects.pkl\n",
            "bert/bert_W_5_Unused_full_list_European_American_names.pkl\n",
            "bert/bert_W_10_Young_peoples_names.pkl\n",
            "bert/bert_W_6_Female_names.pkl\n",
            "bert/bert_W_9_Physical_disease.pkl\n",
            "bert/bert_W_7_Female_terms.pkl\n",
            "bert/big_bert_gender_list_pronouns.pkl\n",
            "bert/bert_W_3_Unused_full_list_European_American_names.pkl\n",
            "bert/bert_W_4_Unused_full_list_African_American_names.pkl\n",
            "bert/bert_W_8_Male_terms.pkl\n",
            "bert/bert_WEFAT_2_Androgynous_Names.pkl\n",
            "bert/bert_WEFAT_1_Careers.pkl\n",
            "bert/bert_W_9_Temporary.pkl\n",
            "bert/bert_W_2_Unpleasant.pkl\n",
            "bert/bert_WEFAT_1_Female_attributes.pkl\n",
            "bert/bert_W_2_Instruments.pkl\n",
            "bert/bert_W_7_Male_terms.pkl\n",
            "bert/bert_W_3_Unpleasant.pkl\n",
            "bert/bert_W_7_Math.pkl\n",
            "bert/bert_WEFAT_2_Male_attributes.pkl\n",
            "bert/bert_W_4_European_American_names.pkl\n",
            "bert/bert_W_1_Unpleasant.pkl\n",
            "bert/bert_W_5_Pleasant.pkl\n",
            "bert/bert_W_4_Unpleasant.pkl\n",
            "bert/bert_W_8_Arts.pkl\n",
            "bert/bert_W_4_African_American_names.pkl\n",
            "bert/bert_W_1_Flowers.pkl\n",
            "bert/bert_W_8_Female_terms.pkl\n",
            "bert/bert_W_6_Family.pkl\n",
            "bert/bert_W_3_European_American_names.pkl\n",
            "bert/bert_W_3_Pleasant.pkl\n",
            "bert/bert_WEFAT_2_Female_attributes.pkl\n",
            "bert/bert_W_3_Unused_full_list_African_American_names.pkl\n",
            "bert/bert_W_9_Permanent.pkl\n",
            "bert/bert_W_10_Old_peoples_names.pkl\n",
            "bert/bert_W_4_Unused_full_list_European_American_names.pkl\n",
            "bert/bert_W_9_Mental_disease.pkl\n",
            "bert/big_bert_gender_list_extended.pkl\n",
            "bert/bert_W_5_Unused_full_list_African_American_names.pkl\n",
            "bert/bert_W_8_Science.pkl\n",
            "bert/bert_W_2_Pleasant.pkl\n",
            "bert/bert_W_7_Arts.pkl\n",
            "bert/bert_W_6_Career.pkl\n",
            "bert/bert_W_4_Pleasant.pkl\n",
            "bert/bert_W_10_Unpleasant.pkl\n",
            "bert/bert_W_3_African_American_names.pkl\n",
            "bert/big_bert_race_list.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xONCr7Ei-FON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load elmo race list embeddings\n",
        "Loading  ELMo embeddings for words in race name lists"
      ]
    },
    {
      "metadata": {
        "id": "gZw8xNOrIdHl",
        "colab_type": "code",
        "outputId": "aed1a2b9-d752-4b0d-9371-81fa12f9b426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1WJBoEHOBkG4EW1MkB0y1QpggtV_J0i9w"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WJBoEHOBkG4EW1MkB0y1QpggtV_J0i9w\n",
            "To: /content/elmo_race_list.pkl\n",
            "\r0.00B [00:00, ?B/s]\r4.88MB [00:00, 75.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5enovlXI8Gfn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load pre-calculated CN for ELMo and BERT embeddings"
      ]
    },
    {
      "metadata": {
        "id": "_Vkc8EKs_CaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUBsA7uWJEYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = pickle.load(open('/content/elmo_race_list.pkl','rb'))\n",
        "#x.keys()\n",
        "#dict_keys(['GnegC', 'type_embedings', 'words'])\n",
        "negC = x['GnegC'] # 'GnegC' includes pre-calculated conceptor negation matrix\n",
        "negC_elmo = np.squeeze(np.asarray(negC))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKHJutPIbWTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = pickle.load(open('/content/bert/big_bert_race_list.pkl','rb'))\n",
        "#x.keys()\n",
        "#dict_keys(['GnegC', 'type_embedings', 'words'])\n",
        "negC = x['GnegC'] # 'GnegC' includes pre-calculated conceptor negation matrix\n",
        "negC_bert = np.squeeze(np.asarray(negC))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDIh1lfL6zrd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load extra BERT for sentiment word embeddings\n",
        "Loading BERT embeddings for sentiment words: 'anger', 'fear', 'joy', and 'sadness'"
      ]
    },
    {
      "metadata": {
        "id": "9qCiPSOoUdLr",
        "colab_type": "code",
        "outputId": "abdad854-f6f6-4110-a1d0-245e36cbb2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1dBiXlxSxfKq0-OhFQyL_CCij8pmThQZI"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dBiXlxSxfKq0-OhFQyL_CCij8pmThQZI\n",
            "To: /content/bert_EEC_sentiment.pkl\n",
            "\r  0% 0.00/65.8k [00:00<?, ?B/s]\r100% 65.8k/65.8k [00:00<00:00, 25.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uPAei1Z7bsBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "senn = pickle.load(open('/content/bert_EEC_sentiment.pkl','rb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UE0jvav_b6Nv",
        "colab_type": "code",
        "outputId": "24627f67-cfa1-4a57-9a13-3558f5b97242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "senn['words']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fear', 'sadness', 'joy', 'anger']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "-2LrbWLOFlyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Racial Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "xH2Dk0jlFGDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load brown corpus to get sentiment embeddings for ELMo\n",
        "We used different ways on ELMo and BERT to get sentiment embeddings. For BERT, as we did above, we saved sentiment-specific embeddings into a pickle file. But for ELMo, we saved embeddings of words in brown corpus into a single file but didn't save sentiment-specfic embeddings into another file, so we have to use brown corpus to get sentiment embeddings."
      ]
    },
    {
      "metadata": {
        "id": "6C0mIN5MftFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD, randomized_svd\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sHTcgevTES5",
        "colab_type": "code",
        "outputId": "d0966ec2-ee43-45ff-91f9-06e35d338e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "## Get brown_emb \n",
        "!gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
        "!pip3 install -q pymagnitude\n",
        "from pymagnitude import *\n",
        "from pymagnitude import MagnitudeUtils\n",
        "import numpy as np\n",
        "import pickle\n",
        "with open('/content/elmo_embeddings_emma_brown.pkl', 'rb') as f:\n",
        "    elmo_data = pickle.load(f)\n",
        "elmo_data = elmo_data['brown_embs']\n",
        "\n",
        "## Get brown corpus\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "brown_corpus = brown.sents()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
            "To: /content/elmo_embeddings_emma_brown.pkl\n",
            "4.78GB [01:57, 40.5MB/s]\n",
            "\u001b[K    100% |████████████████████████████████| 5.4MB 6.1MB/s \n",
            "\u001b[?25h  Building wheel for pymagnitude (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7TPr6-HfTQiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#funcs to get specific word embeddings from a large embedding matrix\n",
        "def pick_embeddings(corpus, sent_embs, word_list):\n",
        "    X = []\n",
        "    #labels = []\n",
        "    #sents = []\n",
        "    for i, s in enumerate(corpus): \n",
        "       # print(s)\n",
        "        for j, w in enumerate(s): #index, words\n",
        "            if w in word_list:\n",
        "                X.append(sent_embs[i][j])\n",
        "                #labels.append(w)\n",
        "                #sents.append(s)\n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HPX2O5nJFPWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run experiment-ELMO\n"
      ]
    },
    {
      "metadata": {
        "id": "8oDXuoVDZlLi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cos_sim_raw_sent_emb(sen_emb, sent1, sent2):\n",
        "  \n",
        "  sentence1 = Sentence(sent1) # flair built-in Sentence class\n",
        "  sentence2 = Sentence(sent2) # flair built-in Sentence class\n",
        "  x = embedding.embed(sentence1) # get sentence embedding of sentence 1\n",
        "  se = torch.stack([token.embedding for token in sentence1])\n",
        "  sent1_emb = se.numpy()\n",
        "  x = embedding.embed(sentence2) # get sentence embedding of sentence 2\n",
        "  se = torch.stack([token.embedding for token in sentence2])\n",
        "  sent2_emb = se.numpy()\n",
        "  sent1_emb = np.average(sent1_emb, axis = 0) #average sentence1 embeddings\n",
        "  sent2_emb = np.average(sent2_emb, axis = 0) #average sentence2 embeddings\n",
        "  proj1 = (sent1_emb/norm(sent1_emb)).dot(sen_emb)/norm(sen_emb) #cosine similarity\n",
        "  proj2 = (sent2_emb/norm(sent2_emb)).dot(sen_emb)/norm(sen_emb) #cosine similarity\n",
        "  #print(sent2_emb)\n",
        "  #print(proj1, proj2)\n",
        "  return proj1, proj2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hnBNClCHcT_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cos_sim_conceptor_sent_emb(sen_emb, sent1, sent2):\n",
        "  sentence1 = Sentence(sent1) \n",
        "  sentence2 = Sentence(sent2)\n",
        "  x = embedding.embed(sentence1) # get sentence embedding of sentence 1\n",
        "  se = torch.stack([token.embedding for token in sentence1])\n",
        "  sent1_emb = se.numpy()\n",
        "  x = embedding.embed(sentence2) # get sentence embedding of sentence 2\n",
        "  se = torch.stack([token.embedding for token in sentence2])\n",
        "  sent2_emb = se.numpy()\n",
        "  sent1_emb = np.average(sent1_emb, axis = 0) #average sentence1 embeddings\n",
        "  sent2_emb = np.average(sent2_emb, axis = 0) #average sentence2 embeddings\n",
        "  #print(sent1_emb.T.shape)\n",
        "  sent1_emb = negC.dot(sent1_emb.T).T  # CN sentence embeddings\n",
        "  #print(sent1_emb.shape)\n",
        "  sent2_emb = negC.dot(sent2_emb.T).T  # CN sentence embeddings\n",
        "  sen_emb =  negC.dot(sen_emb.T).T # CN sentiment embedding\n",
        "  #print(sent2_emb)\n",
        "  proj1 = (sent1_emb/norm(sent1_emb)).dot(sen_emb)/norm(sen_emb) #cosine similarity\n",
        "  proj2 = (sent2_emb/norm(sent2_emb)).dot(sen_emb)/norm(sen_emb) #cosine similarity\n",
        "  #print(proj1,proj2)\n",
        "  return proj1, proj2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJFAY9VHTfD3",
        "colab_type": "code",
        "outputId": "0426b91c-16b4-4464-8ca8-e66e06ba8155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# Get ELMo racial debiasing results \n",
        "embedding = elmo_embedding\n",
        "negC = negC_elmo\n",
        "p= []\n",
        "i =0\n",
        "for sen in ['fear','sadness','joy', 'anger']:\n",
        "  aa = [] # raw score for african american\n",
        "  ea = [] # raw score for european american\n",
        "  aa_cn = [] # CN score for african american\n",
        "  ea_cn = [] # CN score for european american\n",
        "  sent_sent = [] # sentence of certain sentiment\n",
        "  sent_aa = [] # sentence of certain sentiment for african american\n",
        "  sent_ea = [] # sentence of certain sentiment for european american\n",
        "  sent_sent = EEC[EEC['Emotion']==sen] # get sentence of certain sentiment\n",
        "  sent_aa = sent_sent[(sent_sent['Race']=='African-American')] #get AA sentences\n",
        "  sent_ea = sent_sent[(sent_sent['Race']=='European')] #get EA sentences\n",
        "  sen_emb  = pick_embeddings(brown_corpus, elmo_data, str(sen)) # get certain sentiment embedding\n",
        "  sen_emb = np.average(sen_emb, axis = 0)\n",
        "  i+=1\n",
        "  print(sen+' is processed')\n",
        "  for sen1, sen2 in zip(sent_aa['Sentence'], sent_ea['Sentence']):\n",
        "    \n",
        "    score1, score2 = cos_sim_raw_sent_emb(sen_emb, sen1, sen2) #projection score on raw data\n",
        "    score3, score4 = cos_sim_conceptor_sent_emb(sen_emb, sen1, sen2) # projection score on CN\n",
        "    aa.append(score1) # raw score for african american\n",
        "    ea.append(score2) # raw score for european american\n",
        "    aa_cn.append(score3) # CN score for african american\n",
        "    ea_cn.append(score4) # CN score for european american\n",
        "  #paired t-test for raw data\n",
        "  p1 = scipy.stats.ttest_rel(aa, ea)\n",
        "  #paired t-test for CN data\n",
        "  p2 = scipy.stats.ttest_rel(aa_cn, ea_cn)\n",
        "  p.append([p1.statistic, p1.pvalue, p2.statistic, p2.pvalue])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fear is processed\n",
            "sadness is processed\n",
            "joy is processed\n",
            "anger is processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bicd-aY9i1Ya",
        "colab_type": "code",
        "outputId": "975f2fdc-2a1f-4623-810a-55f4eec40ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "cell_type": "code",
      "source": [
        "print('RAW vs CN data using ELMO, racial debiasing')\n",
        "p = pd.DataFrame(p, index=['anger','fear','joy','sadness'], columns= ['raw_t_statistic', 'raw_p_value', 'cn_t_statistic', 'cn_p_value'])\n",
        "print(p)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RAW vs CN data using ELMO, racial debiasing\n",
            "         raw_t_statistic   raw_p_value  cn_t_statistic    cn_p_value\n",
            "anger          -4.734783  2.655787e-06       -9.460424  4.570634e-20\n",
            "fear           -4.916728  1.097727e-06       -9.110020  8.551353e-19\n",
            "joy            -5.927568  4.829135e-09       -3.817722  1.466429e-04\n",
            "sadness        -6.440618  2.213304e-10      -11.236944  4.769611e-27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bAoKAMixcgJA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Bert"
      ]
    },
    {
      "metadata": {
        "id": "9jrDAQk1chdU",
        "colab_type": "code",
        "outputId": "982e13da-5333-4013-df82-7459bbc66fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# Get BERT racial debiasing results\n",
        "embedding = bert_embedding\n",
        "negC = negC_bert\n",
        "p= []\n",
        "i =0\n",
        "for sen in ['fear','sadness','joy', 'anger']:\n",
        "  aa = [] # raw score for african american\n",
        "  ea = [] # raw score for european american\n",
        "  aa_cn = [] # CN score for african american\n",
        "  ea_cn = [] # CN score for european american\n",
        "  sent_sent = [] # sentence of certain sentiment\n",
        "  sent_aa = [] # sentence of certain sentiment for african american\n",
        "  sent_ea = [] # sentence of certain sentiment for european american\n",
        "  sent_sent = EEC[EEC['Emotion']==sen] # get sentence of certain sentiment\n",
        "  sent_aa = sent_sent[(sent_sent['Race']=='African-American')] #get AA sentences\n",
        "  sent_ea = sent_sent[(sent_sent['Race']=='European')] #get EA sentences\n",
        "  sen_emb = np.squeeze(np.asarray(senn['type_embedings'][i]))\n",
        "  print(sen+' is processed')\n",
        "  i+=1\n",
        "  for sen1, sen2 in zip(sent_aa['Sentence'], sent_ea['Sentence']):\n",
        "    \n",
        "    score1, score2 = cos_sim_raw_sent_emb(sen_emb, sen1, sen2) #projection score on raw data\n",
        "    score3, score4 = cos_sim_conceptor_sent_emb(sen_emb, sen1, sen2) # projection score on CN\n",
        "    aa.append(score1) # raw score for african american\n",
        "    ea.append(score2) # raw score for european american\n",
        "    aa_cn.append(score3) # CN score for african american\n",
        "    ea_cn.append(score4) # CN score for european american\n",
        "  #paired t-test for raw data\n",
        "  p1 = scipy.stats.ttest_rel(aa, ea)\n",
        "  #paired t-test for CN data\n",
        "  p2 = scipy.stats.ttest_rel(aa_cn, ea_cn)\n",
        "  p.append([p1.statistic, p1.pvalue, p2.statistic, p2.pvalue])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fear is processed\n",
            "sadness is processed\n",
            "joy is processed\n",
            "anger is processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JEoQOavtqNNA",
        "colab_type": "code",
        "outputId": "b97cc12c-45a9-4ec2-a827-02720106daba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "cell_type": "code",
      "source": [
        "print('RAW vs CN data using BERT, racial debiasing')\n",
        "p = pd.DataFrame(p, index=['fear','sadness','joy','anger'], columns= ['raw_t_statistic', 'raw_p_value', 'cn_t_statistic', 'cn_p_value'])\n",
        "print(p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RAW vs CN data using BERT, racial debiasing\n",
            "         raw_t_statistic   raw_p_value  cn_t_statistic    cn_p_value\n",
            "fear            5.647380  2.370618e-08       22.624530  1.853297e-85\n",
            "sadness        -3.449800  5.946922e-04        3.267547  1.137887e-03\n",
            "joy             4.756097  2.398224e-06        9.040590  1.512630e-18\n",
            "anger           1.633738  1.027643e-01       13.350301  2.237348e-36\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
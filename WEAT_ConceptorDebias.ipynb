{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "## THis is how you link to a colab notebook\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/WEAT/WEAT_(Final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bY4RMFQxrWyv",
    "outputId": "7926df4a-0561-4111-95a0-2a41ddc89e38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, filterfalse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf6_liysF8en"
   },
   "outputs": [],
   "source": [
    "def swAB(W, A, B):\n",
    "  \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
    "     Arguments\n",
    "              W, A, B : n x d matrix of word embeddings stored row wise\n",
    "  \"\"\"\n",
    "  WA = cosine_similarity(W,A)\n",
    "  WB = cosine_similarity(W,B)\n",
    "  \n",
    "  #Take mean along columns\n",
    "  WAmean = np.mean(WA, axis = 1)\n",
    "  WBmean = np.mean(WB, axis = 1)\n",
    "  \n",
    "  return (WAmean - WBmean)\n",
    "  \n",
    "def test_statistic(X, Y, A, B):\n",
    "  \"\"\"Calculates test-statistic between the pair of association words and target words\n",
    "     Arguments\n",
    "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
    "     Returns\n",
    "              Test Statistic\n",
    "  \"\"\"\n",
    "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9ZO4--vpBh6"
   },
   "source": [
    "## Effect Size\n",
    "\n",
    "The ''effect size'' is a normalized measure of how separated the two distributions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHPKwHLjo-m8"
   },
   "outputs": [],
   "source": [
    "def weat_effect_size(X, Y, A, B, embd):\n",
    "  \"\"\"Computes the effect size for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "     Returns\n",
    "              Effect Size\n",
    "  \"\"\"\n",
    "  \n",
    "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "  \n",
    "  XuY = list(set(X).union(Y))\n",
    "  XuYmat = []\n",
    "  for w in XuY:\n",
    "    if w.lower() in embd:\n",
    "      XuYmat.append(embd[w.lower()])\n",
    "  XuYmat = np.array(XuYmat)\n",
    "\n",
    "  \n",
    "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
    "  \n",
    "  return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8foGwVSGI16"
   },
   "source": [
    "## P-Value\n",
    "\n",
    "The one-sided P value measures the likelihood that a random permutation of the attribute words would produce at least the observed test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDy-duFOFj71"
   },
   "outputs": [],
   "source": [
    "def random_permutation(iterable, r=None):\n",
    "  \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
    "  pool = tuple(iterable)\n",
    "  r = len(pool) if r is None else r\n",
    "  return tuple(random.sample(pool, r))\n",
    "\n",
    "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
    "  \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "              sample : Number of random permutations used.\n",
    "     Returns\n",
    "  \"\"\"\n",
    "  size_of_permutation = min(len(X), len(Y))\n",
    "  X_Y = X + Y\n",
    "  test_stats_over_permutation = []\n",
    "  \n",
    "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "  \n",
    "  if not sample:\n",
    "      permutations = combinations(X_Y, size_of_permutation)\n",
    "  else:\n",
    "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
    "      \n",
    "  for Xi in permutations:\n",
    "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
    "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
    "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
    "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
    "    \n",
    "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
    "  \n",
    "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
    "  \n",
    "  return is_over.sum() / is_over.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbRMmhwbGL98"
   },
   "source": [
    "## Test on sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings into Gensim Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Word-Node2Vec embedding has been loaded\n"
     ]
    }
   ],
   "source": [
    "wordNode2Vec = KeyedVectors.load_word2vec_format('data/embeddings/Word-Node2Vec/data_node2vec/dbpedia.cwvec12.200.vec')\n",
    "print('The Word-Node2Vec embedding has been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec = KeyedVectors.load_word2vec_format('data/embeddings/Word2Vec/GoogleNews-vectors-negative300.bin', \n",
    "                                             binary = True)\n",
    "print(\"Word2Vec embedding has been loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding has been loaded.\n"
     ]
    }
   ],
   "source": [
    "glove = KeyedVectors.load_word2vec_format('data/embeddings/GloVe/GloVe_twitter_27B/glove.twitter.27B.100d.txt', \n",
    "                                          binary = False, \n",
    "                                          no_header = True)\n",
    "\n",
    "print(\"GloVe embedding has been loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upscdvtwFm0l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE\n",
      "WEAT d =  1.0688018\n",
      "WEAT p =  0.0\n",
      "Word-Node2Vec\n",
      "WEAT d =  1.2736316\n",
      "WEAT p =  0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"List of association and target word pairs for the sample test (Instruments, Weapons) vs (Pleasant, Unpleasant)\"\"\"\n",
    "\n",
    "# Instruments\n",
    "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
    "\"flute\", \"horn\", \"saxophone\", \"violin\"] \n",
    "# Weapons\n",
    "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
    "    \"mace\", \"slingshot\", \"whip\"] \n",
    "# Pleasant\n",
    "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
    "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
    "# Unpleasant\n",
    "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
    "    \"vomit\", \"agony\", \"prison\"] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Compute the effect-size and P value\"\"\"\n",
    "print(\"GLOVE\")\n",
    "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
    "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))\n",
    "\n",
    "print(\"Word-Node2Vec\")\n",
    "print('WEAT d = ', weat_effect_size(X, Y, A, B, wordNode2Vec))\n",
    "print('WEAT p = ', weat_p_value(X, Y, A, B, wordNode2Vec, 1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfwYNFl2ckGy"
   },
   "source": [
    "# Load all vectors and compute the conceptor\n",
    "\n",
    "A conceptor matrix, $C$, is a regularized identity map (in our case, from the original word embeddings to their debiased versions) that minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "\\|Z - CZ\\|_F^2 + \\alpha^{-2}\\|C\\|_{F}^2.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha^{-2}$ is a scalar parameter.\n",
    "Given that many readers will be unfamiliar with conceptors, we reintroduce matrix conceptors. \n",
    "\n",
    "$C$ has a closed form solution: \n",
    "\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{k} Z Z^{\\top} (\\frac{1}{k} Z Z^{\\top}+\\alpha^{-2} I)^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "Intuitively, $C$ is a soft projection matrix on the linear subspace where the word embeddings $Z$ have the highest variance. Once $C$ has been learned, it can be 'negated' by subtracting it from the identity matrix and then applied to any word embeddings (e.g., those defined by the lists, $\\mathcal{X}$ and $\\mathcal{Y}$) to remove the biased subspace.\n",
    "\n",
    "Conceptors can represent laws of Boolean logic, such as NOT $\\neg$, AND $\\wedge$ and OR $\\vee$. For two conceptors $C$ and $B$, we define the following operations:\n",
    "\\begin{align*}\n",
    "\\neg C:=&  I-C,  \\\\\n",
    "C\\wedge B:=&  (C^{-1} + B^{-1} - I)^{-1} \\\\\n",
    "C \\vee B:=&\\neg(\\neg C \\wedge \\neg B) \n",
    "\\end{align*}\n",
    "\n",
    "Given that the conceptor, $C$, represents the subspace of maximum bias, we want to apply the negated conceptor, NOT $C$ to an embedding space remove its bias. We call NOT $C$ the *debiasing conceptor*. More generally, if we have $K$ conceptors, $C_i$ derived from $K$ different word lists, we call NOT $(C_1 \\vee ... \\vee C_K)$ a debiasing conceptor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEuq-Lt5ctV2"
   },
   "source": [
    "Clone our repository. The repository contains code for computing the conceptors. It also includes word lists needed representing different subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "vqaF4DyZxjLl",
    "outputId": "4034238f-0529-42cb-bb8b-da1247b24023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'ConceptorDebias': No such file or directory\n",
      "Cloning into 'ConceptorDebias'...\n",
      "remote: Enumerating objects: 84, done.\u001b[K\n",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
      "remote: Total 392 (delta 40), reused 5 (delta 2), pack-reused 308\u001b[K\n",
      "Receiving objects: 100% (392/392), 3.70 MiB | 20.92 MiB/s, done.\n",
      "Resolving deltas: 100% (206/206), done.\n",
      "Branch 'ACL-cleanup' set up to track remote branch 'ACL-cleanup' from 'origin'.\n",
      "Switched to a new branch 'ACL-cleanup'\n"
     ]
    }
   ],
   "source": [
    "# our code for debiasing -- also includes word lists\n",
    "!rm -r ConceptorDebias\n",
    "!git clone https://github.com/jsedoc/ConceptorDebias\n",
    "!cd ConceptorDebias;\n",
    "\n",
    "sys.path.append('/content/ConceptorDebias')\n",
    "\n",
    "from Conceptors.conceptor_fxns import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BoXwMINw0sA"
   },
   "source": [
    "## Compute the conceptor matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAG4oqwIc1Z3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_cn_matrix(subspace, alpha = 2):\n",
    "  \"\"\"Returns the conceptor negation matrix\n",
    "  Arguments\n",
    "           subspace : n x d matrix of word vectors from a oarticular subspace\n",
    "           alpha : Tunable parameter\n",
    "  \"\"\"\n",
    "  # Compute the conceptor matrix\n",
    "  C,_ = train_Conceptor(subspace, alpha)\n",
    "  \n",
    "  # Calculate the negation of the conceptor matrix\n",
    "  negC = NOT(C)\n",
    "  \n",
    "  return negC\n",
    "\n",
    "def apply_conceptor(x, C):\n",
    "  \"\"\"Returns the conceptored embeddings\n",
    "  Arguments\n",
    "           x : n x d matrix of all words to be conceptored\n",
    "           C : d x d conceptor matrix\n",
    "  \"\"\"\n",
    "  # Post-process the vocab matrix\n",
    "  newX = (C @ x).T\n",
    "  \n",
    "  return newX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHaXxx5L3Ugb"
   },
   "source": [
    "## Load embeddings of all words from the ref. wordlist from a specific embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WBI1VED3bpM"
   },
   "outputs": [],
   "source": [
    "def load_all_vectors(embd, wikiWordsPath):\n",
    "  \"\"\"Loads all word vectors for all words in the list of words as a matrix\n",
    "  Arguments\n",
    "           embd : Dictonary of word-to-embedding for all words\n",
    "           wikiWordsPath : URL to the path where all embeddings are stored\n",
    "  Returns\n",
    "          all_words_index : Dictonary of words to the row-number of the corresponding word in the matrix\n",
    "          all_words_mat : Matrix of word vectors stored row-wise\n",
    "  \"\"\"\n",
    "  all_words_index = {}\n",
    "  all_words_mat = []\n",
    "  with open(wikiWordsPath, \"r+\") as f_in:\n",
    "    ind = 0\n",
    "    for line in f_in:\n",
    "      word = line.split(' ')[0]\n",
    "      if word in embd:\n",
    "        all_words_index[word] = ind\n",
    "        all_words_mat.append(embd[word])\n",
    "        ind = ind+1\n",
    "        \n",
    "  return all_words_index, all_words_mat\n",
    "\n",
    "def load_subspace_vectors(embd, subspace_words):\n",
    "  \"\"\"Loads all word vectors for the particular subspace in the list of words as a matrix\n",
    "  Arguments\n",
    "           embd : Dictonary of word-to-embedding for all words\n",
    "           subspace_words : List of words representing a particular subspace\n",
    "  Returns\n",
    "          subspace_embd_mat : Matrix of word vectors stored row-wise\n",
    "  \"\"\"\n",
    "  subspace_embd_mat = []\n",
    "  ind = 0\n",
    "  for word in subspace_words:\n",
    "    if word in embd:\n",
    "      subspace_embd_mat.append(embd[word])\n",
    "      ind = ind+1\n",
    "      \n",
    "  return subspace_embd_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzfTYgq0I9WW"
   },
   "source": [
    "## Load all word lists for the ref. subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "D4kndsi_DwNr",
    "outputId": "366d577e-6e20-42a4-8217-9635e7d20b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-18 15:55:21--  https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20190320-words-frequency.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27465330 (26M) [text/plain]\n",
      "Saving to: ‘enwiki-20190320-words-frequency.txt’\n",
      "\n",
      "enwiki-20190320-wor 100%[===================>]  26.19M   166MB/s    in 0.2s    \n",
      "\n",
      "2019-04-18 15:55:22 (166 MB/s) - ‘enwiki-20190320-words-frequency.txt’ saved [27465330/27465330]\n",
      "\n",
      "fatal: destination path 'SIF' already exists and is not an empty directory.\n",
      "fatal: destination path 'gn_glove' already exists and is not an empty directory.\n",
      "fatal: destination path 'corefBias' already exists and is not an empty directory.\n",
      "--2019-04-18 15:55:25--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
      "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
      "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35751 (35K) [text/plain]\n",
      "Saving to: ‘female.txt.1’\n",
      "\n",
      "female.txt.1        100%[===================>]  34.91K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-04-18 15:55:26 (479 KB/s) - ‘female.txt.1’ saved [35751/35751]\n",
      "\n",
      "--2019-04-18 15:55:26--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
      "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
      "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20466 (20K) [text/plain]\n",
      "Saving to: ‘male.txt.1’\n",
      "\n",
      "male.txt.1          100%[===================>]  19.99K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-04-18 15:55:26 (253 MB/s) - ‘male.txt.1’ saved [20466/20466]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# General word list\n",
    "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20190320-words-frequency.txt\n",
    "!git clone https://github.com/PrincetonML/SIF\n",
    "    \n",
    "# Gender word lists\n",
    "!git clone https://github.com/uclanlp/gn_glove\n",
    "!git clone https://github.com/uclanlp/corefBias\n",
    "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
    "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
    "\n",
    "from lists.load_word_lists import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orBvfpqCFOfa"
   },
   "outputs": [],
   "source": [
    "\"\"\"Load list of pronouns representing the 'Pronoun' subspace for gender debiasing\"\"\"\n",
    "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
    "gender_list_pronouns = list(set(gender_list_pronouns))\n",
    "\n",
    "\"\"\"Load an extended list of words representing the gender subspace for gender debiasing\"\"\"\n",
    "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
    "gender_list_extended = list(set(gender_list_extended))\n",
    "\n",
    "\"\"\"Load list of proper nouns representing the 'Proper Noun' subspace for gender debiasing\"\"\"\n",
    "gender_list_propernouns = male_cmu + female_cmu\n",
    "gender_list_propernouns = list(set(gender_list_propernouns))\n",
    "\n",
    "\"\"\"Load list of all representing the gender subspace for gender debiasing\"\"\"\n",
    "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
    "gender_list_all = list(set(gender_list_all))\n",
    "\n",
    "\"\"\"Load list of common black and white names for racial debiasing\"\"\"\n",
    "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
    "race_list = list(set(race_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEiBEVI2I2WR"
   },
   "source": [
    "## Load different embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nrlqu2xvJH_6"
   },
   "source": [
    "**Glove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "PFNSjmY8I1Ut",
    "outputId": "76e70fa5-89cf-45d4-a2e4-2977bc7ab66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
      "To: /content/gensim_glove.840B.300d.txt.bin\n",
      "2.65GB [00:24, 110MB/s]\n",
      "The glove embedding has been loaded!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Download the 'Glove' embeddings if not downloaded\"\"\"\n",
    "!if [ ! -f /content/gensim_glove.840B.300d.txt.bin ]; then gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2; fi\n",
    "\n",
    "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
    "resourceFile = ''\n",
    "if 'glove' not in dir():\n",
    "  glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
    "  print('The glove embedding has been loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aD-bmiCdGKYt"
   },
   "outputs": [],
   "source": [
    "\"\"\"Sample output of the glove embeddings\"\"\"\n",
    "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
    "print(X)\n",
    "a = [glove[w] for w in X if w.lower() in glove]\n",
    "print(np.array(a).shape)\n",
    "glove['Brad']\n",
    "#glove['brad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84af1zflJKLQ"
   },
   "source": [
    "**Word2ve**c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "OmJlU2IiJMMj",
    "outputId": "1b302430-806a-436e-addc-1eae7d51ef1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already downloaded\n",
      "The word2vec embedding has been loaded!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Download the 'Word2Vec' embeddings if not downloaded\"\"\"\n",
    "!if test -e /content/GoogleNews-vectors-negative300.bin.gz || test -e /content/GoogleNews-vectors-negative300.bin; then echo 'file already downloaded'; else echo 'starting download'; gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM; fi\n",
    "!if [ ! -f /content/GoogleNews-vectors-negative300.bin ]; then gunzip GoogleNews-vectors-negative300.bin.gz; fi\n",
    "\n",
    "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
    "resourceFile = ''\n",
    "if 'word2vec' not in dir():\n",
    "  word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "  print('The word2vec embedding has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCiZfu13JPHl"
   },
   "source": [
    "**Fasttex**t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A9zwQ2T_JRBo",
    "outputId": "454342eb-e607-451e-e70d-97f1270a2ef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fasttext embedding has been loaded!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Download the 'Fasttext' embeddings if not downloaded\"\"\"\n",
    "!if [ ! -f /content/fasttext.bin ]; then gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh; fi\n",
    "\n",
    "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
    "resourceFile = ''\n",
    "if 'fasttext' not in dir():\n",
    "  fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
    "  print('The fasttext embedding has been loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hL2zgX6JRhG"
   },
   "source": [
    "**ELMo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXvK1wuvJTVP"
   },
   "outputs": [],
   "source": [
    "\"\"\"Download the 'ELMo' embeddings if not downloaded\"\"\"\n",
    "!if [ ! -f /content/elmo_embeddings_emma_brown.pkl ]; then gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU; fi\n",
    "\n",
    "\"\"\"Load the embeddings to a dictonary\"\"\"\n",
    "data = pickle.load(open(\"elmo_embeddings_emma_brown.pkl\", \"rb\"))\n",
    "\n",
    "def pick_embeddings(corpus, sent_embs):\n",
    "    X = []\n",
    "    labels = {}\n",
    "    sents = []\n",
    "    ind = 0\n",
    "    for i, s in enumerate(corpus):\n",
    "        for j, w in enumerate(s):\n",
    "            X.append(sent_embs[i][j])\n",
    "            if w.lower() in labels:\n",
    "              labels[w.lower()].append(ind)\n",
    "            else:\n",
    "              labels[w.lower()] = [ind]\n",
    "            sents.append(s)\n",
    "            ind = ind + 1\n",
    "    return (X, labels, sents)\n",
    "  \n",
    "def get_word_list(path):\n",
    "    word_list = []\n",
    "    with open(path, \"r+\") as f_in:\n",
    "      for line in f_in:\n",
    "        word = line.split(' ')[0]\n",
    "        word_list.append(word.lower())\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def load_subspace_vectors_contextual(all_mat, all_index, subspace_list):\n",
    "    subspace_mat = []\n",
    "    for w in subspace_list:\n",
    "      if w.lower() in all_index:\n",
    "        for i in all_index[w.lower()]:\n",
    "          #print(type(i))\n",
    "          subspace_mat.append(all_mat[i])\n",
    "    #subspace_mat = [all_mat[i,:] for i in all_index[w.lower()] for w in subspace_list if w.lower() in all_index]\n",
    "    print(\"Subspace: \", np.array(subspace_mat).shape)\n",
    "    return subspace_mat\n",
    "  \n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "brown_corpus = brown.sents()\n",
    "elmo = data['brown_embs']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TkER5fbdsFO"
   },
   "source": [
    "**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDL0wCtSwoOx"
   },
   "outputs": [],
   "source": [
    "def load_bert(all_dict, subspace):\n",
    "  \"\"\"Loads all embeddings in a matrix and a dictonary of words to row numbers\"\"\"\n",
    "  all_mat = all_dict['big_bert_' + subspace + '.pkl']['type_embedings']\n",
    "  words = []\n",
    "  for name in all_dict:\n",
    "    all_mat = np.concatenate((all_mat, all_dict[name]['type_embedings']))\n",
    "    words += all_dict[name]['words']\n",
    "  \n",
    "  words = [w.lower() for w in words]\n",
    "  all_words_index = {}\n",
    "  for i,a in enumerate(words):\n",
    "    all_words_index[a] = i\n",
    "    \n",
    "  return all_words_index, all_mat\n",
    "\n",
    "def load_bert_conceptor(all_dict, subspace):\n",
    "  \"\"\"Loads the required BERT conceptor matrix\"\"\"\n",
    "  if subspace == 'gender_list_pronouns':\n",
    "    cn = all_dict['big_bert_gender_list_pronouns.pkl']['GnegC']\n",
    "  elif subspace == 'gender_list_propernouns':\n",
    "    cn = all_dict['big_bert_gender_list_propernouns.pkl']['GnegC']\n",
    "  elif subspace == 'gender_list_extended':\n",
    "    cn = all_dict['big_bert_gender_list_extended.pkl']['GnegC']\n",
    "  elif subspace == 'gender_list_all':\n",
    "    cn = all_dict['big_bert_gender_list_all.pkl']['GnegC']\n",
    "  elif subspace == 'race_list':\n",
    "    cn = all_dict['big_bert_race_list.pkl']['GnegC']\n",
    "  \n",
    "  return cn\n",
    "\n",
    "\"\"\"Load all bert embeddings in a dictonary\"\"\"\n",
    "all_dict = {}\n",
    "for filename in os.listdir('/home/saketk/bert'):\n",
    "  all_dict[filename] = pickle.load(open(filename, \"rb\"))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkVyOfPKuzpp"
   },
   "source": [
    "**Custom embeddings (From text file)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bD4pT7DDu5si"
   },
   "outputs": [],
   "source": [
    "\"\"\"Download your custom embeddings (text file)\"\"\"\n",
    "\n",
    "\"\"\"Convert to word2vec format (if in GloVe format)\"\"\"\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "input_file = 'glove.txt'\n",
    "output_file = 'word2vec.txt'\n",
    "glove2word2vec(input_file, output_file)\n",
    "\n",
    "\"\"\"Load embeddings as gensim object\"\"\"\n",
    "custom = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIvNmLw-vPWN"
   },
   "source": [
    "# WEAT on Conceptored embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPP2UaVDy2r_"
   },
   "source": [
    "## Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ouWsWnl0csn"
   },
   "outputs": [],
   "source": [
    "resourceFile = ''\n",
    "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
    "\n",
    "\"\"\"Set the embedding to be used\"\"\"\n",
    "embd = 'glove'\n",
    "\n",
    "\"\"\"Set the subspace to be tested on\"\"\"\n",
    "subspace = 'gender_list_all' \n",
    "\n",
    "\"\"\"Load association and target word pairs\"\"\"\n",
    "X = WEATLists.W_8_Science\n",
    "Y = WEATLists.W_8_Arts\n",
    "A = WEATLists.W_8_Male_terms\n",
    "B = WEATLists.W_8_Female_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPCAcond1_b2"
   },
   "source": [
    "## Load the vectors as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3-Nqa2J2FoF"
   },
   "outputs": [],
   "source": [
    "curr_embd = eval(embd)\n",
    "  \n",
    "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
    "if embd == 'elmo':\n",
    "  all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
    "if embd == 'bert':\n",
    "  all_words_index, all_words_mat = load_bert(all_dict, subspace)\n",
    "else:\n",
    "  all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8d4ERsozAVK"
   },
   "source": [
    "## Compute the conceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "dfO_XltO3hp5",
    "outputId": "bdd12666-6a9f-4c09-a85f-d713c04ba655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "(300, 7270)\n",
      "R calculated\n",
      "C calculated\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
    "if subspace != 'without_conceptor':\n",
    "  subspace_words_list = eval(subspace)\n",
    "  if subspace == 'gender_list_and':\n",
    "    if embd == 'elmo':\n",
    "      subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
    "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
    "\n",
    "      subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
    "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
    "\n",
    "      subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
    "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
    "\n",
    "      cn = AND(cn1, AND(cn2, cn3))\n",
    "    elif embd == 'bert':\n",
    "      cn1 = load_bert_conceptor(all_dict, gender_list_pronouns)\n",
    "      \n",
    "      cn2 = load_bert_conceptor(all_dict, gender_list_extended)\n",
    "      \n",
    "      cn3 = load_bert_conceptor(all_dict, gender_list_propernouns)\n",
    "      \n",
    "      cn = AND(cn1, AND(cn2, cn3))\n",
    "    else:\n",
    "      subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
    "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
    "\n",
    "      subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
    "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
    "\n",
    "      subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
    "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
    "\n",
    "      cn = AND(cn1, AND(cn2, cn3))\n",
    "  else: \n",
    "    if embd == 'elmo':\n",
    "      subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
    "      cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
    "    elif embd == 'bert':\n",
    "      cn = load_bert_conceptor(all_dict, subspace)\n",
    "    else:\n",
    "      subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
    "      cn = process_cn_matrix(np.array(subspace_words_mat).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xj-aXqB2zOpI"
   },
   "source": [
    "## Compute conceptored embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoQGSnAH4aZ-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Conceptor all embeddings\"\"\"\n",
    "all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
    "\n",
    "\"\"\"Store all conceptored words in a dictonary\"\"\"\n",
    "all_words = {}\n",
    "for word, index in all_words_index.items():\n",
    "  if embd == 'elmo':\n",
    "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
    "  else:\n",
    "    all_words[word] = all_words_cn[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVmZBwykJmgn"
   },
   "source": [
    "## Calculate WEAT scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PPukMLU2ZosX",
    "outputId": "187bab01-20a1-4ce3-a590-5bd239c4204f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAT d =  0.6839113746730434\n",
      "WEAT p =  0.094\n"
     ]
    }
   ],
   "source": [
    "d = weat_effect_size(X, Y, A, B, all_words)\n",
    "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
    "\n",
    "print('WEAT d = ', d)\n",
    "print('WEAT p = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gbg0wsR_Ykym"
   },
   "source": [
    "# Hard Debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEzUV_AOtRAd"
   },
   "source": [
    "## Mu et. al. Hard Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0ZvHFuFtV_v"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def hard_debias(all_words, subspace):\n",
    "   \"\"\"Project off the first principal component (of the subspace) from all word vectors\n",
    "  Arguments\n",
    "           all_words : Matrix of word vectors of all words stored row-wise\n",
    "           subspace : Matrix of words representing a particular subspace stored row-wise\n",
    "  Returns\n",
    "          ret : Matrix of debiased word vectors stored row-wise\n",
    "  \"\"\"\n",
    "  all_words = np.array(all_words)\n",
    "  subspace = np.array(subspace)\n",
    "  \n",
    "  # Compute the first principal component of the subspace matrix\n",
    "  pca = PCA(n_components = 1)\n",
    "  pca.fit(subspace)\n",
    "  pc1 = np.array(pca.components_)\n",
    "  \n",
    "  # Project off the first PC from all word vectors\n",
    "  temp = (pc1.T @ (pc1 @ all_words.T)).T\n",
    "  ret = all_words - temp\n",
    "  \n",
    "  return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "938VVWqdFxeE"
   },
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2a2HaGBF4N4"
   },
   "outputs": [],
   "source": [
    "resourceFile = ''\n",
    "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
    "\n",
    "\"\"\"Set the embedding to be used\"\"\"\n",
    "embd = 'glove'\n",
    "\n",
    "\"\"\"Set the subspace to be tested on\"\"\"\n",
    "subspace = 'gender_list_all' \n",
    "\n",
    "\"\"\"Load association and target word pairs\"\"\"\n",
    "X = WEATLists.W_8_Science\n",
    "Y = WEATLists.W_8_Arts\n",
    "A = WEATLists.W_8_Male_terms\n",
    "B = WEATLists.W_8_Female_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVR4ydFvF4Xe"
   },
   "source": [
    "### Load the vectors as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hfyet0eF4g9"
   },
   "outputs": [],
   "source": [
    "curr_embd = eval(embd)\n",
    "  \n",
    "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
    "if embd == 'elmo':\n",
    "  all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
    "if embd == 'bert':\n",
    "  all_words_index, all_words_mat = load_bert(all_dict, subspace)\n",
    "else:\n",
    "  all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QnmMCGW7F4pw"
   },
   "source": [
    "### Debias the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzSiXFUlF47C"
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
    "if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
    "  subspace_words_list = eval(subspace)\n",
    "  \n",
    "if subspace != 'without_debiasing':\n",
    "  if embd == 'elmo' or embd == 'bert':\n",
    "    subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
    "    all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
    "  else:\n",
    "    subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
    "    all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
    "else:\n",
    "  all_words_cn = all_words_mat\n",
    "\n",
    "all_words_cn = np.array(all_words_cn)\n",
    "\n",
    "#Store all conceptored words in a dictonary\n",
    "all_words = {}\n",
    "for word, index in all_words_index.items():\n",
    "  if embd == 'elmo' or embd == 'bert':\n",
    "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
    "  else:\n",
    "    all_words[word] = all_words_cn[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ary7T5NGI9F"
   },
   "source": [
    "### Calculate WEAT scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cthQht44GJPQ"
   },
   "outputs": [],
   "source": [
    "d = weat_effect_size(X, Y, A, B, all_words)\n",
    "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
    "\n",
    "print('WEAT d = ', d)\n",
    "print('WEAT p = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iP9IM02Ln-qp"
   },
   "source": [
    "## Bolukbasi hard debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-5CfftKoGCt"
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper methods to debias word embeddings as proposed by the original authors\"\"\"\n",
    "\n",
    "def doPCA(pairs, mat, index, num_components = 5):\n",
    "    matrix = []\n",
    "    for a, b in pairs:\n",
    "        center = (mat[index[a.lower()]] + mat[index[b.lower()]])/2\n",
    "        matrix.append(mat[index[a.lower()]] - center)\n",
    "        matrix.append(mat[index[b.lower()]] - center)\n",
    "    matrix = np.array(matrix)\n",
    "    pca = PCA(n_components = num_components)\n",
    "    pca.fit(matrix)\n",
    "    # bar(range(num_components), pca.explained_variance_ratio_)\n",
    "    return pca\n",
    "\n",
    "def drop(u, v):\n",
    "    return u - v * u.dot(v) / v.dot(v)\n",
    "  \n",
    "def normalize(all_words_mat):\n",
    "    all_words_mat /= np.linalg.norm(all_words_mat, axis=1)[:, np.newaxis]\n",
    "    return all_words_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWOwX9wHoB-7"
   },
   "outputs": [],
   "source": [
    "def debias(all_words_mat, all_words_index, gender_specific_words, definitional, equalize):\n",
    "    \"\"\"Debiases the word vectors as proposed by the original authors\n",
    "    Arguments\n",
    "             all_words_mat : Matrix of word vectors of all words stored row-wise\n",
    "             all_words_index : Dictonary of words to row number in the matrix\n",
    "             gender_specific_words : List of words defining the subspace\n",
    "             definitional : List of definitional words\n",
    "             equalize : List of tuples defined as the set of equalize pairs (downloaded)\n",
    "    Returns\n",
    "            all_words_mat : Matrix of debiased word vectors stored row-wise\n",
    "    \"\"\"\n",
    "    gender_direction = doPCA(definitional, all_words_mat, all_words_index).components_[0]\n",
    "    specific_set = set(gender_specific_words)\n",
    "    for w in list(all_words_index.keys()):\n",
    "        if w not in specific_set:\n",
    "            all_words_mat[all_words_index[w.lower()]] = drop(all_words_mat[all_words_index[w.lower()]], gender_direction)\n",
    "    all_words_mat = normalize(all_words_mat)\n",
    "    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n",
    "                                                     (e1.title(), e2.title()),\n",
    "                                                     (e1.upper(), e2.upper())]}\n",
    "    print(candidates)\n",
    "    for (a, b) in candidates:\n",
    "        if (a.lower() in all_words_index and b.lower() in all_words_index):\n",
    "            y = drop((all_words_mat[all_words_index[a.lower()]] + all_words_mat[all_words_index[b.lower()]]) / 2, gender_direction)\n",
    "            z = np.sqrt(1 - np.linalg.norm(y)**2)\n",
    "            if (all_words_mat[all_words_index[a.lower()]] - all_words_mat[all_words_index[b.lower()]]).dot(gender_direction) < 0:\n",
    "                z = -z\n",
    "            all_words_mat[all_words_index[a.lower()]] = z * gender_direction + y\n",
    "            all_words_mat[all_words_index[b.lower()]] = -z * gender_direction + y\n",
    "    all_words_mat = normalize(all_words_mat)\n",
    "    return all_words_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5J-1XQaGKVgx"
   },
   "source": [
    "### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bj3J-omPKVoS"
   },
   "outputs": [],
   "source": [
    "resourceFile = ''\n",
    "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
    "\n",
    "!git clone https://github.com/tolga-b/debiaswe.git\n",
    "\n",
    "\"\"\"Load definitional and equalize lists\"\"\"\n",
    "%cd debiaswe_tutorial/debiaswe/\n",
    "# Lets load some gender related word lists to help us with debiasing\n",
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f) #gender definitional words\n",
    "\n",
    "defs_list = []\n",
    "for pair in defs:\n",
    "  defs_list.append(pair[0])\n",
    "  defs_list.append(pair[1])\n",
    "\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f) \n",
    "\n",
    "%cd ../../\n",
    "!ls\n",
    "\n",
    "\"\"\"Set the embedding to be used\"\"\"\n",
    "embd = 'glove'\n",
    "\n",
    "\"\"\"Set the subspace to be tested on\"\"\"\n",
    "subspace = 'gender_list_all' \n",
    "\n",
    "\"\"\"Load association and target word pairs\"\"\"\n",
    "X = WEATLists.W_8_Science\n",
    "Y = WEATLists.W_8_Arts\n",
    "A = WEATLists.W_8_Male_terms\n",
    "B = WEATLists.W_8_Female_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GK9EIJ5wKVvv"
   },
   "source": [
    "### Load the vectors as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AwttPHYxKV3C"
   },
   "outputs": [],
   "source": [
    "curr_embd = eval(embd)\n",
    "  \n",
    "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
    "all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRIXvmLbKV9K"
   },
   "source": [
    "### Debias the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXqwn2aOKWEL"
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
    "if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
    "  subspace_words_list = eval(subspace)\n",
    "  \n",
    "if subspace != 'without_debiasing':\n",
    "  all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
    "else:\n",
    "  all_words_cn = all_words_mat\n",
    "\n",
    "all_words_cn = np.array(all_words_cn)\n",
    "\n",
    "#Store all conceptored words in a dictonary\n",
    "all_words = {}\n",
    "for word, index in all_words_index.items():\n",
    "  all_words[word] = all_words_cn[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TvmetCG_Ke83"
   },
   "source": [
    "### Calculate WEAT scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77y-UB_-KfE6"
   },
   "outputs": [],
   "source": [
    "d = weat_effect_size(X, Y, A, B, all_words)\n",
    "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
    "\n",
    "print('WEAT d = ', d)\n",
    "print('WEAT p = ', p)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "WEAT (Final).ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

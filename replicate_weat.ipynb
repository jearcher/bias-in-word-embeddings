{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Config ###\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "import random\n",
    "import read_config\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import stats\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_assoc(w,A,B,embedding):\n",
    "    \"\"\"\n",
    "    Calculates difference in mean cosine similarity between a word and two sets\n",
    "    of words.\n",
    "    \"\"\"\n",
    "    return embedding.n_similarity([w],A) - embedding.n_similarity([w],B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_assoc(X,Y,A,B,embedding):\n",
    "    \"\"\"\n",
    "    Caclulates the WEAT test statics for four sets of words in an embeddings\n",
    "    \"\"\"\n",
    "    word_assoc_X = np.array(list(map(lambda x : word_assoc(x,A,B,embedding), X)))\n",
    "    word_assoc_Y = np.array(list(map(lambda y : word_assoc(y,A,B,embedding), Y)))\n",
    "    mean_diff = np.mean(word_assoc_X) - np.mean(word_assoc_Y)\n",
    "    std = np.std(np.concatenate((word_assoc_X, word_assoc_Y), axis=0))\n",
    "    return mean_diff / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_scores_mean_err(word_pairs,embedding):\n",
    "    \"\"\"\n",
    "    Caculate the mean WEAT statistic and standard error using a permutation test\n",
    "    on the sets of words (defaults to 100 samples)\n",
    "    \"\"\"\n",
    "    # divide smaller word_list by two\n",
    "    subset_size_target = min(len(word_pairs['X']),len(word_pairs['Y']))//2\n",
    "    subset_size_attr = min(len(word_pairs['A']),len(word_pairs['B']))//2\n",
    "    bias_scores = []\n",
    "    for i in range(100):\n",
    "        sX = np.random.choice(word_pairs['X'],subset_size_target,replace=False)\n",
    "        sY = np.random.choice(word_pairs['Y'],subset_size_target,replace=False)\n",
    "        sA = np.random.choice(word_pairs['A'],subset_size_attr,replace=False)\n",
    "        sB = np.random.choice(word_pairs['B'],subset_size_attr,replace=False)\n",
    "        bias_scores.append(diff_assoc(sX,sY,sA,sB,embedding))\n",
    "    return np.mean(bias_scores), stats.sem(bias_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(config, embedding):\n",
    "    word_pairs = {}\n",
    "    min_len = sys.maxsize\n",
    "    # Only include words that are present in the word embedding\n",
    "    for word_list_name, word_list in config.items():\n",
    "        if word_list_name in ['X', 'Y', 'A', 'B']:\n",
    "            word_list_filtered = list(filter(lambda x: x in embedding and np.count_nonzero(embedding[x]) > 0, word_list))\n",
    "            word_pairs[word_list_name] = word_list_filtered\n",
    "            if len(word_list_filtered) < 2:\n",
    "                print('ERROR: Words from list {} not found in embedding\\n {}'.\\\n",
    "                format(word_list_name, word_list))\n",
    "                print('All word groups must contain at least two words')\n",
    "                return None, None\n",
    "    return get_bias_scores_mean_err(word_pairs,embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(embed_path):\n",
    "    if embed_path.endswith('wv'):\n",
    "        return KeyedVectors.load(embed_path)\n",
    "    elif embed_path.endswith('txt'):\n",
    "        return KeyedVectors.load_word2vec_format(embed_path, binary=False)\n",
    "    elif embed_path.endswith('bin'):\n",
    "        return KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    # NOTE reddit embedding is saved as model (no ext) + syn1neg + syn0\n",
    "    else:\n",
    "        return Word2Vec.load(embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print('usage: python weat.py config.json results_file=config_results.json')\n",
    "        sys.exit(1)\n",
    "\n",
    "    fname = sys.argv[1]\n",
    "    if len(sys.argv) > 2:\n",
    "        results_file = sys.argv[2]\n",
    "    else:\n",
    "        results_file = 'results_' + fname\n",
    "    results = {}\n",
    "    config = read_config.read_json_config(fname)\n",
    "    for e_name, e in config['embeddings'].items():\n",
    "        results[e_name] = {}\n",
    "        if not isinstance(e,dict):\n",
    "            print('loading embedding {}...'.format(e_name))\n",
    "            try:\n",
    "                embedding = load_embedding(e)\n",
    "            except:\n",
    "                print('could not load embedding {}'.format(e_name))\n",
    "                continue;\n",
    "            for name_of_test, test_config in config['tests'].items():\n",
    "                mean, err = run_test(test_config, embedding)\n",
    "                print('mean: {} err: {}'.format(mean, err))\n",
    "                if mean is not None:\n",
    "                    results[e_name][name_of_test] = (round(mean, 4), round(err,4))\n",
    "        else:\n",
    "            print('loading time series embeddings...')\n",
    "            for time, embed_path in e.items():\n",
    "                results[e_name][time] = {}\n",
    "                embedding = load_embedding(embed_path)\n",
    "                for name_of_test, test_config in config['tests'].items():\n",
    "                    print(name_of_test)\n",
    "                    mean, err = run_test(test_config, embedding)\n",
    "                    print('mean: {} err: {}'.format(mean, err))\n",
    "                    if mean is not None:\n",
    "                        results[e_name][time][name_of_test] = (round(mean, 4), round(err,4))\n",
    "        with open(results_file, 'wb') as outfile:\n",
    "            json.dump(results, outfile)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "glove_data_file = 'data/embeddings/GloVe/GloVe_6B/glove.6B.100d.txt'\n",
    "\n",
    "\n",
    "words = pd.read_table(glove_data_file, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "the          -0.038194\n",
       ",            -0.107670\n",
       ".            -0.339790\n",
       "of           -0.152900\n",
       "to           -0.189700\n",
       "                ...   \n",
       "chanty       -0.155770\n",
       "kronik       -0.094426\n",
       "rolonda       0.360880\n",
       "zsombor      -0.104610\n",
       "sandberger    0.283650\n",
       "Name: 1, Length: 400000, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

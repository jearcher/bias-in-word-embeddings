{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "## This is how you link to a colab notebook\n",
    "\n",
    "\n",
    "<a href=\"<link to colab notebook>\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bY4RMFQxrWyv",
    "outputId": "7926df4a-0561-4111-95a0-2a41ddc89e38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, filterfalse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Import local functions\n",
    "from WEAT_functions import *\n",
    "from Other_fxns import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Lists used for Debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptor Debias word lists\n",
    "sys.path.append('./ConceptorDebias')\n",
    "from lists.load_word_lists import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbRMmhwbGL98"
   },
   "source": [
    "## Load Word2Vec into Gensim Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Word2Vec' has no attribute 'load_word2vec_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8ccc3349e396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m Word_2Vec = Word2Vec.load_word2vec_format('data/embeddings/Word2Vec/GoogleNews-vectors-negative300.bin', \n\u001b[0m\u001b[1;32m      2\u001b[0m                                           \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         norm_only=True)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Word2Vec (Google News) embedding has been loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Word2Vec' has no attribute 'load_word2vec_format'"
     ]
    }
   ],
   "source": [
    "Word_2Vec = KeyedVectors.load_word2vec_format('data/embeddings/Word2Vec/GoogleNews-vectors-negative300.bin', \n",
    "                                          binary = True)\n",
    "\n",
    "print(\"Word2Vec (Google News) embedding has been loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine lists for WEAT evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load list of pronouns representing the 'Pronoun' subspace for gender debiasing\"\"\"\n",
    "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
    "gender_list_pronouns = list(set(gender_list_pronouns))\n",
    "\n",
    "\"\"\"Load an extended list of words representing the gender subspace for gender debiasing\"\"\"\n",
    "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
    "gender_list_extended = list(set(gender_list_extended))\n",
    "\n",
    "\"\"\"Load list of proper nouns representing the 'Proper Noun' subspace for gender debiasing\"\"\"\n",
    "gender_list_propernouns = male_cmu + female_cmu\n",
    "gender_list_propernouns = list(set(gender_list_propernouns))\n",
    "\n",
    "\"\"\"Load list of all representing the gender subspace for gender debiasing\"\"\"\n",
    "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
    "gender_list_all = list(set(gender_list_all))\n",
    "\n",
    "\"\"\"Load list of common black and white names for racial debiasing\"\"\"\n",
    "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
    "race_list = list(set(race_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resourceFile = ''\n",
    "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
    "\n",
    "\"\"\"Set the embedding to be used\"\"\"\n",
    "embd = 'glove'\n",
    "\n",
    "\"\"\"Set the subspace to be tested on\"\"\"\n",
    "subspace = 'gender_list_all' \n",
    "\n",
    "\"\"\"Load association and target word pairs\"\"\"\n",
    "X = WEATLists.W_8_Science\n",
    "Y = WEATLists.W_8_Arts\n",
    "A = WEATLists.W_8_Male_terms\n",
    "B = WEATLists.W_8_Female_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7f63d5c37e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_embd = eval(embd)\n",
    "all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Conceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "(100, 342)\n",
      "R calculated\n",
      "C calculated\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
    "if subspace != 'without_conceptor':\n",
    "  subspace_words_list = eval(subspace)\n",
    "  if subspace == 'gender_list_and':\n",
    "    if embd == 'elmo':\n",
    "      subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
    "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
    "\n",
    "      subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
    "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
    "\n",
    "      subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
    "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
    "\n",
    "      cn = AND(cn1, AND(cn2, cn3))\n",
    "    elif embd == 'bert':\n",
    "      cn1 = load_bert_conceptor(all_dict, gender_list_pronouns)\n",
    "      \n",
    "      cn2 = load_bert_conceptor(all_dict, gender_list_extended)\n",
    "      \n",
    "      cn3 = load_bert_conceptor(all_dict, gender_list_propernouns)\n",
    "      \n",
    "      cn = AND(cn1, AND(cn2, cn3))\n",
    "    else:\n",
    "      subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
    "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
    "\n",
    "      subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
    "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
    "\n",
    "      subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
    "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
    "\n",
    "      cn = AND(cn1, AND(cn2, cn3))\n",
    "  else: \n",
    "    if embd == 'elmo':\n",
    "      subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
    "      cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
    "    elif embd == 'bert':\n",
    "      cn = load_bert_conceptor(all_dict, subspace)\n",
    "    else:\n",
    "      subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
    "      cn = process_cn_matrix(np.array(subspace_words_mat).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Conceptored Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Conceptor all embeddings\"\"\"\n",
    "all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
    "\n",
    "\"\"\"Store all conceptored words in a dictonary\"\"\"\n",
    "all_words = {}\n",
    "for word, index in all_words_index.items():\n",
    "  if embd == 'elmo':\n",
    "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
    "  else:\n",
    "    all_words[word] = all_words_cn[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAT d =  -0.14090161819055197\n",
      "WEAT p =  0.595\n"
     ]
    }
   ],
   "source": [
    "d = weat_effect_size(X, Y, A, B, all_words)\n",
    "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
    "\n",
    "print('WEAT d = ', d)\n",
    "print('WEAT p = ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "WEAT (Final).ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
